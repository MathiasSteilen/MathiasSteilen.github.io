[
  {
    "objectID": "posts/20230309-Swiss-Electricity-Demand-Forecasting/index.html",
    "href": "posts/20230309-Swiss-Electricity-Demand-Forecasting/index.html",
    "title": "Forecasting Swiss Electricity Demand",
    "section": "",
    "text": "What is this project about?\n\nThe goal of this project is to test the ability of supervised machine learning models, namely elastic net, gradient boosting and random forest, to predict end-user electricity consumption in Switzerland. The goal for the final product is a model that takes on a date in the future, as well as temperature data, and gives out an accurate estimate of aggregate electricity demand in Switzerland.\nThe electricity demand data comes from Swissgrid and is denoted in \\(kwh\\). Additionally, I will use historical weather data as an additional predictor, which comes from Swiss NBCN. To give a short idea of the structure of this document, I will\n\nProcure and clean the data;\nExplore the data;\nBuild and tune initial models;\nEvaluate initial model performance to focus on one model;\nProceed with thorough tuning of one model;\nEvaluate model performance in the out-of-sample period;\nPresent limitations.\n\n\n\n\n\nData procurement and data cleaning\n\n\nEnd-user electricity demand data\nFirstly, I start by loading the data from Swissgrid. Looking at the first 10 rows, it becomes clear that Swissgrid denotes the demand data in 15 minute intervals.\n\nhead(data, n = 10)\n\n# A tibble: 10 × 2\n   date                     kwh\n   &lt;dttm&gt;                 &lt;dbl&gt;\n 1 2009-01-01 00:15:00 1829966.\n 2 2009-01-01 00:30:00 1715173.\n 3 2009-01-01 00:45:00 1732582.\n 4 2009-01-01 01:00:00 1706689.\n 5 2009-01-01 01:15:00 1722373.\n 6 2009-01-01 01:30:00 1691958.\n 7 2009-01-01 01:45:00 1686226.\n 8 2009-01-01 02:00:00 1655858.\n 9 2009-01-01 02:15:00 1674382.\n10 2009-01-01 02:30:00 1636980.\n\n\nLuckily, there are no missing values. However, the frequency for modelling is is too granular. For this project, I would like to forecast energy demand going one year into the future, to see the ability of the model over longer seasonality time periods. Limitations of a forecasting time frame of this magnitude are presented in the last section. To proceed, I aggregate to daily values:\n\ndata &lt;- data %&gt;% \n  mutate(year = year(date),\n         day_in_year = yday(date)) %&gt;% \n  filter(year &lt;= 2021) %&gt;% \n  group_by(year, day_in_year) %&gt;% \n  summarise(date = last(date) %&gt;% as.Date(),\n            mwh = sum(kwh)/1e3) %&gt;% \n  ungroup()\n\nsummary(data)\n\n      year       day_in_year         date                 mwh        \n Min.   :2009   Min.   :  1.0   Min.   :2009-01-01   Min.   :102894  \n 1st Qu.:2012   1st Qu.: 92.0   1st Qu.:2012-04-01   1st Qu.:141458  \n Median :2015   Median :183.0   Median :2015-07-02   Median :153739  \n Mean   :2015   Mean   :183.1   Mean   :2015-07-02   Mean   :155591  \n 3rd Qu.:2018   3rd Qu.:274.0   3rd Qu.:2018-10-01   3rd Qu.:171736  \n Max.   :2021   Max.   :366.0   Max.   :2021-12-31   Max.   :222496  \n\n\n\n\n\nTemperature Data\nThe picture below shows a document I got from opendata.swiss.\n\n\n\n\n\nIn it, I found a link (highlighted) that led to a file with various weather stations and their respective data download links:\n\nread_csv(\"C:/Users/mathi/OneDrive/R/Predictive Modelling/Electricity Demand Prediction/Data/liste-download-nbcn-d.csv\") %&gt;% \n  glimpse()\n\nRows: 29\nColumns: 13\n$ Station                              &lt;chr&gt; \"Altdorf\", \"Andermatt\", \"Basel / …\n$ `station/location`                   &lt;chr&gt; \"ALT\", \"ANT\", \"BAS\", \"BER\", \"CDF\"…\n$ `WIGOS-ID`                           &lt;chr&gt; \"0-20000-0-06672\", \"0-20000-0-066…\n$ `Data since`                         &lt;chr&gt; \"01.01.1864\", \"01.01.1864\", \"01.0…\n$ `Station height m. a. sea level`     &lt;dbl&gt; 438, 1438, 316, 553, 1017, 1028, …\n$ CoordinatesE                         &lt;dbl&gt; 2690181, 2687445, 2610909, 260193…\n$ CoordinatesN                         &lt;dbl&gt; 1193564, 1165044, 1265612, 120441…\n$ Latitude                             &lt;dbl&gt; 46.88707, 46.63091, 47.54114, 46.…\n$ Longitude                            &lt;dbl&gt; 8.621894, 8.580553, 7.583525, 7.4…\n$ `Climate region`                     &lt;chr&gt; \"Central Alpine north slope\", \"Ce…\n$ Canton                               &lt;chr&gt; \"UR\", \"UR\", \"BL\", \"BE\", \"NE\", \"VD…\n$ `URL Previous years (verified data)` &lt;chr&gt; \"https://data.geo.admin.ch/ch.met…\n$ `URL Current year`                   &lt;chr&gt; \"https://data.geo.admin.ch/ch.met…\n\n\nTherefore, I could use these URLs to access the actual weather data for selected weather stations. I stored the data for all stations of interest in the tibble weather_data. The weather stations I have decided for are both in densely populated areas in northern/eastern/western Switzerland, as well as in the Alps, ensuring a good representation of the whole country.\n\n# weather_data &lt;- read_csv(\"Data/liste-download-nbcn-d.csv\") %&gt;%\n#   filter(`station/location` %in% c(\"SMA\", \"GVE\", \"BAS\", \"DAV\",\"LUG\", \"STG\",\n#                                    \"ALT\", \"NEU\", \"BER\", \"LUZ\")) %&gt;%\n#   mutate(station_data = map(`URL Previous years (verified data)`,\n#                             ~ read.csv(.x, sep = \";\")),\n#          station_data = map(station_data,\n#                             ~ select(.x, date, tre200d0, rre150d0,\n#                                      nto000d0, ure200d0)),\n#          station_data = map(station_data,\n#                             ~ na_if(.x, \"-\")),\n#          station_data = map(station_data,\n#                             ~ .x %&gt;%\n#                               mutate(date = as.character(date),\n#                                      across(c(tre200d0, rre150d0,\n#                                               nto000d0, ure200d0), as.numeric)))) %&gt;%\n#   select(Station, `station/location`, Canton, station_data) %&gt;%\n#   unnest(station_data) %&gt;%\n#   mutate(date = ymd(date)) %&gt;%\n#   select(-Station, -Canton) %&gt;% \n#   pivot_longer(c(tre200d0, rre150d0, nto000d0, ure200d0)) %&gt;% \n#   mutate(name = case_when(\n#     name == \"tre200d0\" ~ paste0(`station/location`,\"_\",\"meantemp\"),\n#     name == \"rre150d0\" ~ paste0(`station/location`,\"_\",\"precipitation\"),\n#     name == \"nto000d0\" ~ paste0(`station/location`,\"_\",\"cloudcoverage\"),\n#     name == \"ure200d0\" ~ paste0(`station/location`,\"_\",\"humidity\"))) %&gt;% \n#   select(-`station/location`) %&gt;% \n#   pivot_wider(names_from = \"name\", values_from = \"value\")\n\nFrom here, I have demand and temperature data, which I can left-join. Additionally, I create averages of the weather readings over all stations, to get an “average Swiss” value. Namely, I create averages for:\n\ntemperature,\nprecipitation,\ncloud coverage,\nhumidity.\n\nStarting by left-joining the data:\n\ndata &lt;- data %&gt;%\n  left_join(weather_data, by = \"date\")\n\nFollowed by the creation of the averages, for all of the four measures outlined in the bullet point list above:\n\ndata &lt;- data %&gt;% \n  left_join(data %&gt;% \n              select(-c(mwh)) %&gt;% \n              pivot_longer(c(ALT_meantemp:STG_humidity)) %&gt;% \n              separate(col = name, into = c(\"location_code\", \"measure\"), \n                       sep = \"_\", remove = F) %&gt;% \n              group_by(date, measure) %&gt;% \n              summarise(avg = mean(value, na.rm = T)) %&gt;% \n              pivot_wider(names_from = measure, values_from = avg),\n            by = \"date\")\n\nAfter doing this, I check for missing values again, because new weather data came in:\n\ncolMeans(is.na(data)) %&gt;% \n  enframe() %&gt;%\n  filter(value &gt; 0) %&gt;%\n  transmute(variable = name, missing_percentage = value)\n\n# A tibble: 8 × 2\n  variable          missing_percentage\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 BER_cloudcoverage           0.769   \n2 DAV_cloudcoverage           1       \n3 GVE_cloudcoverage           0.000211\n4 LUG_cloudcoverage           0.000211\n5 LUZ_cloudcoverage           1       \n6 NEU_cloudcoverage           1       \n7 SMA_cloudcoverage           0.00969 \n8 STG_cloudcoverage           0.770   \n\n\nIt becomes visible that not all stations have cloud coverage data for all, or even for any, points in time. Variables with low percentages of missing values can be imputed or left out at a later stage, but everything above half of the data missing is generally considered problematic. Therefore, in a next step, the variables with missing_percentage larger than 50% are dropped:\n\ndata &lt;- data %&gt;% \n  select(-c(colMeans(is.na(data)) %&gt;% \n              enframe() %&gt;% \n              filter(value &gt; 0.5) %&gt;% \n              pull(name)))\n\nNow, the final data is clean and ready to be used for model training. As a last step in the data cleaning process, I split it into pre- and post-COVID for the later test of the model regarding robustness during corona years, which is also part of the last section on limitations.\n\ndata_corona &lt;- data %&gt;% \n  filter(year &gt; 2019)\n\ndata &lt;- data %&gt;% \n  filter(year &lt;= 2019)\n\nThere are 43 variables, 42 numeric predictors and 1 numeric target variable.\n\nglimpse(data)\n\nRows: 4,017\nColumns: 43\n$ year              &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009…\n$ day_in_year       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ date              &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 200…\n$ mwh               &lt;dbl&gt; 143210.0, 154168.8, 160934.3, 154669.9, 187821.2, 19…\n$ ALT_meantemp      &lt;dbl&gt; 0.7, 0.1, -1.3, -3.3, -3.0, -2.0, -1.6, -2.2, -2.7, …\n$ ALT_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ ALT_cloudcoverage &lt;dbl&gt; 29, 100, 96, 17, 100, 100, 100, 100, 100, 100, 67, 6…\n$ ALT_humidity      &lt;dbl&gt; 89.0, 78.5, 70.3, 78.7, 78.2, 74.7, 71.9, 75.7, 86.0…\n$ BAS_meantemp      &lt;dbl&gt; 1.5, -1.0, -3.1, -3.7, -4.1, -4.2, -5.7, -6.4, -6.1,…\n$ BAS_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.…\n$ BAS_cloudcoverage &lt;dbl&gt; 54, 100, 67, 54, 96, 100, 92, 42, 0, 0, 0, 4, 8, 83,…\n$ BAS_humidity      &lt;dbl&gt; 83.9, 89.4, 71.6, 75.4, 84.0, 86.2, 84.3, 82.1, 87.1…\n$ BER_meantemp      &lt;dbl&gt; -1.4, -2.3, -4.9, -6.0, -5.9, -5.0, -5.2, -6.1, -5.9…\n$ BER_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ BER_humidity      &lt;dbl&gt; 95.4, 89.3, 87.0, 87.3, 88.9, 87.7, 86.2, 82.4, 90.3…\n$ DAV_meantemp      &lt;dbl&gt; -6.2, -9.7, -12.5, -8.9, -11.0, -10.3, -9.2, -8.8, -…\n$ DAV_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ DAV_humidity      &lt;dbl&gt; 87.3, 89.2, 75.2, 58.4, 59.4, 69.2, 66.8, 68.9, 61.6…\n$ GVE_meantemp      &lt;dbl&gt; 0.5, 0.0, -2.2, -3.5, -4.3, -2.8, -2.9, -3.5, -2.3, …\n$ GVE_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ GVE_cloudcoverage &lt;dbl&gt; 17, 100, 100, 75, 92, 92, 100, 100, 100, 100, 96, 10…\n$ GVE_humidity      &lt;dbl&gt; 86.5, 82.1, 76.8, 79.7, 83.2, 71.0, 74.0, 74.0, 82.4…\n$ LUG_meantemp      &lt;dbl&gt; 1.4, 1.4, 1.0, -0.2, 0.7, 0.1, 1.3, 2.5, 1.9, 1.3, 2…\n$ LUG_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.4, 8.6, 0.8, 1.8, 0.0, 0.0, 0.…\n$ LUG_cloudcoverage &lt;dbl&gt; 100, 13, 100, 25, 46, 100, 92, 100, 67, 4, 0, 0, 0, …\n$ LUG_humidity      &lt;dbl&gt; 87.5, 84.2, 80.6, 71.9, 68.2, 91.2, 93.8, 91.0, 74.8…\n$ LUZ_meantemp      &lt;dbl&gt; 0.5, -0.8, -3.6, -4.2, -5.1, -3.5, -3.9, -4.6, -4.2,…\n$ LUZ_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.…\n$ LUZ_humidity      &lt;dbl&gt; 93.3, 85.5, 80.0, 81.5, 83.7, 81.9, 79.7, 75.9, 86.6…\n$ NEU_meantemp      &lt;dbl&gt; -0.5, -1.3, -3.9, -5.1, -4.2, -3.6, -4.2, -5.1, -5.2…\n$ NEU_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ NEU_humidity      &lt;dbl&gt; 92.4, 90.0, 80.7, 87.9, 85.8, 80.7, 78.0, 77.4, 90.1…\n$ SMA_meantemp      &lt;dbl&gt; -0.5, -2.2, -5.1, -5.3, -5.8, -4.6, -5.4, -6.5, -5.6…\n$ SMA_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ SMA_cloudcoverage &lt;dbl&gt; 83, 100, 100, 79, 88, 100, 100, 79, 100, 100, 100, 7…\n$ SMA_humidity      &lt;dbl&gt; 92.7, 87.5, 80.9, 82.5, 83.4, 84.4, 82.0, 81.0, 87.7…\n$ STG_meantemp      &lt;dbl&gt; -1.2, -3.8, -6.1, -6.6, -8.1, -5.8, -6.7, -7.8, -7.6…\n$ STG_precipitation &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.…\n$ STG_humidity      &lt;dbl&gt; 94.5, 94.3, 86.9, 87.8, 87.4, 90.6, 91.1, 82.7, 97.5…\n$ cloudcoverage     &lt;dbl&gt; 58.85714, 87.00000, 94.71429, 56.00000, 88.28571, 98…\n$ humidity          &lt;dbl&gt; 90.25, 87.00, 79.00, 79.11, 80.22, 81.76, 80.78, 79.…\n$ meantemp          &lt;dbl&gt; -0.52, -1.96, -4.17, -4.68, -5.08, -4.17, -4.35, -4.…\n$ precipitation     &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.19, 0.94, 0.08, 0.22, 0.00…\n\n\nAt this stage, we are ready to move on to exploring the data.\n\n\n\n\n\nExploratory Data Analysis\n\nExploratory data analysis (EDA) is a tool for exploring the relation between predictors and the target variable of a data set prior to building models. It is important to get a mental picture of the data and helps thinking about how to approach the problem.\n\ndata %&gt;% \n  mutate(day_in_week = wday(date),\n         day = wday(date, label = T),\n         weekend = case_when(\n           day_in_week %in% c(7) ~ \"Saturday\",\n           day_in_week %in% c(1) ~ \"Sunday\",\n           TRUE ~ \"Work Day\") %&gt;% \n           factor(levels = c(\"Work Day\", \"Saturday\", \"Sunday\"))) %&gt;% \n  ggplot(aes(day_in_year, mwh, colour = weekend)) +\n  geom_point(alpha = 0.4) +\n  labs(title = \"Swiss End-User Electricity Consumption\",\n       subtitle = \"For each day in the year (1-365), 11 daily consumption points from the period 2009 to 2019 are plotted.\\nColouring indicates if a day lies on a weekend or not.\",\n       caption = \"Data Source: Swissgrid, Period 2009-2019\",\n       x = \"Day Of The Year (1-365)\",\n       y = \"Daily Consumption\",\n       colour = NULL) +\n  scale_x_continuous(breaks = c(1, seq(50,366,50))) +\n  scale_y_continuous(labels = comma_format(suffix = \" MWh\")) +\n  ggsci::scale_colour_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10,\n                                     colour = \"grey50\"),\n        plot.caption = element_text(face = \"italic\", size = 9,\n                                    colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nThis first chart shows electricity consumption as a function of the day of the year (number 1 to 366) over the period 2009-2019. Clearly, electricity demand peaks in winter and is lowest in summer. Furthermore, there are fixed effects for work days and the two days on the weekends. The conclusion from this chart is that including dummies for day of the week and day of the year is likely important, as well as accounting for intra-year seasonality.\n\ndata %&gt;% \n  filter(year == 2010) %&gt;%\n  ggplot(aes(date, mwh)) +\n  geom_line(colour = \"dodgerblue\", size = 0.75) +\n  labs(title = \"Swiss Daily Energy Consumption in 2010\",\n       y = \"Daily Consumption\",\n       x = NULL) +\n  scale_y_continuous(labels = comma_format(suffix = \" MWh\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10,\n                                     colour = \"grey50\"),\n        plot.caption = element_text(face = \"italic\", size = 9,\n                                    colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nSingling out one year randomly, 2010 for instance, reveals even more than intra-year seasonality. Additionally, there is intra-week seasonality, which needs to be accounted for, as seen before.\n\ndata %&gt;% \n  filter(year == 2019) %&gt;% \n  select(date, meantemp, humidity, precipitation, cloudcoverage) %&gt;% \n  pivot_longer(-date) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line(size = 0.75, alpha = 0.75) +\n  facet_wrap(~ name, scales = \"free_y\") +\n  labs(title = \"Weather Measurements in 2019\",\n       y = \"Measurement\",\n       x = NULL,\n       colour = NULL) +\n  ggsci::scale_colour_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10,\n                                     colour = \"grey50\"),\n        plot.caption = element_text(face = \"italic\", size = 9,\n                                    colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nLooking at the chart above, only temperature shows visible seasonality. All other variables look erratic by themselves. However, let’s look at a scatter plot to see potential linear relationships:\n\ndata %&gt;% \n  select(date, meantemp, cloudcoverage, humidity, precipitation, mwh) %&gt;% \n  pivot_longer(-c(date, mwh)) %&gt;% \n  mutate(day_in_week = wday(date),\n         day = wday(date, label = T),\n         weekend = case_when(\n           day_in_week %in% c(7) ~ \"Saturday\",\n           day_in_week %in% c(1) ~ \"Sunday\",\n           TRUE ~ \"Work Day\") %&gt;% \n           factor(levels = c(\"Work Day\", \"Saturday\", \"Sunday\"))) %&gt;% \n  ggplot(aes(value, mwh, colour = weekend)) +\n  geom_point(alpha = 0.5, size = 0.75) +\n  geom_smooth(size = 0.75, se = F) +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(title = \"Relation Between Weather Conditions And Electricity Demand\",\n       subtitle = \"TBD\",\n       y = \"Daily Electricity Demand\",\n       x = \"Measurement\",\n       colour = NULL) +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  ggsci::scale_colour_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10,\n                                     colour = \"grey50\"),\n        plot.caption = element_text(face = \"italic\", size = 9,\n                                    colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nThe above chart reveals the relationships between weather measurements and the target variable electricity demand better. There is a very strong negative relationship between temperature and the target, with a non-linearity as soon as higher degree Celsius are attained. More clouds lead to higher demand. The same holds for humidity. Lastly, precipitation does not show a clear picture, albeit slightly negative. It has to be noted that the last three predictors leave much more variance unexplained than temperature.\nAfter exploring the predictors in the data, I can move on to building initial models.\n\n\n\n\nBuilding initial models\n\nBefore diving into hyperparameter tuning of the models, I will create three simple models using default parameters to judge the potential of them. The first step in modelling is creating the splits. In this case, the assignment is not random, but by time, as we are dealing with time series. I will use 10 years for training and predict on the holdout, which will be 2019. Additionally, I initiate the folds for cross-validation of the model metrics at a later stage in the tuning process.\n\ndt_split &lt;- data %&gt;% \n  time_series_split(date_var = date, assess = \"1 year\", cumulative = T)\n\ndt_train &lt;- training(dt_split)\ndt_test &lt;- testing(dt_split)\n\nfolds &lt;- vfold_cv(dt_train, v = 5)\n\nVisualising the train/test split, it becomes very clear, what the goal will be: Forecasting the year 2019, for which we have the true data and are able to compute metrics to evaluate model performance.\n\nbind_rows(\n  dt_train %&gt;% mutate(id = \"training\"),\n  dt_test %&gt;% mutate(id = \"testing\")\n) %&gt;% \n  ggplot(aes(date, mwh, colour = id)) +\n  geom_line() +\n  scale_colour_manual(values = c(\"firebrick\", \"dodgerblue\")) +\n  labs(title = \"Training/Testing Split\", \n       y = NULL,\n       x = NULL,\n       colour = NULL) +\n  scale_y_continuous(labels = comma_format(suffix = \" MWh\")) +\n  theme_light() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10,\n                                     colour = \"grey50\"),\n        plot.caption = element_text(face = \"italic\", size = 9,\n                                    colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nThe next step in the modelling process with the package tidymodels is creating the recipe for data preprocessing. As I don’t have a great amount of predictors, only date and weather information, I will try to squeeze as much information from the date column as possible. Namely, I\n\nuse step_date to get factor columns indicating day and month;\nuse step_holiday to create dummy variables for official holidays in Switzerland, on which business is interrupted;\nimpute missing values in numeric predictors with the mean of the respective variable;\nfill missing values in nominal predictors with “unknown”;\ncreate new variables for number of month, semester, weekday, number of the week, quarter and a dummy variable for christmas holidays;\ncreate dummy variables from nominal predictors (not for random forest, as the model can deal with it);\nremove zero variance predictors.\n\n\nmodel_rec &lt;- recipe(mwh ~ .,\n                    data = dt_train) %&gt;%\n  step_date(date) %&gt;%\n  step_holiday(date, holidays = timeDate::listHolidays(\"CH\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_unknown(all_nominal_predictors()) %&gt;% \n  step_mutate(month = lubridate::month(date),\n              year_half = lubridate::semester(date) %&gt;% as.factor,\n              week_day = lubridate::wday(date),\n              week_in_year = lubridate::week(date),\n              quarter = lubridate::quarter(date),\n              date_christmas = ifelse(between(day_in_year, 358, 366),\n                                      1, 0),\n              day_in_year = as.factor(day_in_year)) %&gt;% \n  step_rm(date) %&gt;% \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv()\n\nrf_rec &lt;- recipe(mwh ~ .,\n                 data = dt_train) %&gt;%\n  step_date(date) %&gt;%\n  step_holiday(date, holidays = timeDate::listHolidays(\"CH\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_unknown(all_nominal_predictors()) %&gt;% \n  step_mutate(month = lubridate::month(date),\n              year_half = lubridate::semester(date) %&gt;% as.factor,\n              week_day = lubridate::wday(date),\n              week_in_year = lubridate::week(date),\n              quarter = lubridate::quarter(date),\n              date_christmas = ifelse(between(day_in_year, 358, 366),\n                                      1, 0),\n              day_in_year = as.factor(day_in_year)) %&gt;% \n  step_rm(date) %&gt;%\n  step_zv()\n\nIn a next step, I specify the three models, that I want to use. They are elastic net, gradient boosting and random forest. Elastic net is a linear combination of ridge and lasso regression, which are linear regressions with additional penalty terms. Gradient boosting and random forest are tree ensemble methods with different algorithm, which I will not explain in detail in this project, as the application and not the definition of these methods stands in focus.\n\nen_spec &lt;- linear_reg(penalty = 0.02, mixture = 0.5) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"regression\")\n\nxg_spec &lt;- boost_tree(trees = 1000) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\nrf_spec &lt;- rand_forest(trees = 500) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nAs I am not tuning hyperparameters at this stage yet and just set some default parameters, the workflows can immediately be entirely fit to the training data.\n\nen_wf_fit &lt;- workflow() %&gt;% \n  add_recipe(model_rec) %&gt;% \n  add_model(en_spec) %&gt;% \n  fit(dt_train)\n\nxg_wf_fit &lt;- workflow() %&gt;% \n  add_recipe(model_rec) %&gt;% \n  add_model(xg_spec) %&gt;% \n  fit(dt_train)\n\nrf_wf_fit &lt;- workflow() %&gt;% \n  add_recipe(rf_rec) %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit(dt_train)\n\n\n\n\nEvaluating initial model performance\n\nWith the fitted initial models, I can now make predictions on the holdout data. I use the predict function in combination with the map function, which enables me to run each elements in a list (for instance a column with type list in a tibble) through a given function.\n\npredictions &lt;- tibble(\n  type = c(\"RF\", \"EN\", \"GB\"),\n  wflow = list(rf_wf_fit, en_wf_fit, xg_wf_fit),\n  predictions = map(.x = wflow, .f = ~ augment(.x, dt_test))\n) %&gt;% \n  select(-wflow) %&gt;% \n  unnest(predictions)\n\npredictions %&gt;% select(date, mwh, .pred)\n\n# A tibble: 1,095 × 3\n   date           mwh   .pred\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 2019-01-01 142521. 168353.\n 2 2019-01-02 157049. 171308.\n 3 2019-01-03 177640. 184632.\n 4 2019-01-04 179728. 189062.\n 5 2019-01-05 169006. 173717.\n 6 2019-01-06 156541. 154764.\n 7 2019-01-07 185855. 184220.\n 8 2019-01-08 190850. 188266.\n 9 2019-01-09 194288. 190973.\n10 2019-01-10 196570. 190861.\n# ℹ 1,085 more rows\n\n\nNow that I have the predictions on the holdout data set, I can compute evaluation metrics to compare the performance of each model. Clearly, gradient boosting and random forest have outperformed the elastic net.\n\nevaluation_metrics &lt;- metric_set(rsq, rmse, mae)\n\npredictions %&gt;% \n  group_by(type) %&gt;% \n  evaluation_metrics(truth = mwh, estimate = .pred) %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = \".metric\", values_from = \".estimate\") %&gt;% \n  arrange(-rsq)\n\n# A tibble: 3 × 4\n  type    rsq  rmse   mae\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GB    0.960 4516. 2935.\n2 RF    0.951 5386. 3753.\n3 EN    0.934 5925. 4290.\n\n\nAdditionally, I plot the predictions and the actual values in a scatter plot:\n\npredictions %&gt;% \n  ggplot(aes(mwh, .pred)) +\n  geom_point(alpha = 0.2, colour = \"midnightblue\", size = 2) +\n  facet_wrap(~ type) +\n  geom_abline(lty = \"dashed\", colour = \"grey50\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\nIdeally, all points would lie on the diagonal line from the origin. The spread indicates that elastic net has performed worse than tree-based ensemble models. Furthermore, the gradient boosting has performed better than the random forest. Given these interim results, I will continue with the tuning for both gradient boosting and random forest and abandon the idea of using elastic net.\n\n\n\nBuilding final models\n\nWith the knowledge from above, I will now tune hyperparameters for a random forest and gradient boosting model, to see whether I can fill the gaps of the initial models.\nFor the random forest model, I specify all three parameters, mtry, min_n and trees to be tuned. I also reset the workflow and add the recipe and the model specification. Lastly, I initiate the grid with the values which will be used for hyperparameter tuning. The use of a latin hypercube design ideally chooses hyperparameters to maximally fill out the potential hyperparameter space. This ensures efficient tuning with lower numbers of models to be tried. For 3 hyperparameters, I use 50 configurations, which will be fitted on 5 folds, therefore resulting in 250 fits.\n\nrf_spec &lt;- rand_forest(mtry = tune(),\n                       min_n = tune(),\n                       trees = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wflow &lt;- workflow() %&gt;% \n  add_recipe(rf_rec) %&gt;% \n  add_model(rf_spec)\n\nrf_grid &lt;- grid_latin_hypercube(finalize(mtry(), dt_train),\n                                min_n(),\n                                trees(),\n                                size = 50)\n\nI approach the gradient boosting model with the same approach. For 6 hyperparameters, I use 100 model configurations which will be fitted on all 5 folds each, resulting in 500 fits.\n\ngb_spec &lt;- boost_tree(mtry = tune(),\n                      trees = tune(),\n                      min_n = tune(),\n                      tree_depth = tune(),\n                      learn_rate = tune(),\n                      loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\ngb_wflow &lt;- workflow() %&gt;% \n  add_recipe(model_rec) %&gt;% \n  add_model(gb_spec)\n\ngb_grid &lt;- grid_latin_hypercube(finalize(mtry(), dt_train),\n                                trees(),\n                                min_n(),\n                                tree_depth(),\n                                learn_rate(),\n                                loss_reduction(),\n                                size = 100)\n\nWith everything specified, I proceed with the actual tuning. I use parallel processing with a function obtained from Stackoverflow here and a function from the doParallel package, which enables me to open and close 6 cores instead of 1, speeding up the training process tremendously. The actual tuning function takes on the workflow, the grid with the parameters and the cross-validation resamples.\n\n# Random Forest\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\nrf_tune &lt;- tune_grid(object = rf_wflow,\n                     grid = rf_grid,\n                     resamples = folds)\n\nstopCluster(cl)\nunregister_dopar()\n\n# Gradient Boosting\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\ngb_tune &lt;- tune_grid(object = gb_wflow,\n                     grid = gb_grid,\n                     resamples = folds)\n\nstopCluster(cl)\nunregister_dopar()\n\nAfter this function has run, I can now compare the tuning results. Random forest has performed better after tuning, with an increase of \\(R^2\\) of about 2 percentage points.\n\nrf_tune %&gt;% \n  show_best(metric = \"rsq\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    38  1205     3 rsq     standard   0.952     5 0.00487 Preprocessor1_Model13\n2    26  1302     7 rsq     standard   0.952     5 0.00479 Preprocessor1_Model29\n3    22  1803     6 rsq     standard   0.952     5 0.00481 Preprocessor1_Model43\n4    25   581     7 rsq     standard   0.952     5 0.00480 Preprocessor1_Model19\n5    21   515     5 rsq     standard   0.952     5 0.00467 Preprocessor1_Model25\n\n\n\ngb_tune %&gt;% \n  show_best(metric = \"rsq\")\n\n# A tibble: 5 × 12\n   mtry trees min_n tree_depth learn_rate loss_reduction .metric .estimator\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     \n1    32  1692     6          3     0.0267        1.14e-7 rsq     standard  \n2    11   484     4         11     0.0533        4.07e-1 rsq     standard  \n3    36  1250    24          4     0.0179        4.34e-9 rsq     standard  \n4    19  1989    28         12     0.0217        6.57e+0 rsq     standard  \n5    29  1889    39          8     0.0898        2.15e+1 rsq     standard  \n# ℹ 4 more variables: mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;\n\n\nGradient boosting performed slightly better, but the improvement is likely statistically and economically insignificant, so we can call it the same.\nWith these results, I fit the models with the best configurations from in-sample training onto the entire training data set from the split initiated at the very beginning:\n\ngb_final_fit &lt;- gb_wflow %&gt;% \n  finalize_workflow(select_best(gb_tune, metric = \"rsq\")) %&gt;% \n  last_fit(dt_split)\n\nrf_final_fit &lt;- rf_wflow %&gt;% \n  finalize_workflow(select_best(rf_tune, metric = \"rsq\")) %&gt;% \n  last_fit(dt_split)\n\nAfter this, the final models are done.\n\n\n\nEvaluating final model performance\n\nWith the final models, I can now finally explore the performance on the out-of-sample data, which was held out from model training.\n\nbind_rows(\n  rf_final_fit %&gt;% \n    collect_predictions() %&gt;% \n    mutate(model = \"Random Forest\"),\n  gb_final_fit %&gt;% \n    collect_predictions() %&gt;% \n    mutate(model = \"Gradient Boosting\")\n) %&gt;% \n  select(model, mwh, .pred) %&gt;% \n  ggplot(aes(mwh, .pred)) +\n  geom_point(alpha = 0.5, size = 1.5, colour = \"midnightblue\") +\n  facet_wrap(~ model) +\n  labs(title = \"Out-Of-Sample Model Performance\",\n       y = \"Estimate\",\n       x = \"Truth\") +\n  geom_abline(colour = \"grey50\", lty = \"dashed\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"),\n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\nFrom the above chart, it becomes visually clear that the gradient boosting shows fewer outliers than the random forest, explaining the better \\(R^2\\) of the first.\nLooking at out-of-sample model evaluation metrics in the table below, it looks like the gradient boosting model performed better than the random forest, although both performed very well.\n\nbind_rows(\n  rf_final_fit %&gt;% \n    collect_predictions() %&gt;% \n    mutate(model = \"Random Forest\"),\n  gb_final_fit %&gt;% \n    collect_predictions() %&gt;% \n    mutate(model = \"Gradient Boosting\")\n) %&gt;% \n  group_by(model) %&gt;% \n  evaluation_metrics(truth = mwh, estimate = .pred) %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = \".metric\", values_from = \".estimate\")\n\n# A tibble: 2 × 4\n  model               rsq  rmse   mae\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Gradient Boosting 0.965 4333. 2916.\n2 Random Forest     0.959 4608. 3259.\n\n\nAt this stage, I can plot the actual time series against the predictions of the model, in order to see difference in time:\n\nbind_rows(\n  gb_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    augment(dt_test) %&gt;% \n    mutate(model = \"Gradient Boosting\"),\n  rf_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    augment(dt_test) %&gt;% \n    mutate(model = \"Random Forest\")\n) %&gt;% \n  transmute(date, model, prediction = .pred, actual = mwh) %&gt;% \n  pivot_longer(-c(date, model)) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line(alpha = 0.75, size = 0.75) +\n  facet_wrap(~ model, nrow = 2) +\n  labs(title = \"Out-Of-Sample Time Series\",\n       y = NULL,\n       x = NULL,\n       colour = NULL) +\n  ggsci::scale_colour_jama() +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"))\n\n\n\n\nBoth models have a very good fit. The gradient boosting model has slightly higher accuracy on most weeks than random forest. At this stage, I would like to know what the average error in percentage terms of the model is. Therefore, I calculate the mean absolute error and divide it by the mean of the time series:\n\nmean_abs_error &lt;- gb_final_fit %&gt;%\n  extract_workflow() %&gt;% \n  augment(dt_test) %&gt;% \n  mae(mwh, .pred) %&gt;% \n  pull(.estimate)\n\nmean_mwh &lt;- gb_final_fit %&gt;%\n  extract_workflow() %&gt;% \n  augment(dt_test) %&gt;% \n  summarise(mean(mwh)) %&gt;% \n  pull()\n\nmean_abs_error/mean_mwh\n\n[1] 0.01901622\n\n\nThe model has an average deviation of less than 2%, which is very good considering the only predictors being date and weather.\nNext, I want to look at the errors (delta) of the predictions over the actual values as a function of time:\n\nbind_rows(\n  gb_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    augment(dt_test) %&gt;% \n    mutate(model = \"Gradient Boosting\"),\n  rf_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    augment(dt_test) %&gt;% \n    mutate(model = \"Random Forest\")\n) %&gt;% \n  mutate(delta = .pred/mwh-1,\n         exceedance = ifelse(abs(delta) &gt; 0.05, \n                             \"&gt;5%\", \"&lt;=5%\") %&gt;% as.factor()) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date, delta, colour = exceedance)) +\n  geom_point(alpha = 0.5) +\n  annotate(\"rect\", xmin = ymd(\"20180101\"), xmax = ymd(\"20201231\"),\n           ymin = -0.05, ymax = 0.05, alpha = .1, fill = \"dodgerblue\") +\n  geom_hline(yintercept = 0.05, colour = \"dodgerblue\", lty = \"dashed\") +\n  geom_hline(yintercept = -0.05, colour = \"dodgerblue\", lty = \"dashed\") +\n  facet_wrap(~ model, nrow = 2) +\n  labs(title = \"Out-Of-Sample Delta In Percent\",\n       subtitle = \"Delta calculated as deviation of the estimate from the actual value.\",\n       y = \"Delta\",\n       x = NULL,\n       colour = \"Exceedance\") +\n  scale_colour_manual(values = c(\"dodgerblue\", \"firebrick\")) +\n  scale_y_continuous(labels = scales::percent_format(), \n                     limits = c(-0.25, 0.25)) +\n  coord_cartesian(xlim = c(ymd(\"20190101\"), ymd(\"20191231\"))) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nIn the above chart, I have plotted the relative deviation of the predictions versus the actual values. The blue confidence band marks the 5% relative deviation interval. As can be seen, most values lie within the interval, but sometimes, outliers cannot be avoided from either model. Likely, these are holidays that vary each year, like Easter.\nLastly, I would like to see how the model performs against just predicting the mean of each day from the training data:\n\ndata %&gt;% \n  group_by(day_in_year) %&gt;% \n  summarise(predicting_daily_mean = mean(mwh)) %&gt;% \n  ungroup() %&gt;% \n  head(365) %&gt;% \n  bind_cols(\n    gb_final_fit %&gt;% \n      extract_workflow() %&gt;% \n      predict(dt_test) %&gt;% \n      rename(gradient_boosting = .pred)\n  ) %&gt;% \n  bind_cols(dt_test %&gt;% select(date, mwh)) %&gt;% \n  pivot_longer(-c(day_in_year, mwh, date)) %&gt;% \n  group_by(name) %&gt;% \n  evaluation_metrics(truth = mwh, estimate = value) %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate)\n\n# A tibble: 2 × 4\n  name                    rsq   rmse    mae\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 gradient_boosting     0.965  4333.  2916.\n2 predicting_daily_mean 0.597 14295. 11140.\n\n\nFrom the performance metrics, it becomes abundantly clear that the model performance much better than the mean prediction. The same thing visualised as a time series:\n\ndata %&gt;% \n  group_by(day_in_year) %&gt;% \n  summarise(predicting_daily_mean = mean(mwh)) %&gt;% \n  ungroup() %&gt;% \n  head(365) %&gt;% \n  bind_cols(\n    gb_final_fit %&gt;% \n      extract_workflow() %&gt;% \n      predict(dt_test) %&gt;% \n      rename(gradient_boosting = .pred)\n  ) %&gt;% \n  bind_cols(dt_test %&gt;% transmute(date, actuals = mwh)) %&gt;% \n  pivot_longer(-c(day_in_year, date)) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line(alpha = 0.75, size = 0.5) +\n  labs(title = \"Out-of-sample Time Series\",\n       y = NULL,\n       x = NULL,\n       colour = NULL) +\n  ggsci::scale_colour_jama() +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"))\n\n\n\n\nClearly, the gradient boosting model shows solid performance and could be used on future data points, after retraining on newer data.\n\n\n\n\nLimitations\n\nBefore the end, I want to include some limitations where the model might show deficiencies.\n\n\nPerformance in unforeseen circumstances\nPerformance of predictive model relies heavily on the continuance of the relation of predictors with the target variable. If the relation breaks down due to unforeseen circumstances like a pandemic, then the model will act as if this event had never happened.\nLet me (shortly) demonstrate this, using the data from 2020-2022, which I held out at the beginning for this exact purpose. I can now send this new data through the model and make predictions. Looking at the time series reveals the problem:\n\ngb_final_fit %&gt;% \n  extract_workflow() %&gt;% \n  augment(data_corona) %&gt;%\n  transmute(date, prediction = .pred, actual = mwh) %&gt;% \n  pivot_longer(-c(date)) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line(alpha = 0.75, size = 0.5) +\n  labs(title = \"Out-Of-Sample Time Series: Corona Period\",\n       y = NULL,\n       x = NULL,\n       colour = NULL) +\n  ggsci::scale_colour_jama() +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MWh\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"))\n\n\n\n\nThe model predicted as if the pandemic had never hit. Without retraining and lookback on past values, the model completely mispredicts the future, which is especially grave during the first lockdown period as shown in the chart below. Looking at the delta over time shows this lockdown period nicely.\n\ngb_final_fit %&gt;% \n  extract_workflow() %&gt;% \n  augment(data_corona) %&gt;%\n  group_by() %&gt;% \n  mutate(delta = .pred/mwh-1,\n         exceedance = ifelse(abs(delta) &gt; 0.05, \n                             \"&gt;5%\", \"&lt;=5%\") %&gt;% as.factor()) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date, delta, colour = exceedance)) +\n  geom_point(alpha = 0.5) +\n  annotate(\"rect\", xmin = ymd(\"20180101\"), xmax = ymd(\"20251231\"),\n           ymin = -0.05, ymax = 0.05, alpha = .1, fill = \"dodgerblue\") +\n  annotate(\"rect\", xmin = ymd(\"20200310\"), xmax = ymd(\"20200628\"),\n           ymin = -1, ymax = 1, alpha = .1, fill = \"firebrick\") +\n  geom_hline(yintercept = 0.05, colour = \"dodgerblue\", lty = \"dashed\") +\n  geom_hline(yintercept = -0.05, colour = \"dodgerblue\", lty = \"dashed\") +\n  labs(title = \"Out-Of-Sample Delta In Percent: Corona Period\",\n       subtitle = \"Delta calculated as deviation of the estimate from the actual value.\\nLockdown period in 2020 indicated by red shading.\",\n       y = \"Delta\",\n       x = NULL,\n       colour = \"Exceedance\") +\n  scale_colour_manual(values = c(\"dodgerblue\", \"firebrick\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  coord_cartesian(xlim = c(ymd(\"20200101\"), ymd(\"20220101\")),\n                  ylim = c(-0.17,0.32)) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nThe cloud of red points during the first lockdown shows the model overestimating demand. This abnormal effect has not happened again as the Swiss government have made a strong effort to keep the economy running, however, the period of a couple of months showed the vulnerability of the model. It must be noted that this vulnerability is not unique to this model, virtually all models inferring from the past are subject to it.\n\n\n\nGeneral usability in the energy sector\nIn practice, the forecast is usually much shorter than one year. Production is adjusted almost in real-time, so forecasts over long time-periods are not useful. Additionally, changing conditions of the predictors have to be accounted for, which is more feasible in real-time, short-term models. This goes hand in hand with the next point.\n\n\n\nWeather readings\nThe weather readings for the training of this model were actual values. In practice, when predicting energy demand, weather forecasts are part of the features, instead of actual values. This additional uncertainty will likely impact model performance negatively in practice.\n\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20221114-CEDIES/index.html",
    "href": "posts/20221114-CEDIES/index.html",
    "title": "Luxembourgish Student Aid",
    "section": "",
    "text": "What is Luxembourgish financial aid for higher education?\n\nThe “aide financière” is offered by the Luxembourg government to students enrolled on an accredited higher education course offered by an accredited institution and meeting certain admission criteria. It comprises several grants and a student loan. It is paid in semi-annual instalments (per academic year).\n\n\n\n\nDescription of the Data\n\nThe data for this post comes from the Ministry of Higher Education and Research (MESR) in Luxembourg and is about applications for state financial aid for higher education made by students to the government. The data includes information on the status (rejected, accepted) of applications, as well as the amounts that are paid out each semester.\nThe two tabular data sets that I am mainly concerned with are the actual amounts paid out…\n\n\nCode\nglimpse(montants)\n\n\nRows: 101,934\nColumns: 11\n$ semestre                &lt;chr&gt; \"Hiver\", \"Hiver\", \"Hiver\", \"Hiver\", \"Hiver\", \"…\n$ pays_etablissement_iso3 &lt;chr&gt; \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT…\n$ localite_etablissement  &lt;chr&gt; \"Autres localités\", \"Autres localités\", \"Autre…\n$ resident                &lt;chr&gt; \"Résident\", \"Résident\", \"Résident\", \"Résident\"…\n$ nationalite             &lt;chr&gt; \"DEU\", \"DEU\", \"DEU\", \"DEU\", \"DEU\", \"DEU\", \"LUX…\n$ sexe                    &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"…\n$ type_bourse             &lt;chr&gt; \"Anticumul\", \"Bourse de base\", \"Bourse de mobi…\n$ type_pret               &lt;chr&gt; \"Anticumul\", \"Prêt de base\", \"Prêt de mobilité…\n$ total_bourse            &lt;dbl&gt; 0, 1000, 1225, 0, 0, 1900, 0, 3000, 3675, 0, 2…\n$ total_pret              &lt;dbl&gt; 0, 3250, 0, 0, 0, 0, 0, 9750, 0, 0, 200, 1900,…\n$ date                    &lt;date&gt; 2016-09-01, 2016-09-01, 2016-09-01, 2016-09-0…\n\n\n…and the number of requests made by students, as well as their status.\n\n\nCode\nglimpse(effectifs)\n\n\nRows: 73,280\nColumns: 10\n$ semestre                &lt;chr&gt; \"Hiver\", \"Hiver\", \"Hiver\", \"Hiver\", \"Hiver\", \"…\n$ demande_statut          &lt;chr&gt; \"REFUSE\", \"REFUSE\", \"REFUSE\", \"REFUSE\", \"REFUS…\n$ resident                &lt;chr&gt; \"Résident\", \"Résident\", \"Résident\", \"Résident\"…\n$ sexe                    &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"…\n$ pays_etablissement_iso3 &lt;chr&gt; \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT\", \"AUT…\n$ localite_etablissement  &lt;chr&gt; \"Autres localités\", \"Innsbruck\", \"Innsbruck\", …\n$ domaine_formation       &lt;chr&gt; \"Médecine\", \"Architecture et bâtiment\", \"Médec…\n$ diplome                 &lt;chr&gt; \"Diplôme manquant\", \"Master\", \"Formation de ba…\n$ total_etudiants         &lt;dbl&gt; 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 3, 1, 1…\n$ date                    &lt;date&gt; 2016-09-01, 2016-09-01, 2016-09-01, 2016-09-0…\n\n\nIn the following, I’ll ask and answer questions that I was interested in while inspecting the data set.\n\n\n\n\nHow many applications for state financial aid are made each semester?\n\nGenerally, it can be seen that relatively more applications are made for the summer semester, presumably because more students start their studies in the winter semester and potentially drop out in the subsequent one. Generally, seeing more than 30,000 applications is astonishing, given that this is 5% of the country’s population.\n\n\nCode\neffectifs %&gt;% \n  group_by(date) %&gt;% \n  summarise(etudiants = sum(total_etudiants, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(yoy = etudiants/lag(etudiants, n = 2) - 1) %&gt;% \n  ggplot(aes(date, etudiants)) +\n  geom_line(lty = \"dotted\") +\n  geom_point() +\n  labs(title = \"Applications made to the MESR each semester\",\n       y = \"Applications\",\n       x = NULL) +\n  scale_y_continuous(labels = comma_format())\n\n\n\n\n\nThe chart below shows that there are more requests from female students than from male students, likely an indication of the latter being underrepresented in higher education.\n\n\nCode\neffectifs %&gt;% \n  group_by(date, sexe) %&gt;% \n  summarise(etudiants = sum(total_etudiants, na.rm = T)) %&gt;% \n  ggplot(aes(date, etudiants, colour = sexe)) +\n  geom_line(lty = \"dotted\") +\n  geom_point() +\n  labs(title = \"Applications by sex over time\",\n       colour = \"Sex\",\n       y = \"Students\",\n       x = NULL) +\n  scale_y_continuous(labels = comma_format())\n\n\n\n\n\n\n\n\n\nHow many applications are rejected?\n\nLooking at the rejection percentage out of all applications, it looks like there is a slight upwards trend. Notably, summer semester applications are rejected at a higher rate, likely because new students make more mistakes in their application procedure and have to try again.\n\n\nCode\neffectifs %&gt;% \n  select(date, sexe, demande_statut, total_etudiants) %&gt;% \n  group_by(date, demande_statut) %&gt;% \n  summarise(students = sum(total_etudiants)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = demande_statut, values_from = students) %&gt;% \n  mutate(reject = REFUSE/(REFUSE + VALIDE)) %&gt;% \n  ggplot(aes(date, reject)) +\n  geom_line(lty = \"dotted\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, size = 0.5) +\n  labs(title = \"Rejection percentage in total applications\",\n       y = \"Percent rejected\",\n       x = NULL) +\n  scale_y_continuous(labels = percent_format())\n\n\n\n\n\nInterestingly, applications from male students get rejected at higher rates. This might have several reasons, for example carelessness when preparing applications or a certain criteria that male students do not fulfil at higher rates. As there are not too many predictors in the data, it will be hard to find the true reason here.\n\n\nCode\neffectifs %&gt;% \n  select(date, sexe, demande_statut, total_etudiants) %&gt;% \n  group_by(date, sexe, demande_statut) %&gt;% \n  summarise(students = sum(total_etudiants)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = demande_statut, values_from = students) %&gt;% \n  mutate(reject = REFUSE/(REFUSE + VALIDE)) %&gt;% \n  ggplot(aes(date, reject, colour = sexe)) +\n  geom_line(lty = \"dotted\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, size = 0.5) +\n  labs(title = \"Rejection percentage by sex\",\n       y = \"Percent rejected\",\n       x = NULL,\n       colour = NULL) +\n  scale_y_continuous(labels = percent_format())\n\n\n\n\n\nLooking at rejections by country, it becomes clear that 1) some countries have higher rates of rejection than others and that 2) there is a time trend for some countries whereas there is none for others. Again, finding reasons for that with the data at hand is likely not possible.\n\n\nCode\neffectifs %&gt;% \n  select(date, pays_etablissement_iso3, demande_statut, total_etudiants) %&gt;% \n  group_by(date, pays_etablissement_iso3, demande_statut) %&gt;% \n  summarise(students = sum(total_etudiants)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = demande_statut, values_from = students) %&gt;% \n  mutate(reject = REFUSE/(REFUSE + VALIDE)) %&gt;% \n  ggplot(aes(date, reject, colour = pays_etablissement_iso3)) +\n  geom_line(lty = \"dotted\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, size = 0.5) +\n  facet_wrap(~ pays_etablissement_iso3, scales = \"free\") +\n  labs(title = \"Rejection percentage by country over time\",\n       y = \"Percent rejected\",\n       x = NULL,\n       colour = NULL) +\n  scale_y_continuous(labels = percent_format()) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nTo which countries to applicants go to study?\n\nThe chart below shows an upwards trend in popularity for Austria, France, Canada, Spain, Ireland, Italy and the Netherlands. On the contrary, Belgium, Switzerland, Germany and the United States have lost popularity.\n\n\nCode\neffectifs %&gt;% \n  select(date, pays_etablissement_iso3, total_etudiants) %&gt;% \n  group_by(date, pays_etablissement_iso3) %&gt;% \n  summarise(students = sum(total_etudiants)) %&gt;% \n  ungroup() %&gt;% \n  group_by(date) %&gt;% \n  mutate(students = students/sum(students)) %&gt;% \n  ggplot(aes(date, students, colour = pays_etablissement_iso3)) +\n  geom_point() +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = F, lty = \"dotted\", size = 0.5) +\n  labs(title = \"Which countries do Luxembourgish students choose for their studies?\",\n       y = \"Percentage of students\",\n       x = \"Year\") +\n  facet_wrap(~ pays_etablissement_iso3, scales = \"free\") +\n  scale_y_continuous(labels = percent_format()) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\nThe most popular countries in absolute terms can be seen below:\n\n\nCode\neffectifs %&gt;% \n  transmute(year = year(date), country = pays_etablissement_iso3,\n            total_etudiants) %&gt;% \n  group_by(year, country) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  ggplot(aes(x = students,\n             y = country %&gt;% reorder_within(students, year))) +\n  geom_col() +\n  facet_wrap(~ year, scales = \"free_y\") +\n  labs(title = \"CEDIES applications by year and country\",\n       y = NULL,\n       x = \"Students\") +\n  scale_y_reordered() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\n\nIt is also interesting to see in the chart below that many more female students go to Belgium. There are likely universities in Belgium which offer study subjects that female students choose more frequently than men.\n\n\nCode\neffectifs %&gt;% \n  transmute(year = year(date), sexe, country = pays_etablissement_iso3,\n            total_etudiants) %&gt;% \n  filter(!country %in% c(\"Pays manquant\", \"Autres pays\")) %&gt;% \n  group_by(year, sexe, country) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  mutate(students = students/sum(students)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = sexe, values_from = students) %&gt;% \n  mutate(ppts_delta = (`F` - M)*100) %&gt;% \n  ggplot(aes(ppts_delta,\n             country %&gt;% \n               reorder_within(ppts_delta, year),\n             fill = ifelse(ppts_delta &gt; 0, \"More Women\", \"More Men\"))) +\n  geom_col() +\n  labs(title = \"Countries chosen for studies by Luxembourgish students applying for\\nfinancial aid by year and sex\",\n       subtitle = \"Methodology: Total number of students by year and sex is basis for calculation.\\nCalculate percentage by year and sex going into each country, then subtract both percentages.\\nResulting metric is the delta between both sexes for each country and year in percentage points.\",\n       y = NULL,\n       x = \"ppts Difference (Women-Men)\",\n       fill = NULL) +\n  facet_wrap(~ year, scales = \"free\") +\n  theme(axis.text.y = element_text(size = 8)) +\n  scale_y_reordered() +\n  scale_x_continuous(labels = comma_format(suffix = \" ppts\")) +\n  scale_fill_manual(values = c(\"midnightblue\", \"firebrick\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5,\n                                   size = 7))\n\n\n\n\n\n\n\n\n\nWhich studies do Luxembourgish students choose?\n\nInterestingly, architecture, education, languages, medicine and anthropology are strongly losing popularity. Conversely, computer science, engineering, mathematics, health professions and psychology are strongly gaining in popularity, which is likely linked to generally higher pay in these areas.\n\n\nCode\neffectifs %&gt;% \n  select(date, domaine_formation, total_etudiants) %&gt;% \n  group_by(date, domaine_formation) %&gt;% \n  summarise(students = sum(total_etudiants)) %&gt;% \n  ungroup() %&gt;% \n  group_by(date) %&gt;% \n  mutate(students = students/sum(students)) %&gt;% \n  ggplot(aes(date, students, colour = domaine_formation)) +\n  geom_point() +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = F, lty = \"dotted\", size = 0.5) +\n  labs(title = \"Study subjects of CEDIES applicants\",\n       y = \"Percentage of students\",\n       x = \"Year\") +\n  facet_wrap(~ domaine_formation, scales = \"free\") +\n  scale_y_continuous(labels = percent_format()) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\nGenerally, most students go into business studies or health professions:\n\n\nCode\neffectifs %&gt;% \n  transmute(year = year(date), country = domaine_formation,\n            total_etudiants) %&gt;% \n  group_by(year, country) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  ggplot(aes(x = students,\n             y = country %&gt;% reorder_within(students, year))) +\n  geom_col() +\n  facet_wrap(~ year, scales = \"free_y\") +\n  labs(title = \"CEDIES students' studies by year and country\",\n       y = NULL,\n       x = \"Students\") +\n  scale_y_reordered() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\n\n\n\n\nIs there a difference in interests between female and male students?\n\nThe quick anwer: Yes. The more difficult answer would be a proper piece of research into the reasons. However, it becomes blatantly obvious that more women go into health professions, languages and education, whereas men choose engineering, business studies and computer science more often.\n\n\nCode\neffectifs %&gt;% \n  transmute(year = year(date), sexe, domaine_formation, total_etudiants) %&gt;% \n  group_by(year, sexe, domaine_formation) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  mutate(students = students/sum(students)) %&gt;%\n  ungroup() %&gt;%  \n  pivot_wider(names_from = sexe, values_from = students) %&gt;% \n  mutate(ppts_delta = (`F` - M)*100) %&gt;% \n  ggplot(aes(ppts_delta,\n             domaine_formation %&gt;% \n               reorder_within(ppts_delta, year),\n             fill = ifelse(ppts_delta &gt; 0, \"More Women\", \"More Men\"))) +\n  geom_col() +\n  labs(title = \"Study subjects chosen by Luxembourgish students applying for\\nfinancial aid by year and sex\",\n       subtitle = \"Methodology: Total number of students by year and sex is basis for calculation.\\nCalculate percentage by year and sex going into each study domain, then subtract both percentages.\\nResulting metric is the delta between both sexes for each subject and year in percentage points.\",\n       y = NULL,\n       x = \"ppts Difference (Women-Men)\",\n       fill = NULL) +\n  facet_wrap(~ year, scales = \"free\") +\n  theme(axis.text.y = element_text(size = 8)) +\n  scale_y_reordered() +\n  scale_x_continuous(labels = comma_format(suffix = \" ppts\")) +\n  scale_fill_manual(values = c(\"midnightblue\", \"firebrick\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5,\n                                   size = 7))\n\n\n\n\n\nAre there any observable trends?\n\n\nCode\neffectifs %&gt;% \n  select(date, sexe, domaine_formation, total_etudiants) %&gt;% \n  group_by(date, sexe, domaine_formation) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = sexe, values_from = students) %&gt;% \n  mutate(male = M/(`F` + M)) %&gt;% \n  select(-c(`F`, M)) %&gt;%\n  ggplot(aes(date, male, colour = domaine_formation)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Percentage of male students by study subject\",\n       y = NULL, x = NULL) +\n  geom_smooth(method = \"lm\", se = F, lty = \"dotted\", size = 0.5) +\n  labs() +\n  facet_wrap(~ domaine_formation, scales = \"free_y\") +\n  scale_y_continuous(labels = percent_format()) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nIt looks like the percentage of male students decreases in the majority of subjects. Are men moving out of higher education?\n\n\nCode\neffectifs %&gt;% \n  select(date, sexe, total_etudiants) %&gt;% \n  group_by(date, sexe) %&gt;% \n  summarise(students = sum(total_etudiants, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = sexe, values_from = students) %&gt;% \n  mutate(male = M/(M + `F`)) %&gt;% \n  ggplot(aes(date, male)) +\n  geom_line() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, lty = \"dotted\", size = 0.5) +\n  labs(title = \"Percentage of male students among CEDIES applicants\",\n       y = NULL,\n       x = NULL) +\n  scale_y_continuous(labels = percent_format())\n\n\n\n\n\nIf you made it until here, thank you very much for reading and I hope you enjoyed this post. Please feel free to reach out.\n\n\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20221007-Stacking-Models-Airbnb/index.html",
    "href": "posts/20221007-Stacking-Models-Airbnb/index.html",
    "title": "Stacking Models To Predict Airbnb Prices In Manhattan (NYC)",
    "section": "",
    "text": "What are we looking at?\n\nThe data comes from Kaggle, the biggest online platform for machine learning enthusiasts hosting datasets and competitions around data science. More precisely, the data set was part of season 1 episode 5 of SLICED, a data science competition streamed by Nick Wan and Meg Risdal on Twitch.\nThis dataset is about the prices of Airbnb listings in New York City. The purpose of this post is to demonstrate the usefulness of the package stacks in blending individual machine learning models together into a linear combination of them, often increasing final model performance.\n\n\n\n\nData Cleaning\n\nFirstly, I start by loading the data. The first file is the one to be used for training, whereas the holdout will only be used for submission of out-of-sample predictions, as it doesn’t contain the target variable. The training data is fairly large, holding information on 34,226 listings with 15 predictors and the response variable of the listings price per night.\n\nnames(data)\n\n [1] \"id\"                             \"name\"                          \n [3] \"host_id\"                        \"host_name\"                     \n [5] \"neighbourhood_group\"            \"neighbourhood\"                 \n [7] \"latitude\"                       \"longitude\"                     \n [9] \"room_type\"                      \"price\"                         \n[11] \"minimum_nights\"                 \"number_of_reviews\"             \n[13] \"last_review\"                    \"reviews_per_month\"             \n[15] \"calculated_host_listings_count\" \"availability_365\"              \n\n\nThere exists considerable missing data in the reviews per month (double) and last review (date) column and little missing data in the host name (character) and listing name (character) column. All missing values can likely be imputed, that is if they are missing at random.\n\ncolMeans(is.na(data)) %&gt;% \n  tidy() %&gt;% \n  rename(pct = x) %&gt;% \n  mutate(names = fct_reorder(names, pct)) %&gt;% \n  # filter(pct &gt; 0) %&gt;% \n  ggplot(aes(pct, names)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Missing Data In Variables\",\n       subtitle = \"Percent missingness calculated for each column\",\n       y = NULL,\n       x = NULL) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\nThe target variable distributions of missing and non-missing values in the two columns with considerable missingness look like they don’t exhibit statistically significant differences and are comparable. Therefore, the imputation of these missing values should not pose a problem, even though this conclusion has to be taken with a grain of salt: It is not possible to determine for sure if a variable is missing at random with observed data, it can merely be assumed.\n\ndata %&gt;% \n  transmute(reviews_per_month = ifelse(is.na(reviews_per_month),\n                                       \"missing\",\n                                       \"not missing\"),\n            last_review = ifelse(is.na(last_review),\n                                 \"missing\",\n                                 \"not missing\"),\n            price) %&gt;%\n  pivot_longer(-c(price), names_to = \"variable\", values_to = \"state\") %&gt;% \n  ggplot(aes(variable, price, fill = state)) +\n  geom_boxplot(outlier.alpha = 0.2) +\n  labs(y = \"Price\",\n       x = NULL,\n       fill = \"Status:\") +\n  ggsci::scale_fill_locuszoom() +\n  scale_y_log10(labels = scales::dollar_format()) +\n  theme_bw()\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\nThe next step is to walk through the available predictors and understand relations to the target variable. Below, every variable is briefly looked at and presented, enabling a better understanding of the complete training data.\nIf you are just interested in how to build a model stack with stacks, feel free to skip this part and continue at Building And Training The Stacked Model.\n\n\nid: unique identifier\nThe unique identifier column for each listing is a random number and should not hold any predictive power.\n\ncor(data$price, data$id)\n\n[1] 0.009927947\n\n\n\n\n\nname: name of the listing\nThe name variable contains the title of the listing, which will be useful for tokenisation at a later stage.\n\ndata %&gt;% count(name, sort = T)\n\n# A tibble: 33,705 × 2\n   name                                           n\n   &lt;chr&gt;                                      &lt;int&gt;\n 1 Hillside Hotel                                15\n 2 New york Multi-unit building                  11\n 3 Home away from home                           10\n 4 Private room                                  10\n 5 Private Room                                   9\n 6 &lt;NA&gt;                                           9\n 7 Artsy Private BR in Fort Greene Cumberland     8\n 8 Loft Suite @ The Box House Hotel               8\n 9 Brooklyn Apartment                             6\n10 Harlem Gem                                     6\n# ℹ 33,695 more rows\n\n\n\n\n\nhost_id: unique identifier for the host of the listing\nThe unique identifier column for each listing is a random number and should not contain any predictive power in theory. However, with multiple listings per host and some hosts specialising in luxury apartments or affordable housing, for instance, there might be additional insights in the variable. Therefore, it will still be included in the recipe.\n\ncor(data$price, data$host_id)\n\n[1] 0.01191348\n\n\n\n\n\nhost_name: name of the host\nThe host name variable contains the first names of the hosts. There is likely no deterministic component to them except for known hosts, like Blueground, who specialise in renting out furnished appartments for longer periods. Therefore, it will not be used as a predictive variable, as the relation might be too fragile for the out-of-sample application of the model.\n\ndata %&gt;% \n  group_by(host_name) %&gt;% \n  summarise(n = n(),\n            mean_price = mean(price)) %&gt;% \n  filter(n &gt; 100) %&gt;% \n  mutate(host_name = paste0(host_name, \" (N=\", n, \")\") %&gt;% \n           fct_reorder(mean_price)) %&gt;% \n  ggplot(aes(mean_price, host_name, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  labs(title = \"Mean NYC Airbnb Prices By Host Names\",\n       subtitle = \"Only names with observation count &gt;100 are shown\",\n       x = \"Mean Price Per Night\",\n       y = NULL,\n       size = \"Observation Count:\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_continuous(labels = scales::comma_format(),\n                        range = c(2,5)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nneighbourhood_group: borough where the listing is located (e.g., “Manhattan”)\nThere are only five neighbourhood groups. However, there are very large differences in the price distributions between them, so they will be particulary useful as nominal predictors.\n\ndata %&gt;% \n  ggplot(aes(neighbourhood_group %&gt;% fct_reorder(price),\n             price,\n             fill = neighbourhood_group)) +\n  geom_boxplot(show.legend = F, outlier.alpha = 0.5, alpha = 0.6) +\n  labs(title = \"NYC Airbnb Price Distribution By Neighbourhood Group\",\n       subtitle = NULL,\n       x = NULL,\n       y = \"Price Per Night\") +\n  scale_y_log10(labels = scales::dollar_format()) +\n  ggsci::scale_fill_locuszoom() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\ndata %&gt;% \n  group_by(neighbourhood_group) %&gt;% \n  summarise(n = n(),\n            mean_price = mean(price)) %&gt;% \n  mutate(neighbourhood_group = paste0(neighbourhood_group, \" (N=\", n, \")\") %&gt;% \n           fct_reorder(mean_price)) %&gt;% \n  ggplot(aes(mean_price, neighbourhood_group, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  labs(title = \"Mean NYC Airbnb Prices By Neighbourhood Group\",\n       subtitle = NULL,\n       x = \"Mean Price Per Night\",\n       y = NULL,\n       fill = \"Observation Count:\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_continuous(labels = scales::comma_format(),\n                        range = c(2,5)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nneighbourhood: neighborhood where the listing is located (e.g., “East Harlem”)\nNeighbourhoods, similarly to neighbourhood groups, are useful indicators for prices of Airbnbs in NYC. However, due to the high cardinality, they will have to be lumped together in order not to exceed the memory limits of my machine.\n\ndata %&gt;% \n  group_by(neighbourhood) %&gt;% \n  summarise(n = n(),\n            mean_price = mean(price)) %&gt;% \n  mutate(neighbourhood = paste0(neighbourhood, \" (N=\", n, \")\") %&gt;% \n           fct_reorder(mean_price)) %&gt;% \n  slice_max(order_by = n, n = 25) %&gt;% \n  ggplot(aes(mean_price, neighbourhood, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  labs(title = \"Mean NYC Airbnb Prices By Neighbourhood\",\n       subtitle = \"Only the top 25 most frequent neighbourhoods are shown.\",\n       x = \"Mean Price Per Night\",\n       y = NULL,\n       size = \"Observation Count:\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_continuous(labels = scales::comma_format(),\n                        range = c(2,5)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nlatitude: latitude of the listing location\nLatitude and longitude give information about the location of the listings. This enables me to make a map. Again, it looks like Manhattan is the most expensive place to rent an Airbnb, which will be useful for the model.\n\ndata %&gt;% \n  ggplot(aes(longitude, latitude, z = price)) +\n  stat_bin_hex(bins = 100) +\n  labs(title = \"Hexagon Plot Of Log NYC Airbnb Prices\",\n       subtitle = \"Longitude and latitude are binned into 100 hexagons.\",\n       x = NULL,\n       y = NULL,\n       fill = NULL) +\n  coord_equal() +\n  scale_alpha_continuous(range = c(0, 1), trans = \"log\") +\n  scale_fill_gradient(low = \"azure3\", high = \"red\") +\n  theme_void() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nlongitude: longitude of the listing location\nSee above.\n\n\n\nroom_type: type of room (‘Entire home/apt’, ‘Private room’, or ‘Shared room’)\nThere are three room types. Entire homes/appartments are obviously the most expensive. Given the disparity between the groups, these nominal predictors will be able to explain a lot of variance in the target variable.\n\ndata %&gt;%\n  group_by(room_type) %&gt;% \n  summarise(n = n(),\n            mean_price = mean(price)) %&gt;% \n  mutate(room_type = paste0(room_type, \" (N=\", n, \")\") %&gt;% \n           fct_reorder(mean_price)) %&gt;% \n  slice_max(order_by = n, n = 25) %&gt;% \n  ggplot(aes(mean_price, room_type, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  labs(title = \"Mean NYC Airbnb Prices By Room Type\",\n       subtitle = NULL,\n       x = \"Mean Price Per Night\",\n       y = NULL,\n       size = \"Observation Count:\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_continuous(labels = scales::comma_format(),\n                        range = c(2,5)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nThe combination of neighbourhood group and room type looks interesting for prediction. Staten Island apartments are relatively expensive in comparison to private rooms, while shared rooms in the Bronx rank higher relatively than the other two categories\n\ndata %&gt;% \n  group_by(neighbourhood_group, room_type) %&gt;% \n  summarise(mean_price = mean(price),\n            n = n()) %&gt;% \n  arrange(-mean_price) %&gt;% \n  ggplot(aes(neighbourhood_group %&gt;% fct_reorder(mean_price),\n             mean_price, \n             fill = neighbourhood_group)) +\n  geom_col() +\n  facet_wrap(~ room_type) +\n  labs(title = \"Mean NYC Airbnb Prices By Room Type and Neighbourhood Group\",\n       subtitle = NULL,\n       y = \"Mean Price Per Night\",\n       x = \"Neighbourhood Group\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  ggsci::scale_fill_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\nprice: cost for one night booking of the listing\nThe target is log-normally distributed, hence a log transform would be appropriate for a linear model, for instance. SLICED using RMSLE as a metric for evaluation, the target variable will transformed in the XGBoost model as well in this case, in order for the on-board RMSE metric of the Tidymodels package to apply (just a decision of convenience in this case).\n\ndata %&gt;% \n  ggplot(aes(price)) +\n  geom_histogram(fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Distribution Of NYC Airbnb Listing Prices\",\n       subtitle = NULL,\n       y = \"Frequency\",\n       x = \"Price Per Night\") +\n  scale_x_log10(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nminimum_nights: minimum number of nights required to book the listing\nCreating 100 bins for minimum nights, taking the average price per night by percentile and plotting the correlation in a scatter plot reveals that there might be a positive effect of minimum nights on listing price. This has likely to do with apartment being more expensive than private rooms and apartments being more likely of having a contractual obligation of minimum stay.\n\ndata %&gt;% \n  mutate(minimum_nights_ntile = ntile(minimum_nights, n = 100)) %&gt;% \n  group_by(minimum_nights_ntile) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(minimum_nights_ntile, mean_price)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Correlation Of Minimum Nights And NYC Airbnb Price\",\n       subtitle = \"Minimum nights have been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Minimum Nights Percentiles\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nnumber_of_reviews: number of reviews the listing has\nThe number of reviews shows a slightly negative effect.\n\ndata %&gt;% \n  mutate(ntile = ntile(number_of_reviews, n = 100)) %&gt;% \n  group_by(ntile) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(ntile, mean_price)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Correlation Of Number Of Reviews And NYC Airbnb Price\",\n       subtitle = \"Number of reviews have been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Number of Reviews Percentiles\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nlast_review: date the last review of the listing was made\nThis variable will probably generate more interpretable value for the model if I transform it into a days since last review from today’s perspective. This will have to be done in the recipe, in order for it to equally be applied to the holdout set as well.\nThere does not seem to be any relation between days since last review and price. If anything, more recently reviewed Airbnbs have slightly higher prices, but the confidence bands don’t show a large statistically significant effect.\n\ndata %&gt;% \n  mutate(ntile = ntile(last_review, n = 100)) %&gt;% \n  group_by(ntile) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(ntile, mean_price)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Correlation Of Days Since Last Review And NYC Airbnb Price\",\n       subtitle = \"Days since last review have been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Days Since Last Review Percentiles\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nreviews_per_month: number of reviews the listing gets per month on average\nThere seems to be a slightly negative relation between reviews per month and price.\n\ndata %&gt;% \n  mutate(ntile = ntile(reviews_per_month, n = 100)) %&gt;% \n  group_by(ntile) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(ntile, mean_price)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Correlation Of Reviews Per Month And NYC Airbnb Price\",\n       subtitle = \"Reviews per month have been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Reviews Per Month Percentiles\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\ncalculated_host_listings_count: number of listing the host has\nIt seems like hosts with the highest number of listings, that is professional hosts, likely have higher room prices on average, even though that might be a non-significant outlier.\nThe binned averages show a non-linear trend downwards and a sudden increase in the highest ten percent for both entire homes and private rooms. However, this trend is reversed for shared rooms. This means, that the hosts with exceptionally many listings likely have expensive homes or private rooms, but likely cheaper shared rooms. I wonder if the first two are a reflection of a dominant position of larger firms in the market, or whether it has something to do with the quality of the offerings. For the shared apartments, I wonder whether it has something to do with subsidised housing or larger firms specialising in affordable rooms and owning entire buildings of rooms. Either way, very interesting.\n\ndata %&gt;% \n  mutate(ntile = ntile(calculated_host_listings_count, n = 100)) %&gt;% \n  group_by(ntile, room_type) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(ntile, mean_price, colour = room_type)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  labs(title = \"Correlation Of Listings Per Host And NYC Airbnb Price\",\n       subtitle = \"Listings per host have been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Listings Per Host (Percentiles)\",\n       colour = \"Room Type\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) + \n  ggsci::scale_colour_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\navailability_365: number of days out of the year the listing is available\nAgain, very interesting to observe that an interaction between room type and another variable exists. For private rooms, availability throughout the year has virtually no effect on price, whereas more available shared rooms are usually cheaper and more available entire apartments and houses are the most expensive. I believe that the latter might be due to location, as most touristy places are available all year around, therefore attracting people with concentrated spending power over shorter time periods. These offerings also have to compensate for the time being empty, if no tourists are around.\nAnyway, this being speculation, let’s get into the thick of it and continue with correlation analysis for the numeric predictors.\n\ndata %&gt;% \n  mutate(ntile = ntile(availability_365, n = 100)) %&gt;% \n  group_by(ntile, room_type) %&gt;% \n  summarise(mean_price = mean(price)) %&gt;% \n  ggplot(aes(ntile, mean_price, colour = room_type)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  labs(title = \"Correlation Of Availability And NYC Airbnb Price\",\n       subtitle = \"Availability per year has been binned into 100 percentiles and averaged.\",\n       y = \"Mean Price Per Night\",\n       x = \"Availability Per Year (Percentiles)\",\n       colour = \"Room Type\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) + \n  ggsci::scale_colour_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nProceeding with correlations\nLet’s take a look at the correlation matrix from the GGally package to gauge relations of numeric predictors with the target variable.\n\ndata %&gt;% \n  mutate(last_review = difftime(Sys.time(), last_review) %&gt;% as.numeric()) %&gt;% \n  select_if(is.numeric) %&gt;% \n  select(-price, price) %&gt;% \n  drop_na() %&gt;% \n  ggcorr(label = T, label_size = 3)\n\n\n\n\nIt does not look like numeric variables are good predictors of the price in this setting. However, the correlation only being a linear measure, it might well be that a non-linear machine learning model will figure out non-linear relationships that are hidden right now.\nProceeding to look at variance inflation factors:\n\ndata %&gt;%\n  select_if(is.numeric) %&gt;% \n  lm(formula = price ~ .) %&gt;% \n  vif()\n\n                            id                        host_id \n                      2.164903                       1.654317 \n                      latitude                      longitude \n                      1.011668                       1.067277 \n                minimum_nights              number_of_reviews \n                      1.039035                       2.322328 \n             reviews_per_month calculated_host_listings_count \n                      2.297050                       1.082071 \n              availability_365 \n                      1.141782 \n\n\nVIFs are lower than 10 for all numeric variables, so there is no problem of multicollinearity that has to be dealt with or at least mentioned.\nWith the ideas gathered from the exploratory data analysis, I can now proceed with building the model.\n\n\n\n\n\nBuilding And Training The Stacked Model\n\nFirst, the data is split into training and testing sets. Also, three-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.\n\ndt_split &lt;- data %&gt;% \n  mutate(price = log(price + 1)) %&gt;% \n  initial_split(strata = \"price\")\n\ndt_train &lt;- training(dt_split)\ndt_test &lt;- testing(dt_split)\n\nfolds &lt;- vfold_cv(dt_train, v = 3, strata = \"price\")\n\nThe recipe in the tidymodels framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit.\n\ngb_rec &lt;- recipe(price ~ .,\n                 data = dt_train) %&gt;%\n  step_rm(host_name) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_tokenize(name) %&gt;%\n  step_stopwords(name) %&gt;%\n  step_tokenfilter(name, max_tokens = 40) %&gt;%\n  step_tf(name) %&gt;%\n  step_mutate(last_review = difftime(Sys.time(), last_review) %&gt;%\n                as.numeric()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_other(neighbourhood, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\n\nen_rec &lt;- recipe(price ~ .,\n                 data = dt_train) %&gt;%\n  step_rm(host_name) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_tokenize(name) %&gt;%\n  step_stopwords(name) %&gt;%\n  step_tokenfilter(name, max_tokens = 40) %&gt;%\n  step_tf(name) %&gt;%\n  step_mutate(last_review = difftime(Sys.time(), last_review) %&gt;%\n                as.numeric()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;% \n  step_other(neighbourhood, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nSetting up the model specifications with tuning options for hyperparameters:\n\ngb_spec &lt;- \n  boost_tree(\n    trees = 1000,\n    tree_depth = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    mtry = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nen_spec &lt;- linear_reg(penalty = tune(),\n                      mixture = tune()) %&gt;% \n  set_engine(\"glmnet\")\n\nIn the model specification, you can specify the variable importance, which is calculated based on impurity in this case. Proceeding with setting up the workflow:\n\ngb_wflow &lt;- \n  workflow() %&gt;% \n  add_recipe(gb_rec) %&gt;% \n  add_model(gb_spec)\n\nen_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(en_rec) %&gt;% \n  add_model(en_spec)\n\nSetting up a space-filling design for time-efficient hyperparameter tuning:\n\ngb_grid &lt;- \n  grid_latin_hypercube(\n    tree_depth(),\n    min_n(),\n    loss_reduction(),\n    sample_size = sample_prop(),\n    finalize(mtry(), dt_train),\n    learn_rate(),\n    size = 50\n  )\n\nen_grid &lt;- \n  grid_latin_hypercube(\n    penalty(),\n    mixture(),\n    size = 100\n    )\n\nNow, the hyperparameters can be trained with parallel computing in order to utilise more available computing power.\n\n# Gradient Boosting\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\ngb_tune &lt;- tune_grid(object = gb_wflow,\n                     resamples = folds,\n                     grid = gb_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\nTime difference of 12.00451 mins\n\n# Elastic Net\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\nen_tune &lt;- tune_grid(object = en_wflow,\n                     resamples = folds,\n                     grid = en_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\nTime difference of 1.425809 mins\n\n\nLooking at the tuning results reveals that the model captures strong signal in the predictors, as the \\(R^2\\) is fairly high.\n\ngb_tune %&gt;% \n  show_best(metric = \"rsq\") %&gt;%\n  transmute(model = \"XGBoost\", .metric, mean, n, std_err)\n\n# A tibble: 5 × 5\n  model   .metric  mean     n std_err\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 XGBoost rsq     0.597     3 0.00499\n2 XGBoost rsq     0.590     3 0.00622\n3 XGBoost rsq     0.589     3 0.00563\n4 XGBoost rsq     0.572     3 0.00557\n5 XGBoost rsq     0.561     3 0.00424\n\nen_tune %&gt;% \n  show_best(metric = \"rsq\") %&gt;% \n  transmute(model = \"Elastic Net\", .metric, mean, n, std_err)\n\n# A tibble: 5 × 5\n  model       .metric  mean     n std_err\n  &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Elastic Net rsq     0.532     3 0.00559\n2 Elastic Net rsq     0.532     3 0.00557\n3 Elastic Net rsq     0.532     3 0.00544\n4 Elastic Net rsq     0.532     3 0.00544\n5 Elastic Net rsq     0.532     3 0.00554\n\n\nBefore creating a stacked model, let’s take a look at the variable importance within both individual models.\n\ngb_final_wflow &lt;- gb_wflow %&gt;%\n  finalize_workflow(select_best(gb_tune, metric = \"rmse\"))\n\ngb_final_fit &lt;- gb_final_wflow %&gt;% \n  last_fit(dt_split)\n\ngb_final_fit %&gt;%\n  pluck(\".workflow\", 1) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vi() %&gt;%\n  slice_max(order_by = Importance, n = 20) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance))) +\n  geom_col(fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = \"Only the most important predictors are shown.\",\n       y = \"Predictor\",\n       x = \"Relative Variable Importance\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\nFor the XGBoost model, the type of room as well as the location, especially information about Manhattan, was important to predict price. It becomes visible now, how important the inclusion of nominal predictors was for model performance.\n\nen_final_wflow &lt;- en_wflow %&gt;%\n  finalize_workflow(select_best(en_tune, metric = \"rmse\"))\n\nen_final_fit &lt;- en_final_wflow %&gt;% \n  last_fit(dt_split)\n\nen_final_fit %&gt;%\n  pluck(\".workflow\", 1) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vi() %&gt;% \n  slice_max(order_by = Importance, n = 30) %&gt;% \n  mutate(Importance = ifelse(Sign == \"NEG\", Importance * -1, Importance)) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance),\n             fill = Sign)) +\n  geom_col(colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = \"Only the most important predictors are shown.\",\n       y = \"Predictor\",\n       x = \"Coefficient\") +\n  ggsci::scale_fill_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nFor the elastic net, interestingly, the neighbourhoods were very decisive. Only the 30 most important variables are shown, and most of them contain information on geographic location from the neighbourhood variable. Furthermore, most of the important variables negatively impact price.\nWith both these individual tuning results, a blended (“stacked”) model can easily be built with the stacks package.\n\nblended_gb_en &lt;- stacks() %&gt;% \n  add_candidates(gb_tune) %&gt;% \n  add_candidates(en_tune) %&gt;% \n  blend_predictions()\n\nblended_gb_en\n\n# A tibble: 7 × 3\n  member        type            weight\n  &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;\n1 gb_tune_1_16  boost_tree 1077472.   \n2 gb_tune_1_28  boost_tree   72822.   \n3 gb_tune_1_23  boost_tree    1893.   \n4 en_tune_1_085 linear_reg       6.98 \n5 gb_tune_1_47  boost_tree       0.619\n6 gb_tune_1_36  boost_tree       0.228\n7 gb_tune_1_41  boost_tree       0.122\n\n\nThe stacks package creates a model additively blending the predictions from the separately trained models before. The optimisation for this is built into the package shows the output above. Interestingly, no elastic net candidate was chosen. Instead, a linear combination of XGBoost models is selected. In order to proceed with the prediction on the final holdout set, the stack is now fitted onto the training data.\n\nblended_gb_en &lt;- blended_gb_en %&gt;% \n  fit_members()\n\n\n\n\n\nEvaluating Model Performance On The Training Data\n\nUsing the fitted model to predict and evaluate on the test set:\n\nblended_gb_en %&gt;% \n  predict(dt_test) %&gt;% \n  bind_cols(dt_test %&gt;% select(price)) %&gt;% \n  rsq(.pred, truth = price) %&gt;% \n  mutate(model = \"Stacked Model\")\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        \n1 rsq     standard       0.614 Stacked Model\n\n\nSuccess! The blended model stack attained an \\(R^2\\) slightly higher than the individual XGBoost model on the test data set. This goes to show how stacking individual models can give the final predictions an additional edge.\n\ngb_final_fit %&gt;% \n  extract_workflow() %&gt;% \n  predict(dt_test) %&gt;% \n  bind_cols(dt_test %&gt;% select(price)) %&gt;% \n  rsq(.pred, truth = price) %&gt;% \n  mutate(model = \"XGBoost Model\")\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        \n1 rsq     standard       0.612 XGBoost Model\n\n\n\nblended_gb_en %&gt;% \n  predict(dt_test) %&gt;% \n  bind_cols(dt_test %&gt;% select(price)) %&gt;% \n  ggplot(aes(exp(price), exp(.pred))) +\n  geom_point(colour = \"midnightblue\", alpha = 0.4) +\n  geom_abline(lty = \"dashed\", colour = \"grey50\") +\n  scale_x_log10(labels = scales::dollar_format()) +\n  scale_y_log10(labels = scales::dollar_format()) +\n  labs(title = \"Out-Of-Sample Fit Of The Blended Model\",\n       subtitle = NULL,\n       y = \"Prediction\",\n       x = \"Truth\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nEvaluating Model Performance on the Prediction Data\n\nWith the trained stack model, I can now make predictions for the holdout dataset, which will be submitted to the leader board on Kaggle.\n\nblended_gb_en %&gt;% \n  predict(holdout) %&gt;% \n  bind_cols(holdout %&gt;% select(id)) %&gt;% \n  transmute(id, price = exp(.pred))\n\n# A tibble: 14,669 × 2\n         id price\n      &lt;dbl&gt; &lt;dbl&gt;\n 1 10449807 255. \n 2  1178389 130. \n 3 23838063  61.3\n 4 14415799 143. \n 5  6555262  66.0\n 6 33782555  66.4\n 7  6669443  87.1\n 8 36306938  57.6\n 9 12814195  78.0\n10 10100799 176. \n# ℹ 14,659 more rows\n\n\nThis model ranks at 5/30 on the SLICED competition leader board, which I believe speaks volumes about the power of XGBoost and stacking in competition settings given the lack of new features and extensive tuning in this post.\nConclusively, it can be said that the models performed fairly well in fitting the data, even though the predictions are not highly impressive seen from an absolute perspective. In order to make them highly accurate, more information on the level of the listings would have been useful, for instance size of the rooms, capacity, proxies for luxuriousness, details on reviews and information on amenities.\nI hope this post has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out.\nThank you for reading.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220908-Swiss-End-User-Electricity-Consumption/index.html",
    "href": "posts/20220908-Swiss-End-User-Electricity-Consumption/index.html",
    "title": "Swiss End-User Electricity Consumption",
    "section": "",
    "text": "Description of the Data\n\nThe data comes from Swissgrid, who regularly update their total end-user energy consumption data sets for the Switzerland.\nTo show only the results, large code chunks are hidden, but can be unfolded by clicking the “Code” boxes on the top right of each hidden code chunk.\n\n\n\nData Cleaning\n\nThe annual files were only merged to one. No additional data preparation is needed.\n\n\n\nHow Did The Pandemic Affect Swiss End-User Energy Consumption?\n\n\n\nCode\n# Aggregate to monthly values\nelec &lt;- electricity %&gt;%\n  mutate(date = as.character(substr(Zeitpunkt,1,10)),\n         year = as.character(substr(Zeitpunkt,1,4)),\n         month_in_year = month(Zeitpunkt)) %&gt;%\n  group_by(year, month_in_year) %&gt;%\n  summarise(kWh = sum(kWh),\n            month_in_year = last(month_in_year),\n            year = last(year)) %&gt;%\n  head(-1)\n\nelec %&gt;%\n  mutate(kWh = kWh/1000000) %&gt;% \n  ggplot(aes(x = month_in_year, y = kWh, color = as.factor(year))) +\n  geom_line(size = 0.7) +\n  labs(title = \"The effect of the COVID-19 pandemic on total energy consumption in Switzerland\",\n       subtitle = \"End user electricity consumption in the Swiss controlblock by month (in millions of kWh)\",\n       x = \"Calendar Month Number\",\n       y = \"kWh (in millions)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                   size = 10),\n        legend.title = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        axis.title = element_text(size = 10)) +\n  scale_colour_manual(values = c(rep(\"gray75\", 11), \"red\", \"firebrick\", \"orange\")) +\n  scale_x_continuous(breaks = seq(1,12,1), limits = c(1,12)) +\n  scale_y_continuous(labels = scales::comma_format())\n\n\n\n\n\nAggregating the data to monthly values and overlaying them for comparison reveals that the pandemic greatly impacted total energy consumption, especially during the first lockdown.\n\n\n\nAre There Daily Patterns Within Each Week?\n\n\n\nCode\nelec &lt;- electricity %&gt;% \n  mutate(hour = hour(Zeitpunkt),\n         weekday = wday(Zeitpunkt, label = TRUE)) %&gt;% \n  group_by(hour, weekday) %&gt;% \n  summarise(mean_c = mean(kWh)) %&gt;% \n  mutate(weekday = ordered(weekday, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\",\n                                               \"Fri\", \"Sat\", \"Sun\")))\nelec %&gt;% \n  ggplot(aes(x = hour+1, y = weekday)) +\n  geom_tile(aes(fill = mean_c), width=0.95, height=0.95) +\n  labs(title = \"Swiss End-User Electricity Consumption: Hourly Usage Patterns Throughout Weekdays\",\n       subtitle = \"Averages are formed based on historical data from the period 2009-2022\",\n       y = \"Weekday\",\n       x = \"Hour In The Day (Starting At Midnight)\",\n       fill = \"Mean Consumption:\") +\n  scale_x_continuous(breaks = 1:24,  expand = c(0, 0)) +\n  scale_fill_distiller(type = \"seq\", direction = 1,\n                       labels = scales::comma_format(suffix = \" kWh\")) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        axis.title = element_text(size = 10))\n\n\n\n\n\nFrom the heat map above, it can be seen that peaks take place during weekday business hours, whereas the weekend looks milder. It can be concluded that the industry has a very high impact on total energy consumption.\n\n\nAre Holiday Lows Visible?\n\n\n\nCode\nelectricity %&gt;% \n  mutate(day = yday(Zeitpunkt),\n         year = year(Zeitpunkt)) %&gt;% \n  group_by(day) %&gt;% \n  summarise(mean_kWh = mean(kWh),\n            lower_kWh = quantile (kWh, 0.1),\n            higher_kWh = quantile(kWh, 0.9)) %&gt;% \n  ggplot(aes(day, mean_kWh)) +\n  geom_line(colour = \"dodgerblue\", size = 0.75) +\n  geom_ribbon(aes(ymin = lower_kWh, ymax = higher_kWh),\n              colour = \"dodgerblue\", fill = \"dodgerblue\",\n              alpha = 0.5) +\n  labs(title = \"Daily Averages With Confidence Levels (10th And 90th Percentile)\",\n       subtitle = \"Averages are formed based on historical data from the period 2009-2022\",\n       y = \"kWh\",\n       x = \"Day In The Year\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        axis.title = element_text(size = 10))\n\n\n\n\n\nNot only the mean, but also the confidence bands reveal a strong intra-year pattern, which in itself carries another weekly pattern shown in the heat map above. The lows during summer and Christmas holidays are considerable.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220716-StPetersburg-Dilemma/index.html",
    "href": "posts/20220716-StPetersburg-Dilemma/index.html",
    "title": "The St. Petersburg Paradox",
    "section": "",
    "text": "What Is The St. Petersburg Paradox?\n\n\nA casino offers a game of chance for a single player in which a fair coin is tossed at each stage. The initial stake begins at 2 dollars and is doubled every time heads appears. The first time tails appears, the game ends and the player wins whatever is in the pot. Thus the player wins 2 dollars if tails appears on the first toss, 4 dollars if heads appears on the first toss and tails on the second, 8 dollars if heads appears on the first two tosses and tails on the third, and so on. (Source)\n\nSimple enough. Toss a coin until tails appears for the first time, then stop. For k heads that appeared until that last toss, where tails appeared, you get \\(2^k\\) dollars.\nFor instance, one game could be: Heads - Heads - Heads - Tails. Congratulations, you just won \\(2^3 = 8\\) dollars. However, it could also go: Tails. Tough luck, that’s \\(2^0 = 0\\) dollars.\nHow much would you be willing to pay to enter this game? Or more generally: What would be a fair price to pay the casino for entering the game?\nInterestingly, most people go with very low amounts in the single or double digits. And we’re not to blame for realistically/pessimistically estimating that a 50/50 coin toss will likely not bless us with long streaks of heads. However, the payout in this game grows with a power of 2. The latter is exactly, why we should pay any price for this game.\n\n\n\nThe Theoretical Answer\n\nMathematically, the player wins \\(2^k\\) dollars, where \\(k\\) is the toss at which tails shows for the first time. The multiplied chances of each side of the coin shrink symmetrically to the square power of the pay-off sum, hence the expected value of each next coin toss is 1, growing infinitely.\nPurely speaking of expected values, you should be willing to pay any infinitely large sum to enter this game, because the potential of the game’s pay-off is infinite.\nDid you ever hear of consecutive coin tosses revealing the same side hundreds, or millions or infinitely many times though? If the answer is no, then the problem with the St. Petersburg lottery becomes clear: Infinity is very large.\nLet’s look at a simulation of a person dedicating their life to this game.\n\n\n\nSimulating A Lifetime Of Games In The St. Petersburg Lottery\n\nLet’s assume a 20 year old person, who has 65 more years to live. Assuming 250 working days per year, the person has 16,250 days to play the game for 8h a day (full-time job), which results in 130,000 work hours. Assuming an average of 1 game per minute (60 games per hour), this person will be able to play 7,800,000 games over their lifetime.\nSimulating 7,800,000 games:\n\n\nCode\n#params\nsimulations = 1\ngames = 7800000\nfirst_tail &lt;- matrix(ncol = simulations, nrow = games)\npayoffs &lt;- matrix(ncol = simulations, nrow = games)\n\n#simulation\nfor (i in 1:simulations){\n  \n  for (x in 1:games){\n    \n    index = 0\n    #head = 0, tail = 1\n    while (sample(x = c(0,1), size = 1, replace = TRUE) == 0){\n    \n      index = index + 1\n      \n    }\n    \n    #populate matrices\n    first_tail[x,i] = index + 1\n    payoffs[x,i] = 2^(index + 1)\n  \n  }\n  \n}\n\n\n\n\nCode\npayoffs &lt;- payoffs %&gt;% \n  as_tibble() %&gt;% \n  rename(payoff = V1)\n\npayoffs %&gt;% \n  count(payoff) %&gt;%\n  mutate(payoff = scales::dollar(payoff) %&gt;% \n           as.factor %&gt;% \n           fct_reorder(payoff, .desc = T)) %&gt;% \n  ggplot(aes(n, payoff)) +\n  geom_col(fill = \"dodgerblue\", colour = \"white\") +\n  labs(y = \"Payoff\",\n       x = \"Frequency (Log10 scale)\") +\n  scale_x_log10(labels = scales::comma_format()) +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\npaste(\"Total Winnings:\", scales::comma(sum(payoffs)))\n\n\n[1] \"Total Winnings: 201,710,954\"\n\n\nIt becomes clear, that the winnings from the lottery become large and the total winnings over a lifetime do look impressive. However, if you had paid an unfathomably large sum, say 100 trillion to enter each game, these winnings wouldn’t even cover a fraction of your loss.\n\n\nCode\naverage_winnings &lt;- payoffs\naverage_winnings &lt;- cumsum(average_winnings)/(1:nrow(average_winnings))\n\naverage_winnings %&gt;% \n  as_tibble() %&gt;% \n  mutate(game = 1:nrow(.)) %&gt;% \n  ggplot(aes(game, payoff)) +\n  geom_line() +\n  labs(title = \"The Expected Payoff Over A Lifetime Of Games\",\n       subtitle = \"The cumulative average payoff reveals no convergence.\",\n       y = \"Running Average Payoff Per Game\",\n       x = \"Games\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\n\nLooking at the average winnings per game to approximate an expected value per game reveals that there exists no convergence and that the mean exhibits erratic jumps, very rarely. These are outliers, large sums won, which reflect the potential for infinite wins in this lottery. However, they are still not large enough to make a difference, even over an entire lifetime. Despite all this, the possibility remains. From a frequentist perspective however, you’re most likely be dead long before seeing wins large enough to justify the infinite price.\n\n\n\nThe Catch\n\nThe St. Petersburg Paradox is the perfect example to demonstrate how large “infinity” really is. Very large numbers, for instance a lifetime of games for a young person, are not even close to large enough to even begin to exploit the exponential potential of the St. Petersburg lottery, on average. Therefore, the implication of a theoretically infinite entry price to the game does not extend to our very finite lifetime and the entry price paid in real life should realistically lie in the low double digits to avoid catastrophic amounts of debt. Then, the next problem lies in finding someone who will play this game with you for the rest of your lives.\nThanks for reading!\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220630-Moving-Averages-Trading/index.html",
    "href": "posts/20220630-Moving-Averages-Trading/index.html",
    "title": "Do Moving Averages Convey Information About Stock Price Movements?",
    "section": "",
    "text": "Purpose Of This Post\n\nThere is plethora of day trading coaches claiming indicators like moving averages are useful for timing the market in stock trading. With this short post, I want to investigate whether supervised machine learning models like random forests are capable of extracting information from moving averages (technical indicator) about the future direction of a stock on a daily basis, which will in turn show whether the previously mentioned claims hold water.\n\n\n\nDescription of the Data\n\nThe data I’ll be working with in this post is daily price information on the MSCI World from 2012 to the end of 2022:\n\n\n      date                close     \n Min.   :2012-07-09   Min.   :1202  \n 1st Qu.:2015-02-09   1st Qu.:1677  \n Median :2017-09-12   Median :1940  \n Mean   :2017-09-11   Mean   :2035  \n 3rd Qu.:2020-04-14   3rd Qu.:2283  \n Max.   :2022-11-15   Max.   :3248  \n\n\nUsing the zoo package, I can apply rolling window means of any length. For the visualisation below, I use 2 weeks, 1 month, 6 months and 1 year windows. For the actual machine learning models, I will go all the way down to 3 day windows, to get as many of these curves into the model, as I can. As random forests are capable of feature selection, there is no issue with putting a large amount of potentially useless predictors into the model.\n\n\nCode\ndt %&gt;% \n  arrange(date) %&gt;% \n  transmute(date, close = price) %&gt;% \n  mutate(mov_avg30 = rollapply(lag(close), width = 30, FUN = mean, \n                               align = \"right\", fill = \"NA\"),\n         mov_avg180 = rollapply(lag(close), width = 180, FUN = mean, \n                                align = \"right\", fill = \"NA\"),\n         mov_avg365 = rollapply(lag(close), width = 365, FUN = mean, \n                                align = \"right\", fill = \"NA\"),\n         year = year(date)) %&gt;% \n  # filter(year == 2022) %&gt;% \n  pivot_longer(-c(date, year)) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line() +\n  labs(title = \"MSCI Closing Price With Moving Averages\",\n       y = \"Closing Price\",\n       x = NULL,\n       colour = NULL) +\n  scale_colour_manual(values = c(\"#173F5F\", \"firebrick\",\n                                 \"#3CAEA3\", \"grey25\")) +\n  scale_y_continuous(labels = dollar_format())\n\n\n\n\n\nBefore I proceed with the modelling, I calculate the moving averages and normalise them to a percentage expressing the distance to the price. Hence, if the current closing price is above the trend line, the trend line position is expressed as a percentage in \\([0,1]\\), which is calculated as \\(\\frac{\\text{Trend Line Value}}{\\text{Index Value}}\\). If the trend line is above the price, it takes a value greater than one. At this stage, it is important to exclude the latest closing price from the rolling windows, as this would lead to data leakage.\n\n\nCode\ndata &lt;- dt %&gt;% \n  arrange(date) %&gt;% \n  transmute(date, open, close = price) %&gt;% \n  mutate(direction = ifelse(close &gt; open, \"up\", \"down\"),\n         mov_avg3 = rollapply(lag(close), width = 3, FUN = mean, \n                              align = \"right\", fill = \"NA\"),\n         mov_avg7 = rollapply(lag(close), width = 7, FUN = mean, \n                              align = \"right\", fill = \"NA\"),\n         mov_avg14 = rollapply(lag(close), width = 14, FUN = mean, \n                               align = \"right\", fill = \"NA\"),\n         mov_avg30 = rollapply(lag(close), width = 30, FUN = mean, \n                               align = \"right\", fill = \"NA\"),\n         mov_avg180 = rollapply(lag(close), width = 180, FUN = mean, \n                                align = \"right\", fill = \"NA\"),\n         mov_avg365 = rollapply(lag(close), width = 365, FUN = mean, \n                                align = \"right\", fill = \"NA\"),\n         across(c(mov_avg3:mov_avg365), ~ .x/lag(close)-1),\n         yday = yday(date),\n         yweek = week(date),\n         ymonth = month(date),\n         wday = wday(date),\n         year = year(date),\n         across(where(is.character), as.factor)) %&gt;% \n  drop_na(direction)\n\nglimpse(data %&gt;% drop_na())\n\n\nRows: 2,336\nColumns: 15\n$ date       &lt;date&gt; 2013-12-02, 2013-12-03, 2013-12-04, 2013-12-05, 2013-12-06…\n$ open       &lt;dbl&gt; 1627.21, 1620.69, 1613.05, 1606.40, 1599.90, 1612.65, 1618.…\n$ close      &lt;dbl&gt; 1621.34, 1613.31, 1604.74, 1598.88, 1612.61, 1617.60, 1614.…\n$ direction  &lt;fct&gt; down, down, down, down, up, up, down, down, down, down, up,…\n$ mov_avg3   &lt;dbl&gt; -6.427498e-04, 3.005744e-03, 4.781061e-03, 5.228261e-03, 4.…\n$ mov_avg7   &lt;dbl&gt; -0.002782715, 0.002012445, 0.006128483, 0.009813784, 0.0114…\n$ mov_avg14  &lt;dbl&gt; -6.386120e-03, -1.043229e-03, 4.449149e-03, 9.451909e-03, 1…\n$ mov_avg30  &lt;dbl&gt; -0.0110247970, -0.0062618986, -0.0012636546, 0.0041344185, …\n$ mov_avg180 &lt;dbl&gt; -0.07192371, -0.06720412, -0.06193912, -0.05632089, -0.0522…\n$ mov_avg365 &lt;dbl&gt; -0.12856759, -0.12409107, -0.11906142, -0.11369795, -0.1097…\n$ yday       &lt;dbl&gt; 336, 337, 338, 339, 340, 343, 344, 345, 346, 347, 350, 351,…\n$ yweek      &lt;dbl&gt; 48, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 51, 51, 51, 51,…\n$ ymonth     &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,…\n$ wday       &lt;dbl&gt; 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6,…\n$ year       &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,…\n\n\nAs can be seen above, the target variable I am trying to predict is whether the price of the index will go up or down within the next day. As a regression will be much more difficult than just predicting movements, the classification approach is giving the model more of a chance to get predictions right. As for the trading, we are not concerned with the absolute price swings in this model. More on that in the last section on letting the model trade.\n\n\n\nFitting A Classification Model\n\nFor the classification, I use all data pre-2019 to train the model and let it trade on hold-out data from 2019 to 2022. Therefore, I split the data into training and testing:\n\n\nCode\ndt_train &lt;- data %&gt;% \n  filter(year &lt;= 2018)\n\ndt_test &lt;- data %&gt;% \n  filter(year &gt; 2018)\n\n\nIn a next step, I fit the model without hyperparameter tuning, as I want to see how the model fares generally, without trying to optimise already. Note that I include lags up to 7 days for all predictors in the model, which means that the model can also account for changes in the levels of the trend lines in the past week:\n\n\nCode\nset.seed(1)\nrf_fit &lt;- workflow() %&gt;% \n  add_model(rand_forest() %&gt;% \n              set_mode(\"classification\") %&gt;% \n              set_engine(\"ranger\", importance = \"permutation\")) %&gt;% \n  add_recipe(recipe(direction ~ ., data = dt_train) %&gt;%\n               step_rm(close) %&gt;% \n               step_lag(all_numeric_predictors(), lag = seq(1, 7)) %&gt;% \n               step_impute_median(all_numeric_predictors()) %&gt;%\n               step_normalize(all_numeric_predictors())) %&gt;% \n  fit(dt_train)\n\n\nLooking at the evaluation metrics:\n\n\nCode\neval_metrics &lt;- metric_set(accuracy, sensitivity, specificity, precision,\n                           recall)\n\nrf_fit %&gt;% \n  augment(dt_test) %&gt;% \n  eval_metrics(truth = direction, estimate = .pred_class)\n\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.480\n2 sensitivity binary         0.777\n3 specificity binary         0.235\n4 precision   binary         0.456\n5 recall      binary         0.777\n\n\nThe evaluation metrics don’t look good: accuracy is a coin toss and the model is a little too trigger happy forecasting upward movements, as the sensitivity is high, but precision low.\n\n\nCode\nrf_fit %&gt;% \n  augment(dt_test) %&gt;% \n  roc_auc(direction, .pred_up)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.489\n\n\n\n\nCode\nrf_fit %&gt;% \n  augment(dt_test) %&gt;% \n  select(direction, .pred_class, .pred_up, .pred_down) %&gt;% \n  roc_curve(direction, .pred_up) %&gt;% \n  autoplot() %&gt;% \n  labs(title = \"ROC AUC Curve\")\n\n\n[[1]]\n\n\n\n\n\n\n$title\n[1] \"ROC AUC Curve\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nROC AUC looks truly terrible. The model is no better than a random coin toss. What I really want to see though is the performance of the model trading on the holdout data set, as we are not only concerned with the directions of movements, but also with the impact of different movement magnitudes on the portfolio value.\n\n\n\nLetting The Model Trade\n\nFirst, I use the trained model to make predictions on the holdout data set:\n\n\nCode\npreds &lt;- rf_fit %&gt;% \n  augment(dt_test) %&gt;% \n  select(date, open, .pred_class)\n\nhead(preds)\n\n\n# A tibble: 6 × 3\n  date        open .pred_class\n  &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;      \n1 2019-01-01 1885. down       \n2 2019-01-02 1885. down       \n3 2019-01-03 1883. down       \n4 2019-01-04 1854. up         \n5 2019-01-07 1904. up         \n6 2019-01-08 1920. up         \n\n\nNow comes the interesting part: I will let the model trade based on the following rules:\n\nStart at Portfolio Value of 100%\nIf the model signals “up”, then enter the market and hold for each “up” signal, until the first “down” prediction arrives\nIn that case, sell and bank the difference\n\nThis will be repeated over the whole trading period:\n\n\nCode\nfor (i in 1:nrow(preds)){\n  \n  # on first day, no trading, portfolio value is at 100%\n  if (i == 1){\n    preds[i, \"PV\"] = 1\n    next\n  }\n  \n  # hold if current is \"up\" and last was \"up\"\n  else if (preds$.pred_class[i] == \"up\" & preds$.pred_class[i-1] == \"up\"){\n    preds[i,\"PV\"] = preds[i-1,\"PV\"]\n  }\n  \n  # stay out of market if current is down and previous was down\n  else if (preds$.pred_class[i] == \"down\" & preds$.pred_class[i-1] == \"down\"){\n    preds[i, \"PV\"] = preds[i-1, \"PV\"]\n  }\n  \n  # go into market if previous was \"down\" and current is \"up\"\n  else if (preds$.pred_class[i] == \"up\" & preds$.pred_class[i-1] == \"down\"){\n    buy_price = preds$open[i]\n    preds[i,\"PV\"] = preds[i-1, \"PV\"]\n  }\n  \n  # go out of market if previous was \"up\" and current is \"down\"\n  # this is the only transaction that actually affects PV\n  else if (preds$.pred_class[i] == \"down\" & preds$.pred_class[i-1] == \"up\"){\n    sale_price = preds$open[i]\n    preds[i, \"PV\"] = preds[i-1, \"PV\"] * (sale_price/buy_price)\n  }\n}\n\n\nAfter the loop has finished, we can inspect the performance on the holdout data:\n\n\nCode\ndt_test %&gt;% \n  transmute(date, msci = close) %&gt;% \n  mutate(msci = msci/first(msci)) %&gt;% \n  left_join(\n    preds %&gt;% \n      transmute(date, model = PV)   \n  ) %&gt;% \n  fill(model) %&gt;% \n  mutate(model = ifelse(is.na(model), 1, model)) %&gt;% \n  pivot_longer(-date) %&gt;% \n  ggplot(aes(date, value, colour = name)) +\n  geom_step() +\n  labs(title = \"Comparing One Model To Just Holding The MSCI\",\n       y = \"Cumulative Performance\",\n       x = NULL,\n       colour = NULL) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  ggsci::scale_colour_d3()\n\n\n\n\n\nFrom the chart, it becomes apparent that the random forest model does not perform particularly well, coming in significantly lower than the MSCI. The model missed out on the 12 months upwards rally throughout the year of 2021, but subsequently rode down the contraction in 2022 - suboptimal performance to put it lightly.\nLastly, let’s fit 50 random forest models with the exact same parameters to the data in order to inspect the robustness of the predictions and how strong performance varies between models. Writing to functions to save some redundant typing:\n\n\nCode\nfit_rf &lt;- function(){\n  workflow() %&gt;% \n    add_model(rand_forest() %&gt;% \n                set_mode(\"classification\") %&gt;% \n                set_engine(\"ranger\", importance = \"permutation\")) %&gt;% \n    add_recipe(recipe(direction ~ ., data = dt_train) %&gt;%\n                 step_rm(close) %&gt;% \n                 step_lag(all_numeric_predictors(), lag = seq(1, 7)) %&gt;% \n                 step_impute_median(all_numeric_predictors()) %&gt;%\n                 step_normalize(all_numeric_predictors())) %&gt;% \n    fit(dt_train)\n}\n\ntrade &lt;- function(tbl){\n  for (i in 1:nrow(tbl)){\n    \n    # on first day, no trading, portfolio value is at 100%\n    if (i == 1){\n      tbl[i, \"PV\"] = 1\n      next\n    }\n    \n    # hold if current is \"up\" and last was \"up\"\n    else if (tbl$.pred_class[i] == \"up\" & tbl$.pred_class[i-1] == \"up\"){\n      tbl[i,\"PV\"] = tbl[i-1,\"PV\"]\n    }\n    \n    # stay out of market if current is down and previous was down\n    else if (tbl$.pred_class[i] == \"down\" & tbl$.pred_class[i-1] == \"down\"){\n      tbl[i, \"PV\"] = tbl[i-1, \"PV\"]\n    }\n    \n    # go into market if previous was \"down\" and current is \"up\"\n    else if (tbl$.pred_class[i] == \"up\" & tbl$.pred_class[i-1] == \"down\"){\n      buy_price = tbl$open[i]\n      tbl[i,\"PV\"] = tbl[i-1, \"PV\"]\n    }\n    \n    # go out of market if previous was \"up\" and current is \"down\"\n    # this is the only transaction that actually affects PV\n    else if (tbl$.pred_class[i] == \"down\" & tbl$.pred_class[i-1] == \"up\"){\n      sale_price = tbl$open[i]\n      tbl[i, \"PV\"] = tbl[i-1, \"PV\"] * (sale_price/buy_price)\n    }\n  }\n  \n  return(tbl)\n}\n\nmultiple_rf &lt;- tibble(models = replicate(fit_rf(), n = 50, simplify = F)) %&gt;% \n  mutate(model_nr = paste(\"Model\", 1:nrow(.)),\n         preds = map(models, ~ augment(.x, dt_test)),\n         preds = map(preds, ~ trade(.x)))\n\n\nI can plot the results, showing the range of the predictions with confidence bands versus the actual MSCI price:\n\n\nCode\nmultiple_rf %&gt;% \n  unnest(preds) %&gt;% \n  transmute(model_nr, date, value = PV) %&gt;% \n  group_by(date) %&gt;% \n  summarise(min_model = min(value),\n            mean_model = mean(value),\n            max_model = max(value),\n            q25 = quantile(value, 0.25),\n            q75 = quantile(value, 0.75)) %&gt;% \n  left_join(dt_test %&gt;% \n              arrange(date) %&gt;% \n              transmute(date, msci = close/first(close)),\n            by = \"date\") %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = msci)) +\n  geom_line(aes(y = mean_model), colour = \"dodgerblue\") +\n  geom_ribbon(aes(ymin = q75, ymax = max_model),\n              alpha = 0.25, colour = NA, fill = \"dodgerblue\") +\n  geom_ribbon(aes(ymin = q25, ymax = q75),\n              alpha = 0.4, fill = \"dodgerblue\") +\n  geom_ribbon(aes(ymin = min_model, ymax = q25),\n              alpha = 0.25, fill = \"dodgerblue\") +\n  labs(title = \"Comparing 50 Random Forests To The MSCI\",\n       subtitle = \"Blue line indicates the average performance of 50 models.\\nThe darker blue ribbon indicates the 25th and 75th percentile range.\\nThe lighter blue ribbon indicates the total range of model performances.\",\n       y = \"Cumulative Performance\",\n       x = NULL) +\n  scale_y_continuous(labels = percent_format())\n\n\n\n\n\nAs seen in the chart above, the random forest models have huge variance. Combined with the previously seen ROC AUC score, this is a clear indication that there is not much useful information in the predictors. Therefore, I can conclude that moving averages convey close to no information about the price development in the MSCI, and instead of spending countless hours analysing and working on daily trading decisions, one might be much better off just holding an ETF on the MSCI over a longer period of time. Of course, the disclaimer here is: We never know, whether things might change completely going forward.\nI hope this post has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out.\nThank you for reading.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220603-Mutual-Fund-Selection/index.html",
    "href": "posts/20220603-Mutual-Fund-Selection/index.html",
    "title": "Application Of Machine Learning Methods To Mutual Fund Selection",
    "section": "",
    "text": "Purpose Of This Document\n\nThis document serves the purpose of an executive summary of my bachelor’s thesis on machine learning in fund selection. The main findings and implications with regard to practice are presented. Granular details on the methods and results, both quantitative and qualitative, are omitted in view of the length of this document. You are welcome, however, to reach out if you would like to see more detail.\n\n\n\nResearch Question\n\nMy bachelor’s thesis revolves around the research question of whether machine learning methods are capable of assisting asset and wealth managers in selecting funds which generate strictly positive alpha over an extended period of time based on their fund characteristics. Furthermore, it ties together the technical analysis of using machine learning methods in predicting outperforming mutual funds with implications of their implementation in practice gathered from interviews with investment professionals from various financial institutions.\n\n\n\nThe Data\n\nThe dataset employed as a basis for the algorithms is provided by the Center for Research in Security Prices (CRSP) in the form of a survivorship-bias free mutual fund data base hosted by Wharton Research Data Services (WRDS). Looking back at historical performance, including delisted funds, which have been liquidated due to subpar performance, constitutes a crucial step in order to prevent a performance bias in the training data as shown by Carhart (1995). Practically, this entails including all operating funds for any present time period in the data. Intuitively, readding unsuccessful mutual funds is logical, because if one wants to infer from historical data, the present risk of failure should also be reflected in the historical data.\nMore than \\(64,000\\) open-ended mutual funds, including equity funds, fixed income funds, international funds, variable annuity underlying funds as well as passive ETFs and ETNs are contained in the CRSP data base. The observation period stretches from December 1961 until 2021 and various information is stored in separate files linked by an identification number for each fund. Instances of available data sets are the history of each mutual fund’s name, investment style, fee structure, holdings, returns, net asset values (NAV), total net assets (TNA), distributions, asset class codes, management information and so forth as shown below (CRSP Survivorship-Bias Free Mutual Fund Data Base Manual, 2022).\n\n\n\n\n\n\n\n\n\nDue to the wide range of information, some exclusions are made prior to starting the analysis. Firstly, funds with front or rear load fees are unsuitable for an investment approach entailing frequent rebalancing, as these types of fees significantly weigh down the net return achieved by the fund. As the forecast period of the algorithms is one year, this approach entails frequent rebalancing. Hence, similarly to the approach of De Miguel (2021), all observations from these funds are dropped. Taking a step back and reflecting on the initial goal of the analysis leads to the conclusion that passive funds need to be excluded from the analysis as well. Investors buying into passive ETFs and index funds tend to do so in order to build exposure in a certain region or industry, not however to identify outperforming fund managers, as these products are not actively managed and in a practical setting often constitute the benchmark themselves. In addition, the data contain equity, fixed income, mixed and other funds. Examples of the latter category are mortgage funds, event driven funds, multi-strategy funds, options arbitrage funds and stable value funds. These different types of investments have different characteristics and constitute separate problems from the point of view of return prediction and identification of best performers. Due to information on equity funds being the richest with a share of nearly 60% of all observations in the CRSP data set, the analysis continues with the equity asset class, disregarding fixed income, mixed and other funds. In practice, investment professionals often filter out funds below a certain size with regard to total net assets. There exists a conflict between data availability and comparability to practice, in that higher TNA thresholds lead to a higher loss in the quantity of observations. For instance, a limit of USD MM \\(100\\) to USD MM \\(500\\), which is representative of limits set in practice, would lead to a loss between \\(60.6 \\%\\) to \\(83.9 \\%\\) of observations for the final cleaned data set. Therefore, as a compromise, a limit of USD MM \\(20\\) is used, which eliminates \\(37.3 \\%\\) of the remaining observations and leaves a sufficient absolute number of observations for training. Additionally, a lower threshold from today’s perspective also constitutes a compromise for observations from several decades ago, bearing in mind asset inflation leading to larger average fund sizes over time. From the interviews with practitioners, it has also become apparent that these absolute limits are not set in stone, as there exist additional complications in practice with regard to specialised mandates and large foreign strategies registering securities in the home country, mirroring the original strategy.\n\n\n\nMethods And Approaches\n\nFor the analysis of the technical aspect of ML in fund selection five methods, namely random forest, gradient boosting, elastic net, support vector machines and k-nearest neighbours, are employed on a set of fund characteristics in two scenarios. Firstly, absolute alpha, which is calculated based on the four factor model introduced by Carhart (1997), constitutes the target variable and is predicted annually based on 19 fund characteristics over the period from 1999 to 2021. Secondly, relative alpha, which is calculated as the relative outperformance compared to the peer group of the respective mutual fund, constitutes the target variable and is predicted in the same manner based on 13 fund characteristics. The models are trained annually based on a 10-year rolling retrospective training data window and the top decile funds of the projected year ahead are incorporated into an equally weighted portfolio, enabling the generation of time series for transparent comparison of each of the different methods. This paper focuses on the performance of ML models in the most recent decade, i.e. the representative and relevant time frame in practice. Other academic papers analysed cumulative performance over decades in the past century, like the one by DeMiguel et al. (2021), who drew conclusions as to the success of the identification of absolute alpha based on a single factor model regression over the period 1980-2020, where all methods were only successfully identifying alpha prior to the 21st century. Furthermore, the models are evaluated based on the comparison of time series between the full sample and selected funds, which follows the literature review of Buczynski, Cuzzolin, and Sahakian (2021, p. 228), who criticised a large fraction of academic literature for not utilising time series for the final evaluation of their models.\nThe predictors employed for both the absolute and relative approach and their respective way of aggregation to equal annual frequencies are depicted in the tables below.\n\n\n\nAggregation of variables to yearly values for absolute alpha prediction\n\n\nVariable\nAggregation (for each fund and year)\n\n\n\n\nLagged Annual Realised Alpha (Target)\nSum of monthly values\n\n\nAnnual Realised Alpha\nSum of monthly values\n\n\nTotal Net Assets\nEnd of calendar year value\n\n\nManager Tenure\nEnd of calendar year value\n\n\nFund Age\nEnd of calendar year value\n\n\nExpense Ratio\nAverage of monthly values\n\n\nTurnover Ratio\nEnd of calendar year value\n\n\nFund Flows\nAverage of monthly values\n\n\nStandard Deviation of Flows\nGenerated based on monthly values\n\n\nSharpe Ratio\nGenerated based on monthly values\n\n\nStandard Deviation of Returns\nGenerated based on monthly values\n\n\nSkewness of Returns\nGenerated based on monthly values\n\n\nKurtosis of Returns\nGenerated based on monthly values\n\n\nMaximum Drawdown\nGenerated based on monthly values\n\n\nAlpha from Rolling Factor Models ($\\beta_0$ t-stat)\nEnd of calendar year value\n\n\nMKTRF from Rolling Factor Models ($\\beta_1$ t-stat)\nEnd of calendar year value\n\n\nHML from Rolling Factor Models ($\\beta_2$ t-stat)\nEnd of calendar year value\n\n\nSMB from Rolling Factor Models ($\\beta_3$ t-stat)\nEnd of calendar year value\n\n\nUMD from Rolling Factor Models ($\\beta_4$ t-stat)\nEnd of calendar year value\n\n\n$R^2$ from Rolling Factor Models\nEnd of calendar year value\n\n\n\n\n\n\n\n\n\n\nAggregation of variables to yearly values for relative alpha prediction\n\n\nVariable\nAggregation (for each fund and year)\n\n\n\n\nLagged Relative Alpha (Target)\nGenerated from returns post-aggregation\n\n\nRelative Alpha\nGenerated from returns post-aggregation\n\n\nTotal Net Assets\nEnd of calendar year value\n\n\nManager Tenure\nEnd of calendar year value\n\n\nFund Age\nEnd of calendar year value\n\n\nExpense Ratio\nAverage of monthly values\n\n\nTurnover Ratio\nEnd of calendar year value\n\n\nFund Flows\nAverage of monthly values\n\n\nStandard Deviation of Flows\nGenerated based on monthly values\n\n\nSharpe Ratio\nGenerated based on monthly values\n\n\nStandard Deviation of Returns\nGenerated based on monthly values\n\n\nSkewness of Returns\nGenerated based on monthly values\n\n\nKurtosis of Returns\nGenerated based on monthly values\n\n\nMaximum Drawdown\nGenerated based on monthly values\n\n\n\n\n\n\n\nAs a short note, for the generation of the target variable of annual realised alpha, the four-factor model from Carhart (1997) is employed for each fund separately and for each month on the rolling 36-months window basis, as shown in equation 1.\n\\[\nr_{i,t} - rf_{t} = \\alpha_{i,t} + \\beta_{i,t}^{MKT}*MKTRF_t + \\beta_{i,t}^{HML} * HML_t + \\beta_{i,t}^{SMB} * SMB_t + \\beta_{i,t}^{UMD} * UMD_t + \\epsilon_{i,t} \\tag{1}\n\\]\nIn the regression, \\(r_{it}\\) is the fund i’s monthly return at time t, \\(rf_{t}\\) is the one-month treasury bill return constituting the risk-free rate, \\(MKTRF_t\\) is the excess return of the market on the risk-free rate approximated by CRSP’s value-weighted market proxy index, \\(HML_t\\) is the monthly premium of the book-to-market factor capturing value, \\(SMB_t\\) is the monthly premium of the size factor measured by market capitalisation and \\(UMD_t\\) is the premium on one-year momentum in equities (Carhart, 1997). The rolling window regression yields factor loadings for all four factors, t-stats, p-values as well as \\(R^2\\) for each fund, from which time series are built, excluding the first 36 observations. The figure below provides a visual example of the factor loadings time series generated in the rolling window regression for one fund in the sample. The t-stats of the latter constitute predictors for the models.\n\n\n\n\n\nAfter the rolling window regressions, realised monthly alpha generated by each fund in each monthly period is calculated as\n\\[\n\\alpha_{i,t} = (r_{i,t} - rf_{t}) - (\\beta_{i,t}^{MKT}*MKTRF_t + \\beta_{i,t}^{HML} * HML_t + \\beta_{i,t}^{SMB} * SMB_t + \\beta_{i,t}^{UMD} * UMD_t) \\tag{2}\n\\]\nthat is, realised alpha for each period constitutes the regression’s intercept plus the error term \\(\\epsilon_{i,t}\\) resulting from the imperfect fit of the linear regression, as \\(R^2&lt;1\\). In other words, monthly realised alpha is defined as the excess return over the return attributable to the four factors each fund generated for each period and represents the target variable at yearly frequency for all predictive models on absolute alpha at later stages. The chart below shows the distribution of annual realised alpha in the sample by year.\n\n\n\n\n\n\n\n\n\nRelative alpha, the target variable in the second approach, is just the relative outperformance of one fund compared to its peer group for any given period, as defined below.\n\\[\\alpha^{\\text{relative}}_{i_p,t} = r_{i,t} - \\frac{1}{N_p} \\sum_{j_p = 1}^{N_p}{r_{j_p}} \\tag{4}\\]\nDetails on further feature generation, the data cleaning process, functioning of the algorithms and more are omitted to keep this summary concise. However, you are welcome to reach out for further, detailed information.\n\n\n\nMain Results\n\nThe two approaches in predicting absolute and relative alpha show slightly different results, though they do not stand in complete contrast to each other.\nFirstly, no model is capable of identifying continuously positive mean absolute alphas, leading to a negative cumulative absolute alpha across all methods over the most recent decade. However, despite cumulative absolute alpha being negative, the majority of all models, except for k-nearest neighbours, are capable of selecting funds beating the available sample alpha. Namely, linear methods, that is elastic net and ordinary least squares, perform significantly better than their non-linear counterparts, which is due to the deterioration of goodness of fit caused by the time-varying and weak predictive power of fund characteristics when applying the trained algorithms to the post-sample prediction period. From the first chart below, no apparent superiority of the portfolios selected by the methods can be concluded. Looking at the cumulative performance in the second chart below paints a clearer image.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the out-of-sample fit in the chart below, it quickly becomes apparent that the predictors carry little predictive power over into the post-sample period.\n\n\n\n\n\n\n\n\n\nSecondly, the prediction of relative alpha shows stark differences even just among the five most prevalent fund classes, with highest success for multi-cap core funds and worst selection for small-cap core funds as seen in the chart below. It has to be noted that the sample performance is approximately zero by definition, hence successfully performing methods must exhibit positive relative alpha, not merely higher alpha than the sample as in the absolute case. Averaging performance of the same models across all fund classes reveals that all models are able to capture relative alpha cumulatively over the recent decade, although strong variability of the cumulative results pose a threat for shorter investment periods. The latter is shown in the second chart below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThirdly, following the qualitative analysis of the interviews conducted with fund selection experts, several barriers to implementation of the methods were identified, namely data availability and quality, trading costs, the need to account for soft information, academic assumptions conflicting with practice and the need to maximise the right target variable. The latter emerges from absolute and relative alpha not being representative of the situation faced in practice, as other factors, such as ESG, are becoming increasingly important and need to be accounted for.\nDetails on time-varying variable importance, the characteristics of chosen portfolios and more are omitted to keep this summary the most concise possible. However, you are welcome to reach out for further, detailed information.\n\n\n\nConclusion And Implications For Practitioners\n\nConclusively, the thesis shows that the predictability of abnormal returns of equity mutual funds has strongly deteriorated over time to a point where the reliable selection of outperforming mutual funds based on fund characteristics with machine learning methods is not given any more. By extending methods and remediating inconsistencies in the conclusions of existing literature with the consideration of the practical perspective gained from the qualitative evaluation of interviews, this thesis constitutes an extension of the literature and stands in contrast to literature claiming exceptional performance of the methods based on cumulative performance over many decades reaching into the past century. The approach of predicting relative alpha faces the same limitations, but instead leads to additional style biases creeping in, in turn not constituting a viable alternative in its form as presented in this thesis either.\nFrom recent literature, it has become apparent that the inclusion of macroeconomic variables and alternative predictors, such as sentiment, is crucial in order to exploit interactions and enable the identification of positive abnormal returns, even though the latter need further replication to account for assumptions conflicting with practice, such as the shorting of the bottom decile portfolio. Further research on the quantification and predictive ability of macroeconomic, political and regulatory variables constitutes the crucial building block on top of the obtained results of this thesis, enabling a more holistic conclusion on the usefulness of machine learning methods for practitioners in fund selection.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220531-Monty-Hall/index.html",
    "href": "posts/20220531-Monty-Hall/index.html",
    "title": "The Monty Hall Dilemma",
    "section": "",
    "text": "What Is The Monty Hall Dilemma?\n\nThe game show host leads you to a wall with three closed doors. Behind one of the doors is the car of your dreams, and behind each of the other two is a goat. The three doors all have even chances of hiding the car.\nThe host, a trustworthy person who knows precisely what is behind each of the three doors, explains how the game will work. First, you will choose a door without opening it. The host will then open one of the two remaining doors to reveal a goat. When this has been done, you will be given the opportunity to switch doors. You will win whatever is behind the door you choose at this stage of the game.\nDo you stick to your initial choice or do you switch doors?\nDo you raise your chance of winning the car by switching doors?\n\n\n\nNot Switching Doors\n\nDefine:\n\n1: Door 1\n2: Door 2\n3: Door 3\nC: Car\nG: Goat\n\nThe first simulation will be sticking with the initial choice.\n\n\nCode\n#################################\n## Simulation 1: Not switching ##\n#################################\n\n# params\nsimulations = 10000\noutcomes &lt;- c(\"C\", \"G\", \"G\")\nresmat_ns &lt;- matrix(ncol = 7, nrow = simulations)\ncolnames(resmat_ns) &lt;- c(1, 2, 3, \"Initial Choice\", \n                      \"Revealed Door\", \"Subsequent choice\", \"Win\")\noptions &lt;- c(1:3)\n\n# Simulation\nfor (i in 1:simulations){\n  \n  # host sets up the doors: 1 Car and 2 Goats\n  resmat_ns[i,1:3] &lt;- sample(x = outcomes, size = 3, replace = FALSE)  \n\n  # guest makes a random choice\n  resmat_ns[i,4] &lt;- sample(x = options, size = 1, replace = FALSE)\n  \n  # host reveals a goat and gives choice between chosen closed\n  # and unchosen closed door\n  doors &lt;- resmat_ns[i,1:3]\n  choice &lt;- as.numeric(resmat_ns[i,4])\n  \n  if (resmat_ns[i,choice] == \"C\"){ # situation where guest chose car\n    \n    resmat_ns[i,5] &lt;- sample(x = options[! options %in% c(choice)], size = 1, \n                          replace = TRUE)\n    \n  } else { # situation where guest chose goat\n    \n    resmat_ns[i,5] &lt;- options[! options %in% c(choice,\n                                          which(resmat_ns[i,1:3] == \"C\")[[1]])]\n    \n  }\n  \n  # Offer to switch DECLINED\n  resmat_ns[i,6] = resmat_ns[i,4]\n  \n  #Win or no win\n  if ( resmat_ns[i,6] == which(resmat_ns[i,1:3] == \"C\")[[1]] ){\n    \n    resmat_ns[i,7] = 1 # correct choice: win\n    \n  } else {\n    \n    resmat_ns[i,7] = 0 # wrong choice: lose\n    \n  }\n}\n\nhead(as.data.frame(resmat_ns), 10)\n\n\n   1 2 3 Initial Choice Revealed Door Subsequent choice Win\n1  G G C              2             1                 2   0\n2  G C G              1             3                 1   0\n3  C G G              2             3                 2   0\n4  C G G              1             3                 1   1\n5  C G G              2             3                 2   0\n6  G C G              3             1                 3   0\n7  C G G              3             2                 3   0\n8  G C G              1             3                 1   0\n9  G C G              2             3                 2   1\n10 G G C              2             1                 2   0\n\n\n\n\n\nSwitching\n\nIn the second simulation, the strategy is always switching the choice to the remaining door, that is never sticking to the initial choice.\n\n\nCode\n#############################\n## Simulation 2: Switching ##\n#############################\n\n# params\nresmat_s &lt;- matrix(ncol = 7, nrow = simulations)\ncolnames(resmat_s) &lt;- c(1, 2, 3, \"Initial Choice\", \n                      \"Revealed Door\", \"Subsequent choice\", \"Win\")\n\n# Simulation\nfor (i in 1:simulations){\n  \n  # host sets up the doors: 1 Car and 2 Goats\n  resmat_s[i,1:3] &lt;- sample(x = outcomes, size = 3, replace = FALSE)  \n\n  # guest makes a random choice\n  resmat_s[i,4] &lt;- sample(x = options, size = 1, replace = FALSE)\n  \n  # host reveals a goat and gives choice between chosen closed\n  # and unchosen closed door\n  doors &lt;- resmat_s[i,1:3]\n  choice &lt;- as.numeric(resmat_s[i,4])\n  \n  if (resmat_s[i,choice] == \"C\"){ # situation where guest chose car\n    \n    resmat_s[i,5] &lt;- sample(x = options[! options %in% c(choice)], size = 1, \n                          replace = TRUE)\n    \n  } else { # situation where guest chose goat\n    \n    resmat_s[i,5] &lt;- options[! options %in% c(choice,\n                                          which(resmat_s[i,1:3] == \"C\")[[1]])]\n    \n  }\n  \n  # Offer to switch ACCEPTED\n  resmat_s[i,6] &lt;- options[! options %in% c(choice, resmat_s[i,5])]\n  \n  #Win or no win\n  if ( resmat_s[i,6] == which(resmat_s[i,1:3] == \"C\")[[1]] ){\n    \n    resmat_s[i,7] = 1 # correct choice: win\n    \n  } else {\n    \n    resmat_s[i,7] = 0 # wrong choice: lose\n    \n  }\n}\n\nhead(as.data.frame(resmat_s), n = 10)\n\n\n   1 2 3 Initial Choice Revealed Door Subsequent choice Win\n1  G C G              1             3                 2   1\n2  G C G              2             1                 3   0\n3  G G C              1             2                 3   1\n4  G G C              3             2                 1   0\n5  G G C              1             2                 3   1\n6  C G G              1             2                 3   0\n7  G G C              2             1                 3   1\n8  G G C              3             2                 1   0\n9  C G G              2             3                 1   1\n10 G G C              1             2                 3   1\n\n\n\n\n\nPlotting The Results\n\nFrom the simulations and the plots below, it quickly becomes apparent that the superior strategy is switching doors, leading to a win probability of \\(\\frac{2}{3}\\) as opposed to \\(\\frac{1}{3}\\). Leveraging the law of large numbers, the convergence of the probabilities in simulations shows us a convenient and quick solution to an initially challenging problem.\nTherefore, the optimal strategy for the Monty Hall game show would be to always deviate from your initial choice.\n\n\nCode\nplotmat_ns &lt;- matrix(ncol = 2, nrow = simulations)\nplotmat_ns[,1] &lt;- (1:simulations)\nplotmat_ns[,2] &lt;- cumsum(resmat_ns[,7])/(1:simulations)\n\nplotmat_ns %&gt;% \n  as_tibble() %&gt;% \n  rename(simulation = V1, cumprob = V2) %&gt;% \n  ggplot(aes(simulation, cumprob)) +\n  geom_line() +\n  geom_hline(yintercept = 1/3, colour = \"firebrick\", lty = \"dashed\") +\n  labs(title = \"Monty Hall Dilemma: Not Switching\",\n       x = \"Games\",\n       y = \"Win Probability\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1),\n                     limits = c(0,1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\nplotmat_s &lt;- matrix(ncol = 2, nrow = simulations)\nplotmat_s[,1] &lt;- (1:simulations)\nplotmat_s[,2] &lt;- cumsum(resmat_s[,7])/(1:simulations)\n\nplotmat_s %&gt;% \n  as_tibble() %&gt;% \n  rename(simulation = V1, cumprob = V2) %&gt;% \n  ggplot(aes(simulation, cumprob)) +\n  geom_line() +\n  geom_hline(yintercept = 2/3, colour = \"firebrick\", lty = \"dashed\") +\n  labs(title = \"Monty Hall Dilemma: Switching\",\n       x = \"Games\",\n       y = \"Win Probability\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1),\n                     limits = c(0,1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_bw()\n\n\n\n\n\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/2022-09-19-Used-Motorcycles/index.html",
    "href": "posts/2022-09-19-Used-Motorcycles/index.html",
    "title": "Scraping and Modelling Used Motorcycles",
    "section": "",
    "text": "Description of the Data\n\nThis blog post is about how to create your own data sets by scraping websites with rvest. Specifically, I will scrape all available used motorcycles from the online platform Anibis, explore the data, make conclusions and train a model on the prices.\n\n\n\nData Scraping\n\nFirstly, let’s start by analysing the website. Searching for motorcycles yields an overview of 20 sub-page links (2 shown below). My plan is first getting links to all 24,000 of these sub-pages and store them for later use. This will result in a tibble with one link for each available motorcycle on the website.\n\n\n\n\n\nSpecifically, by inspecting the source code of the website, I found out the class I needed to get to the hyperlinks hidden behind the title of the respective motorcycle sub-page. Firstly, I constructed the links of the overview pages by varying the page number from 1 to 1200. Then, I mapped them onto a function including a wait, in order to prevent errors caused by Anibis’ servers timing out after too many request.\n\n# Store main page URLs\npages &lt;- 1:1200\n\nstart_urls &lt;- paste0(\"https://www.anibis.ch/de/c/motorrad-velo-motorraeder?pi=\",\n                     pages) %&gt;% \n  as_tibble() %&gt;% \n  rename(url = value)\n\n# Fetch subpages from listings from each overview page\nlistings &lt;- start_urls %&gt;%\n  mutate(subpages = map(url, function(.x) {\n    return(\n      GET(.x, timeout(10)) %&gt;% \n        read_html(.) %&gt;%\n        html_nodes(\"[class ='sc-1yo7ctu-0 bRDNul']\") %&gt;%\n        html_attr('href') %&gt;%\n        as_tibble() %&gt;%\n        rename(subpage = value) %&gt;%\n        mutate(subpage = paste0(\"https://www.anibis.ch\", subpage))\n    )\n  }))\n\n# Extract subpage urls and clean as tibble\nsubpage_urls &lt;- listings %&gt;% \n  select(subpages) %&gt;% \n  unnest(subpages)\n\nNow that I have scraped and unnested all links to all subpages, I will load the entire content of each subpage into memory using a for loop with a time out exception like previously. This part takes the longest time to run at a couple of hours.\n\nsubpages_content &lt;- vector(mode = \"list\", length = nrow(subpage_urls))\n\nfor (x in 1:nrow(subpage_urls)){\n   \n  url_tmp &lt;- subpage_urls[x, \"subpage\"] %&gt;% \n    pull()\n  \n  tryCatch(\n    subpages_content[[x]] &lt;- url_tmp %&gt;%\n      GET(., timeout(90)) %&gt;% \n      read_html(.),\n    error = function(e){NA}\n  )\n  \n  print(paste(\"Link\", x, \"retrieved\"))\n  \n}\n\nsubpage_content &lt;- tibble(listing = subpages_content)\n\nHaving fetched the entire HTML of each sublisting, I can now extract the interesting information by using the SelectorGadget to find the right CSS selectors and then mapping rvest functions onto the stored xml objects in the previously generated tibble, in order to get the information in character format.\n\nmotorcycles_raw &lt;- tibble(\n  listing_no = 1:nrow(subpage_content),\n  listing = subpage_content %&gt;% pull(),\n  header = map(listing, function(.x){\n      return(.x %&gt;% \n        html_nodes(\".fauvte\") %&gt;%\n        html_text())\n      }),\n  content = map(listing, function(.x){\n      return(.x %&gt;% \n        html_nodes(\".goTXZq\") %&gt;%\n        html_text())\n      }),\n  price = map(listing, function(.x){\n      return(.x %&gt;% \n        html_node(\".knSuBJ\") %&gt;%\n        html_text())\n      })\n  ) %&gt;%\n  select(-listing) %&gt;% \n  unnest(everything()) %&gt;% \n  pivot_wider(values_from = content, names_from = header)\n\nAfter some unnesting, I am left with the raw data:\n\n\nRows: 23,974\nColumns: 17\n$ listing_no         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ price              &lt;chr&gt; \"CHF 6'500.– / Verhandlungspreis\", \"CHF 5'500.– / V…\n$ Art                &lt;chr&gt; \"Gegenstand\", \"Gegenstand\", \"Gegenstand\", \"Gegensta…\n$ Zustand            &lt;chr&gt; \"Neuwertig\", \"Geringe Gebrauchsspuren\", \"Geringe Ge…\n$ `Hubraum (Ccm)`    &lt;chr&gt; \"500 Cm3\", \"800 Cm3\", \"600 Cm3\", \"1200 Cm3\", \"50 Cm…\n$ Baujahr            &lt;dbl&gt; 2014, 2014, 2013, 1989, 2013, 2019, 2019, NA, 2017,…\n$ Marke              &lt;chr&gt; \"YAMAHA\", \"YAMAHA\", \"YAMAHA\", \"YAMAHA\", \"BETA\", \"PO…\n$ Modell             &lt;chr&gt; \"450 wrf\", \"Mt-07\", NA, \"FJ 1200\", \"50 RR\", \"RZR 10…\n$ Kilometer          &lt;chr&gt; \"5'660 km\", \"41'000 km\", \"20'000 km\", \"7'156 km\", \"…\n$ Getriebeart        &lt;chr&gt; \"Schaltgetriebe\", NA, \"Schaltgetriebe\", \"Schaltgetr…\n$ Treibstoff         &lt;chr&gt; \"Benzin\", \"Benzin\", \"Benzin\", \"Benzin\", \"2-Takt-Gem…\n$ Aussenfarbe        &lt;chr&gt; \"Blau\", \"Rot\", \"Schwarz\", \"Grau\", \"Rot\", NA, \"Orang…\n$ `Art des Inserats` &lt;chr&gt; \"Angebot\", \"Angebot\", \"Angebot\", \"Angebot\", \"Angebo…\n$ `Letzte Änderung`  &lt;chr&gt; \"05.09.2022\", \"05.09.2022\", \"05.09.2022\", \"05.09.20…\n$ Inseratnummer      &lt;dbl&gt; 44818239, 44818276, 44818264, 44818210, 44818257, 4…\n$ `Ab MFK`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Antrieb            &lt;chr&gt; NA, NA, NA, \"Kette\", NA, NA, NA, NA, \"Kette\", \"Hint…\n\n\n\n\n\nData Cleaning\n\nAs most of the columns are not clean yet, I’ll continue by parsing numbers, trimming trailing spaces, interpreting missing values and renaming variables.\n\nmotorcycles &lt;- motorcycles_raw %&gt;% \n  mutate(across(c(price, Kilometer), ~ gsub(\"'\", \"\", .x)),\n         across(c(price, Kilometer, `Hubraum (Ccm)`), ~ parse_number(.x)),\n         across(c(Marke, Modell, Getriebeart, Treibstoff, Aussenfarbe,\n                  Antrieb, Art), ~ trimws(.x)),\n         across(`Letzte Änderung`, ~ lubridate::dmy(.x)),\n         across(`Ab MFK`, ~ case_when(is.na(.x) ~ \"no\", .x == \"\" ~ \"yes\"))) %&gt;% \n  rename(brand = Marke, model = Modell, mileage_km = Kilometer, \n         year = Baujahr, transmission = Getriebeart, fuel = Treibstoff,\n         colour = Aussenfarbe, last_edited = `Letzte Änderung`,\n         id = Inseratnummer, drive = Antrieb, mfk = `Ab MFK`,\n         price_chf = price, type = Art, condition = Zustand, \n         displacement = `Hubraum (Ccm)`) %&gt;% \n  filter(`Art des Inserats` == \"Angebot\",\n         type %in% c(\"Gegenstand\", NA),\n         mileage_km &lt; 1e6,\n         price_chf &lt; 1e6) %&gt;% \n  select(- c(`Art des Inserats`, type))\n\nWith that, the clean data set is ready and I have all the available motorcycles on Anibis as of September 2022 loaded into memory, ready to be visualised and modelled.\n\nglimpse(motorcycles)\n\nRows: 22,432\nColumns: 15\n$ listing_no   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18, 20…\n$ price_chf    &lt;dbl&gt; 6500, 5500, 4000, 1500, 2400, 29000, 5000, 1100, 5300, 10…\n$ condition    &lt;chr&gt; \"Neuwertig\", \"Geringe Gebrauchsspuren\", \"Geringe Gebrauch…\n$ displacement &lt;dbl&gt; 500, 800, 600, 1200, 50, 1000, 125, 800, 350, 750, 750, 1…\n$ year         &lt;dbl&gt; 2014, 2014, 2013, 1989, 2013, 2019, 2019, NA, 2017, 2021,…\n$ brand        &lt;chr&gt; \"YAMAHA\", \"YAMAHA\", \"YAMAHA\", \"YAMAHA\", \"BETA\", \"POLARIS\"…\n$ model        &lt;chr&gt; \"450 wrf\", \"Mt-07\", NA, \"FJ 1200\", \"50 RR\", \"RZR 1000 tur…\n$ mileage_km   &lt;dbl&gt; 5660, 41000, 20000, 7156, 14500, 2700, 3600, 60000, 8100,…\n$ transmission &lt;chr&gt; \"Schaltgetriebe\", NA, \"Schaltgetriebe\", \"Schaltgetriebe\",…\n$ fuel         &lt;chr&gt; \"Benzin\", \"Benzin\", \"Benzin\", \"Benzin\", \"2-Takt-Gemisch\",…\n$ colour       &lt;chr&gt; \"Blau\", \"Rot\", \"Schwarz\", \"Grau\", \"Rot\", NA, \"Orange\", NA…\n$ last_edited  &lt;date&gt; 2022-09-05, 2022-09-05, 2022-09-05, 2022-09-05, 2022-09-…\n$ id           &lt;dbl&gt; 44818239, 44818276, 44818264, 44818210, 44818257, 4481803…\n$ mfk          &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"…\n$ drive        &lt;chr&gt; NA, NA, NA, \"Kette\", NA, NA, NA, NA, \"Kette\", \"Hinterrada…\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\nBefore training a model, I will explore the clean data set and gauge relations between variables. Let’s first take a look at the frequency of categorical predictors. I lumped levels beyond \\(N=15\\) together, so that the y axis can be read properly.\n\nmotorcycles %&gt;% \n  select(where(is.character)) %&gt;% \n  select(-c(model)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  drop_na() %&gt;% \n  group_by(name) %&gt;%  \n  mutate(value = fct_lump(value, n = 15)) %&gt;% \n  count(value) %&gt;% \n  mutate(value = reorder_within(value, n, name)) %&gt;% \n  ggplot(aes(n, value)) +\n  geom_col(fill = \"midnightblue\", alpha = 0.8)  +\n  facet_wrap(~ name, scales = \"free\", ncol = 4) +\n  labs(title = \"Frequency Of Used Motorcycle Properties on anibis.ch\",\n       subtitle = \"Sample Size = 22422 | Data as of 09/22\",\n       y = NULL,\n       x = \"Count\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_reordered() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nMost common brands are Yamaha, Honda, Kawasaki, Harley Davidson and BMW. Most motorcycles are black, grey or silver - neutral colours. Most motorcycles on Anibis are in used condition. Virtually all motorcycles use petrol instead of diesel and are manual. There is quite a significant number of motorcycles without MFK.\nNext up, let’s inspect the numerical variables:\n\nmotorcycles %&gt;% \n  select(where(is.numeric)) %&gt;% \n  select(-c(listing_no, id)) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(value)) +\n  stat_ecdf() +\n  facet_wrap(~ name, scales = \"free\") +\n  labs(title = \"Cumulative Distribution Of Used Motorcycle Characteristics on anibis.ch\",\n       subtitle = \"Sample Size = 22422 | Data as of 09/22\",\n       y = NULL,\n       x = NULL) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        panel.grid.major.y = element_blank())\n\n\n\n\nDisplacement is surprisingly close to uniformly distributed, though very high numbers are a bit less frequent and medium-sized motorcycles are most common. The steep steps almost make it seem like a categorical variable. Mileage and price suffer from large outliers, which might be faulty data in the first case. The last could use a log-scale. Lastly, most motorcycles (about 90%) are not older than 20 years.\n\nSome interesting plots\nBefore doing EDA and training a model, let’s first draw a few interesting charts, not related to any purpose except for fun.\n\nmotorcycles %&gt;% \n  select(brand, displacement) %&gt;% \n  drop_na() %&gt;% \n  mutate(brand = fct_lump(brand, n = 15),\n         brand = fct_reorder(brand, displacement, .desc = TRUE)) %&gt;%\n  ggplot(aes(displacement, brand)) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Displacement Of Used Motorcycles on anibis.ch\",\n       subtitle = \"Sample Size: N = 5987 | as of 09/22 | excluding outliers\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" ccm\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\nInterestingly, less frequent brands lumped into “Other” sell smaller motorcycles. Likely, these brand target the budget range. Choppers have huge displacement, central European producers have large median displacement and Asian producers like Honda are a bit smaller.\n\nmotorcycles %&gt;% \n  select(brand, price_chf) %&gt;%\n  drop_na() %&gt;%  \n  mutate(brand = fct_lump(brand, n = 20),\n         brand = fct_reorder(brand, price_chf, .desc = TRUE)) %&gt;%\n  ggplot(aes(price_chf, brand)) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Used Motorcycle Prices on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | excluding outliers\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" CHF\")) +\n  coord_cartesian(xlim = c(0, 40000)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\nThere are quite substantial price differences between motorcycle producers. Choppers are very expensive, while some lesser known budget options have medians below 5,000 CHF.\n\nmotorcycles %&gt;% \n  select(year, price_chf, brand) %&gt;% \n  drop_na() %&gt;% \n  mutate(age = 2023 - year) %&gt;% \n  filter(age &lt; 20,\n         age &gt; 0,\n         fct_lump(brand, n = 10) != \"Other\") %&gt;% \n  group_by(brand, age) %&gt;% \n  summarise(median_price = median(price_chf)) %&gt;% \n  mutate(change = median_price/first(median_price)) %&gt;% \n  select(age, brand, change) %&gt;% \n  ggplot(aes(age, change, colour = brand)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0.5, lty = \"dashed\", colour = \"grey50\") +\n  facet_wrap(~ brand) +\n  expand_limits(y = 0) +\n  labs(title = \"Used Motorcycle Price Change By Age on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | Age 1 is 100%\",\n       x = \"Vehicle Age\",\n       y = \"Price (as %) compared to age = 1\") +\n  scale_y_continuous(labels = percent_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\nThis chart depicts the value of motorcycles at each age up to 20 years for a given brand. It is not possible to draw conclusions regarding depreciation of a defined model group, given the heterogeneity of the available models at each age class across brands. However, assuming flat or linearly increasing median prices across the offering for each brand, we can approximately gauge the stability of prices. The later the curves cross the 50% line, the higher price stability. Harley Davidson, Kawasaki, Yamaha and Honda have very stable prices. In contrast, BMW, Aprilia and Ducati seem to suffer greater price declines with age.\n\nmotorcycles %&gt;% \n  select(condition, price_chf) %&gt;%\n  drop_na() %&gt;%   \n  ggplot(aes(price_chf, condition %&gt;% fct_reorder(price_chf))) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Used Motorcycle Prices on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | excluding outliers\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" CHF\")) +\n  coord_cartesian(xlim = c(0, 21000)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\nThere is not a lot to say about this one, except that new motorcycles cost more than used ones and worse condition implies lower prices, which is intuitive.\n\nmotorcycles %&gt;% \n  add_count(colour) %&gt;% \n  filter(n &gt; 500) %&gt;% \n  select(colour, price_chf, n) %&gt;%\n  drop_na() %&gt;% \n  mutate(colour = paste0(colour, \" (n=\", n, \")\")) %&gt;% \n  ggplot(aes(price_chf, colour %&gt;% fct_reorder(price_chf))) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Used Motorcycle Prices on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | excluding outliers\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" CHF\")) +\n  coord_cartesian(xlim = c(0, 35000)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\nInterestingly, there exists a relation between prices and colour of motorcycles.\n\nmotorcycles %&gt;%\n  select(brand, colour) %&gt;% \n  drop_na() %&gt;% \n  group_by(brand = fct_lump(brand, n = 10)) %&gt;% \n  count(colour) %&gt;% \n  filter(n &gt; 15) %&gt;% \n  ggplot(aes(y = colour %&gt;% reorder_within(by = n, within = brand),\n             x = n)) +\n  geom_col() +\n  facet_wrap(~ brand, scales = \"free\") +\n  labs(title = \"Motorcycle Colours By Brand on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | showing most frequent colours and brand\",\n       x = \"Count\",\n       y = NULL) +\n  scale_y_reordered() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\nColours reveal the design identities of certain brands: Kawasaki Green, Ducati Red, Suzuki Blue and KTM Orange.\n\n\nEDA\nLet’s now inspect the relation of the predictors with the target variable. Intuitively, it makes sense that mileage, as well as age and displacement have the strongest relation with price. Additionally, as seen before, condition and brand also play an important role. Let’s just look at the first two and exclude outliers.\n\nset.seed(12)\n\nmotorcycles %&gt;% \n  sample_n(1000) %&gt;% \n  mutate(age = 2022 - year) %&gt;% \n  select(price_chf, mileage_km, displacement, age) %&gt;% \n  drop_na() %&gt;% \n  filter(mileage_km &lt; 100000,\n         age &lt; 40) %&gt;% \n  pivot_longer(-price_chf) %&gt;% \n  ggplot(aes(value, price_chf)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = F) +\n  facet_wrap(~ name, scales = \"free\") +\n  scale_y_continuous(labels = comma_format(suffix = \" CHF\")) +\n  theme_bw()\n\n\n\n\nClearly, the intuition holds. However, not all the variation is explained and the relationship is not as strict, as expected. Presumably, there is a lot of information in the brands as well as the model names. Let’s look at that. Extracting the model specifications from the titles by tokenisation and then removing punctuation and numbers, I calculate the median price for the 30 most frequent model names. What becomes apparent is the disparity in median prices between brand names. Clearly, tokenisation is a way with which the model will achieve greater accuracy.\n\nmotorcycles %&gt;% \n  unnest_tokens(input = model, output = \"tokens\") %&gt;% \n  select(tokens, price_chf) %&gt;% \n  drop_na() %&gt;% \n  mutate(tokens = tokens %&gt;% \n           tm::removeNumbers() %&gt;% \n           tm::removePunctuation()) %&gt;% \n  filter(tokens != \"\") %&gt;% \n  mutate(tokens = fct_lump(tokens, n = 30)) %&gt;% \n  group_by(tokens) %&gt;% \n  summarize(median_price = median(price_chf), \n            n = n()) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(median_price, tokens %&gt;% fct_reorder(median_price))) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Motorcycle Model Names and Median Prices on anibis.ch\",\n       subtitle = \"sample size: n = 21242 | as of 09/22 | showing 30 most frequent tokens\",\n       x = \"Median Price\",\n       y = NULL) +\n  scale_x_continuous(labels = comma_format(suffix = \" CHF\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\n\n\n\nFitting A Model\n\nFirst, the data is split into training and testing sets. Also, three-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.\n\nmotorcycles &lt;- motorcycles %&gt;% \n  mutate(across(where(is.character), as.factor))\n\n\ndt_split &lt;- initial_split(motorcycles)\n\ndt_train &lt;- training(dt_split)\ndt_test &lt;- testing(dt_split)\n\nfolds &lt;- vfold_cv(dt_train, v = 5)\n\nThe recipe in the tidymodels framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit.\n\nmodel_rec &lt;- recipe(price_chf ~ .,\n                 data = dt_train) %&gt;%\n  step_rm(listing_no, id, last_edited) %&gt;% \n  step_impute_median(all_numeric_predictors()) %&gt;% \n  step_impute_mode(all_nominal_predictors()) %&gt;% \n  step_tokenize(model) %&gt;% \n  step_stopwords(model) %&gt;% \n  step_tokenfilter(model, max_tokens = 300) %&gt;% \n  step_tf(model) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_scale(all_numeric_predictors()) %&gt;% \n  step_center(all_numeric_predictors())\n\nSetting up the model specifications with tuning options for hyperparameters:\n\ngb_spec &lt;- \n  boost_tree(\n    trees = 1000,\n    tree_depth = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    mtry = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nen_spec &lt;- linear_reg(penalty = tune(),\n                      mixture = tune()) %&gt;% \n  set_engine(\"glmnet\")\n\nSpecifying the workflows by adding the model specifications and the recipes:\n\ngb_wflow &lt;- workflow() %&gt;% \n  add_recipe(model_rec) %&gt;% \n  add_model(gb_spec)\n\nen_wflow &lt;- workflow() %&gt;% \n  add_recipe(model_rec) %&gt;% \n  add_model(en_spec)\n\nSetting up the space-filling design tuning grids for hyperparameters of both models:\n\ngb_grid &lt;- \n  grid_latin_hypercube(\n    tree_depth(),\n    min_n(),\n    loss_reduction(),\n    sample_size = sample_prop(),\n    finalize(mtry(), dt_train),\n    learn_rate(),\n    size = 20\n  )\n\nen_grid &lt;- \n  grid_latin_hypercube(\n    penalty(),\n    mixture(),\n    size = 50\n    )\n\nTuning hyperparameters:\n\n# Gradient Boosting\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\ngb_tune &lt;- tune_grid(object = gb_wflow,\n                     resamples = folds,\n                     grid = gb_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\nTime difference of 7.960418 mins\n\n# Elastic Net\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\nen_tune &lt;- tune_grid(object = en_wflow,\n                     resamples = folds,\n                     grid = en_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\nTime difference of 1.449482 mins\n\n\nLooking at the tuning results reveals that the model captures strong signal in the predictors, as the \\(R^2\\) is fairly high. Gradient boosting works better than the elastic net, in this case, though.\n\ngb_tune %&gt;% \n  show_best(metric = \"rsq\") %&gt;% \n  transmute(model = \"Gradient Boosting\", .metric, mean, n, std_err)\n\n# A tibble: 5 × 5\n  model             .metric  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Gradient Boosting rsq     0.586     5  0.0174\n2 Gradient Boosting rsq     0.564     5  0.0173\n3 Gradient Boosting rsq     0.559     5  0.0186\n4 Gradient Boosting rsq     0.497     5  0.0156\n5 Gradient Boosting rsq     0.485     5  0.0152\n\nen_tune %&gt;% \n  show_best(metric = \"rsq\") %&gt;% \n  transmute(model = \"Elastic Net\", .metric, mean, n, std_err)\n\n# A tibble: 5 × 5\n  model       .metric  mean     n std_err\n  &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Elastic Net rsq     0.534     5  0.0227\n2 Elastic Net rsq     0.534     5  0.0227\n3 Elastic Net rsq     0.534     5  0.0227\n4 Elastic Net rsq     0.534     5  0.0227\n5 Elastic Net rsq     0.534     5  0.0227\n\n\n\ngb_final_wflow &lt;- gb_wflow %&gt;%\n  finalize_workflow(select_best(gb_tune, metric = \"rmse\"))\n\ngb_final_fit &lt;- gb_final_wflow %&gt;% \n  last_fit(dt_split)\n\ngb_final_fit %&gt;%\n  extract_workflow() %&gt;% \n  extract_fit_parsnip() %&gt;%\n  vi() %&gt;%\n  slice_max(order_by = Importance, n = 20) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance))) +\n  geom_col(fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = \"Only the most important predictors are shown.\",\n       y = \"Predictor\",\n       x = \"Relative Variable Importance\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\n\nen_final_wflow &lt;- en_wflow %&gt;%\n  finalize_workflow(select_best(en_tune, metric = \"rsq\"))\n\nen_final_fit &lt;- en_final_wflow %&gt;% \n  last_fit(dt_split)\n\nen_final_fit %&gt;%\n  extract_workflow() %&gt;% \n  extract_fit_parsnip() %&gt;%\n  vi() %&gt;% \n  slice_max(order_by = Importance, n = 30) %&gt;% \n  mutate(Importance = ifelse(Sign == \"NEG\", Importance * -1, Importance)) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance),\n             fill = Sign)) +\n  geom_col(colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = \"Only the most important predictors are shown.\",\n       y = \"Predictor\",\n       x = \"Coefficient\") +\n  ggsci::scale_fill_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nFrom the variable importance, it becomes clear that\n\nbrand names are important for prices;\ninformation on the model name is vital for prediction;\nnewer models tend to be more expensive;\nhigh mileage decreases the price;\ndisplacement and price are correlating positively.\n\n\n\n\n\nEvaluating Model Performance On The Holdout Data\n\n\n\n\n\n\nThe out-of-sample fit is fairly good. Naturally, prices being set by humans, there must remain error for comparable bikes. Notably, the model fails to predict prices on the far right side of the price distribution. Assuming that these motorcycle models are rare and potentially occur for the first time in the holdout set, naturally, the model doesn’t manage to be very precise with them.\nI hope this post has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out.\nThank you for reading.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nForecasting Swiss Electricity Demand\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\nThe goal of this project is to test the ability of supervised machine learning models, namely elastic net, gradient boosting and random forest, to predict end-user electricity consumption in Switzerland.\n\n\n\n\n\n\nMar 9, 2023\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nSchweizerische Wasserkraft\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nVisualisierungen über die Standorte, Arten und Füllungsgrade der Wasserkraft in der Schweiz mit Daten des Bundesamts für Energie\n\n\n\n\n\n\nDec 15, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nLuxembourgish Student Aid\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nAnalysing country level data of Luxembourgish financial aid for students, which gives insights into where students go, what they study and what subjects are prevalent by sex\n\n\n\n\n\n\nNov 14, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nUsing Classification Models and Gain Curves For Profit Maximisation\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\nThis post shows how customer churn classification models can be set up, trained and used to create financial value within organisations, specifically using gain curves and classification thresholds.\n\n\n\n\n\n\nOct 25, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nStacking Models To Predict Airbnb Prices In Manhattan (NYC)\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\nThe purpose of this post is to demonstrate the usefulness of the package stacks in blending individual machine learning models together into a linear combination of them (using LASSO regression), often increasing final model performance.\n\n\n\n\n\n\nOct 7, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nDeploying A Model Predicting Used Car Prices With Shiny\n\n\n\n\n\n\n\nMachine Learning\n\n\nWeb Scraping\n\n\nR\n\n\n\n\nThe data for this post was scraped from a Swiss online platform. The goal of this post is to demonstrate a deployed model with Shiny, which allows users to enter information about their car and get back a prediction for the price in Swiss francs. A bonus is using tokenisation for extracting predictive information from the natural language title of each listing.\n\n\n\n\n\n\nSep 23, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nScraping and Modelling Used Motorcycles\n\n\n\n\n\n\n\nMachine Learning\n\n\nWeb Scraping\n\n\nR\n\n\n\n\nThis blog post is about how to create your own data sets by scraping websites with rvest. Specifically, I will scrape all available used motorcycles from a Swiss online platform, explore the data, make conclusions and train a model on the prices.\n\n\n\n\n\n\nSep 19, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nBirthday Problem\n\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\n\nThe birthday problem is the surprising fact that in a group of just 23 people, there’s a 50-50 chance that two of them share the same birthday, which is way more likely than expected by most.\n\n\n\n\n\n\nAug 9, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nUS Wind Energy\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nVisualising the wind turbine data base from USGS, which covers 71,666 wind turbines in the United States over a period from 1981 to 2022.\n\n\n\n\n\n\nAug 9, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nSwiss End-User Electricity Consumption\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nVisualising Swiss End-User Electricity Consumption data provided by Swissgrid over the past decade\n\n\n\n\n\n\nAug 9, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nAlternative Fuel Stations in the US\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nVisualising alternative fuelling stations in the US\n\n\n\n\n\n\nJul 29, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nThe St. Petersburg Paradox\n\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\n\nThe St. Petersburg paradox is a puzzling situation where a simple gambling game seems to promise a lot of money, but most people wouldn’t be willing to pay much to play it, even though the math suggests they should.\n\n\n\n\n\n\nJul 16, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nDo Known Mutual Fund Brands Outperform?\n\n\n\n\n\n\n\nData Visualisation\n\n\nR\n\n\n\n\nAnalysing if mutual funds from better known fund managers performed better as opposed to mutual funds from lesser known fund managers\n\n\n\n\n\n\nJul 8, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nDo Moving Averages Convey Information About Stock Price Movements?\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\nThere is plethora of day trading coaches claiming technical indicators like moving averages are useful for timing the market in stock trading. With this short post, I want to investigate whether supervised machine learning models like random forests are capable of extracting information from moving averages (technical indicator) about the future direction of a stock on a daily basis.\n\n\n\n\n\n\nJun 30, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nApplication Of Machine Learning Methods To Mutual Fund Selection\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\nThis is an executive summary of my bachelor’s thesis on machine learning in mutual fund selection. The main findings and implications with regard to practice are presented.\n\n\n\n\n\n\nJun 3, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nHow Often Should You Check Your Portfolio?\n\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\n\nOnce you have invested in the stock market, it is hard not to check your portfolio every morning. But how often should you check it to maximise your mental health? The quick answer is: As infrequently as possible.\n\n\n\n\n\n\nJun 2, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\n  \n\n\n\n\nThe Monty Hall Dilemma\n\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\n\nThe Monty Hall problem is a brain-teasing scenario in which you’re asked to choose between three doors, with a hidden prize behind one of them, and switching your choice after one door is revealed can dramatically increase your chances of winning, which might sound counterintuitive at first.\n\n\n\n\n\n\nMay 31, 2022\n\n\nMathias Steilen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! 👋",
    "section": "",
    "text": "I’m a graduate statistics student at ETH Zurich in Switzerland with a deep interest in machine learning engineering and operations. I hold a BA in Economics from the University of St.Gallen, during which I have discovered a passion for discovering meaning in data and which allows me to connect the rigorous quantitative methods taught at ETH to the business perspective of the real world.\nIn my part-time role as a Junior Operations Research Analyst at BKW, I employ predictive modelling techniques and data analysis, predominantly using Python and R and assist the asset optimisation team by creating actionable and digestible insights from data.\nIn addition to my academic and professional interests, I have a passion for outdoor sports (🏃), human-powered (🚲) and motorised (🏍️) two-wheelers, and a background in boxing (🥊).\nThank you very much for visiting and feel free to reach out for feedback or to connect over my socials."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hello! 👋",
    "section": "Education",
    "text": "Education\n\nETH Zürich | Zurich, Switzerland\nMSc in Statistics | Sep 2023 – Present\n\nAnticipated graduation in 2025\n\n\nUniversity of St. Gallen (HSG) | Switzerland\nM.A. in Banking and Finance (MBF) | Sep 2022 – Feb 2023 (discontinued)\n\nSelected coursework: Applications of Machine Learning, Programming in Advanced Computer Languages, Predictive Econometrics, Quantitative Methods\nDiscontinued due to a desire to pursue quantitative methods deeper at ETHZ\n\n\nUniversity of St. Gallen (HSG) | St. Gallen, Switzerland\nB.A. in Economics | Sep 2019 – Jul 2022\n\nThesis: Application of Machine Learning to Mutual Fund Selection (5.75/6.00)\nFocus on Data Handling and Statistical Modelling\nSelected Course Work: Mathematics for Economists, Statistics, Causal Econometrics, Data Analytics, Fundamentals of Computer Science, Monte Carlo and Quantitative Decision Support, Development of Web Applications, Corporate Finance, Corporate Valuation, Money Banking and Finance"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hello! 👋",
    "section": "Experience",
    "text": "Experience\n\nBKW | Berne, Switzerland\nJunior Operations Research Analyst | Oct 2023 – Present\n\nSupporting the operations research team with predictive modeling, data analysis, and data visualisation\nMajority of tasks involve Python (Pandas, Polars, PyTorch, Scikit-learn, Plotly, Shiny) and R (Tidymodels, Shiny, ggplot2)\n\n\nBKW | Berne, Switzerland\nInternship – Machine Learning Intern in Asset Optimization | Apr 2023 – Sep 2023\n\nResponsible for a feasibility study on the application of reinforcement learning algorithms for the optimisation of flexible powerplants in the day-ahead markets\nOther side-projects: Fine-tuning deep learning models for load forecasting, developing interactive Shiny dashboards, backtesting trading strategies in the forward markets\n\n\nBergos Privatbank | Zurich, Switzerland\nInternship – Active Advisory | Jul 2021 – Sep 2021\n\nBuy-side investment advisory for private clients across asset classes (Focus on Equities and Alternatives)\nAutomation of monitoring tasks (e.g. automated downgrade alerts, gap analysis for missing data on funds)\n\n\nPwC | Luxembourg\nInternship – Regulatory & Compliance Advisory Services | Apr 2019 – May 2019\n\nOptimisation of Excel models and creation of VBA applications for Anti-Money-Laundering screenings (web scraping, automated documentation)"
  },
  {
    "objectID": "posts/20220416-Birthday-Problem/index.html",
    "href": "posts/20220416-Birthday-Problem/index.html",
    "title": "Birthday Problem",
    "section": "",
    "text": "What is the birthday problem?\n\n\nIn probability theory, the birthday problem asks for the probability that, in a set of n randomly chosen people, at least two will share a birthday. The birthday paradox is that, counterintuitively, the probability of a shared birthday exceeds 50% in a group of only 23 people. (Source)\n\n\n\n\n\nSolving the Birthday Problem\n\nTo answer what the probability of at least two birthdays falling together is, I simulate 5,000 trials from 1 to 100 people and overlay the empirically obtained results with the closed-form mathematical solution in a plot. The results show that 5,000 simulations for any number of people is sufficient to obtain a fairly stable result, which nicely matches the closed form solution.\n\n\nCode\n# params\nsimulations = 5000\nplayers = c(2:100)\nresmat = matrix(ncol = 3, nrow = length(players))\nresmat[,1] = players\nresmat[1,3] = 365/365 * 364/365\n\nfor (i in 2:length(players)){\n  resmat[i,3] = resmat[i-1,3] * (365-i)/365\n}\nresmat[,3] = 1 - resmat[,3]\n\n#simulation\nfor (v in players){\n\n  valmat &lt;- matrix(ncol = simulations, nrow = v)\n  occvec &lt;- vector(mode = \"logical\", length = simulations)\n  \n  for (i in 1:simulations){\n    \n    valmat[,i] = sample(x = 1:365, size = v, replace = TRUE)\n    occvec[i] = sum(duplicated(valmat[,i])) &gt; 0\n    \n  }\n  \n  resmat[v-1,2] = sum(occvec)/simulations\n}\n\n#convert to data frame for ggplot\nresmat &lt;- resmat %&gt;% as.data.frame()\n\n#confidence levels\nfifty_percent_confidence = min(which(resmat$V3 &gt; 0.5) + 1)\nninetynine_percent_confidence = min(which(resmat$V3 &gt; 0.99) + 1)\n\n\n\n\nCode\nggplot(resmat, aes(x = V1)) +\n  geom_line(aes(y = V2, colour = \"Simulation\"), lty = \"solid\", \n  linewidth = 1) +\n  geom_line(aes(y = V3, colour = \"Closed Form\"), lty = \"dashed\", \n  linewidth = 0.5) +\n  labs(title = \"A Birthday Problem\",\n       subtitle = \"How many people would you put into a room until you are 99% \\ncertain that at least two people share the same birthday? \",\n       caption = paste(simulations, \"simulations for every number 'n' people\")) +\n  guides(linetype = \"none\") +\n  labs(x = \"People in a Room\",\n       y = \"Prob. of at least 2 birthdays falling together\",\n       colour = NULL) +\n  geom_segment(aes(x = fifty_percent_confidence,\n                   y = 0,\n                   xend = fifty_percent_confidence,\n                   yend = resmat[fifty_percent_confidence-1,3]),\n               lty = 3, colour = \"black\") +\n  geom_segment(aes(x = ninetynine_percent_confidence,\n                   y = 0,\n                   xend = ninetynine_percent_confidence,\n                   yend = resmat[ninetynine_percent_confidence-1,3]),\n               lty = 3, colour = \"black\") +\n  annotate(geom = \"text\", size = 3, x = 27, y = 0.25,\n           label = \"50%\", fontface = \"italic\") +\n  annotate(geom = \"text\", size = 3, x = 61, y = 0.49,\n           label = \"99%\", fontface = \"italic\") +\n  scale_colour_manual(values = c(\"black\", \"dodgerblue\")) +\n  scale_x_continuous(limits = c(1, 100), breaks = c(seq(0, 100, by=50), 23, 57)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_bw() +\n  theme(text=element_text(size = 11, color = \"black\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title=element_text(size=14, face = \"bold\", color=\"black\"),\n        plot.subtitle=element_text(size=12, face=\"italic\", color=\"grey50\"),\n        plot.caption=element_text(size=6, face=\"italic\", color=\"grey50\"))\n\n\n\n\n\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220602-Portfolio-Checking/index.html",
    "href": "posts/20220602-Portfolio-Checking/index.html",
    "title": "How Often Should You Check Your Portfolio?",
    "section": "",
    "text": "The Short Answer\n\nThe short answer is: as infrequently as possible. The more often you check, the higher the likelihood of noise in the returns pulling your cumulative performance into the red and subsequently causing you unnecessary distress.\n\n\n\nSome Assumptions\n\nFor the simulations, normally distributed returns \\(X\\) with mean \\(\\mu = 0.1\\) and standard deviation \\(\\sigma = 0.15\\) are assumed.\n\n\\(X \\sim \\mathcal{N}(\\mu = 10\\% , \\ \\sigma = 15\\%)\\)\n\nSummarising this distribution of annual returns, the CDF of the normal distribution will be used giving the investor looking at their portfolio at the end of the year two possible outcomes with probabilities:\n\n\nPositive Portfolio Performance - Annually\n\n\n\nCode\n1-pnorm(0, mean = 0.1, sd = 0.15)\n\n\n[1] 0.7475075\n\n\n\n\nNegative Portfolio Performance - Annually\n\n\n\nCode\npnorm(0, mean = 0.1, sd = 0.15)\n\n\n[1] 0.2524925\n\n\n\n\n\n\nScaling Effects of Return Probabilities - The General Case\n\nChecking your portfolio under the given assumptions would give you positive news \\(75\\%\\) of the time. However, a scale effect exists wherein a higher frequency of checking the portfolio would lead to a lower percentage of positive news, even though the underlying annual performance still remains the same. Checking your portfolio more frequently exposes you to the noise in the market trajectory, that is its variance, and not actually worse performance.\n\n\nQuarterly example\n\nIn order to show the scale to the shorter time horizon, the parameters need to be adjusted accordingly. For the quarterly case:\n\\(\\mu_Q = \\frac{\\mu_A}{4}\\) and \\(\\sigma_Q = \\frac{\\sigma_A}{\\sqrt{4}}\\)\nHence we have:\n\nPositive outcome - Quarterly:\n\n\nCode\n1-pnorm(0, mean = 0.1/4, sd = 0.15/sqrt(4))\n\n\n[1] 0.6305587\n\n\n\nNegative outcome (Quarterly):\n\n\nCode\npnorm(0, mean = 0.1/4, sd = 0.15/sqrt(4))\n\n\n[1] 0.3694413\n\n\nFor one example, the percentage of \\(\\frac{Positive \\space News}{Total \\space News}\\) decreased with an increase the frequency of portfolio checks. The general case will be demonstrated by applying frequencies of \\(x\\) times per year.\n\n\n\nAn Illustration for N Portfolio Checks per Year\n\nWe have seen that an increase by 4 scales parameters in a way, where more frequent news seem to portray a more negative view on portfolio performance due to higher perceived noise in shorter time frames. In order to show the case for even more updates (up to weekly) per year, a visualisation will be used:\n\n\nCode\n# Initialise data frame\nfreq_plot_df &lt;- data.frame(matrix(ncol = 2, nrow = 52))\ncolnames(freq_plot_df) &lt;- c(\"X_Times_per_Year\", \"%_Good_News\")\n\n# Populate data frame\nfreq_plot_df &lt;- freq_plot_df %&gt;% \n  mutate(X_Times_per_Year = 1:52,\n         `%_Good_News` = 1 - pnorm(0,\n                                   mean = 0.1/X_Times_per_Year,\n                                   sd = 0.15/sqrt(X_Times_per_Year)))\nfreq_plot_df %&gt;% \nggplot(aes(x = `X_Times_per_Year`, y = `%_Good_News`)) +\n  geom_line() +\n  labs(title = \"How Often Should You Check Your Portfolio?\",\n       y = \"% Good News\",\n       x = \"Checking The Portfolio N Times Per Year\") +\n  scale_y_continuous(expand = c(0, 0), \n                     limits = c(0, 1), \n                     breaks = seq(0,1,0.1),\n                     labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(expand = c(0, 0), \n                     limits = c(0, max(freq_plot_df$X_Times_per_Year + 1)), \n                     breaks = seq(0,max(freq_plot_df$X_Times_per_Year),5)) +\n  theme_bw() +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nIt can be observed, that the percentage of positive news decreases and converges closer to 50% as \\(Frequency \\rightarrow \\infty\\). The deduction, as outlined by Nassim Taleb is the following:\n\n“Over the very narrow time increment, the observation will reveal close to nothing. […] Finally, this explains why people who look too closely at randomness burn out, their emotions drained by the series of pangs they experience. Regardless of what people claim, a negative pang is not offset by a positive one (some behavioral economists estimate the effect to be up to 2.5 the magnitude of a positive one); it will lead to an emotional deficit. […] When I see an investor monitoring his portfolio with live prices on his cellular phone or his PalmPilot, I smile and I smile.”\n\nDear Nassim, I promise to never check live prices on my PalmPilot ever again.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220628-US-Wind/index.html",
    "href": "posts/20220628-US-Wind/index.html",
    "title": "US Wind Energy",
    "section": "",
    "text": "Description of the Data\n\nThe data comes from USGS, who regularly update their Wind Turbine Data Base for the US. It covers 71,666 wind turbines in the United States over a period from 1981 to 2022.\nTo show only the results, large code chunks are hidden, but can be unfolded by clicking the “Code” boxes on the top right of each hidden code chunk.\n\n\n\nData Cleaning\n\nNext steps are the removal of unnecessary variables and the cleaning of missing observations.\n\n\nCode\nwind &lt;- \n  wind %&gt;% \n  select(-c(case_id, faa_ors, faa_asn, usgs_pr_id, t_img_date, \n            t_img_srce, eia_id, t_fips)) %&gt;% \n  mutate(t_manu = ifelse(t_manu == \"\", NA, t_manu),\n         t_model = ifelse(t_model == \"\", NA, t_model))\n\n\nLooking at missing observations:\n\n\nCode\ncolMeans(is.na(wind)) %&gt;% \n  tidy() %&gt;% \n  rename(pct = x) %&gt;% \n  mutate(names = fct_reorder(names, pct)) %&gt;% \n  filter(pct &gt; 0) %&gt;% \n  ggplot(aes(pct, names)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Missing Data In Variables\",\n       subtitle = \"Percent missingness calculated for each column\n       \",\n       y = NULL,\n       x = NULL) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     limits = c(0,1)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\n\nIt is visible that turbines which have not been retrofit don’t exhibit any data in the year of retrofitting, which is not a problem for later visualisation. Additionally, there are low levels of missing data across other variables, such as the total height, hub height, rotor swept area, rotor diameter, turbine model, manufacturer, turbine capacity, project capacity and year of project completion. All of the latter will not be imputed, as this blog post does not entail modelling, but visualisation exclusively.\n\n\nCode\nglimpse(wind)\n\n\nRows: 71,666\nColumns: 19\n$ t_state       &lt;chr&gt; \"CA\", \"CA\", \"CA\", \"IA\", \"IA\", \"IA\", \"IA\", \"IA\", \"IA\", \"I…\n$ t_county      &lt;chr&gt; \"Kern County\", \"Kern County\", \"Kern County\", \"Story Coun…\n$ p_name        &lt;chr&gt; \"251 Wind\", \"251 Wind\", \"251 Wind\", \"30 MW Iowa DG Portf…\n$ p_year        &lt;int&gt; 1987, 1987, 1987, 2017, 2017, 2017, 2017, 2017, 2017, 20…\n$ p_tnum        &lt;int&gt; 194, 194, 194, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 2…\n$ p_cap         &lt;dbl&gt; 18.43, 18.43, 18.43, 30.00, 30.00, 30.00, 30.00, 30.00, …\n$ t_manu        &lt;chr&gt; \"Vestas\", \"Vestas\", \"Vestas\", \"Nordex\", \"Nordex\", \"Norde…\n$ t_model       &lt;chr&gt; NA, NA, NA, \"AW125/3000\", \"AW125/3000\", \"AW125/3000\", \"A…\n$ t_cap         &lt;int&gt; 95, 95, 95, 3000, 3000, 3000, 3000, 3000, 3000, 3000, 30…\n$ t_hh          &lt;dbl&gt; NA, NA, NA, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87…\n$ t_rd          &lt;dbl&gt; NA, NA, NA, 125.0, 125.0, 125.0, 125.0, 125.0, 125.0, 12…\n$ t_rsa         &lt;dbl&gt; NA, NA, NA, 12271.85, 12271.85, 12271.85, 12271.85, 1227…\n$ t_ttlh        &lt;dbl&gt; NA, NA, NA, 150.0, 150.0, 150.0, 150.0, 150.0, 150.0, 15…\n$ retrofit      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ retrofit_year &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ t_conf_atr    &lt;int&gt; 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ t_conf_loc    &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ xlong         &lt;dbl&gt; -118.36441, -118.36420, -118.36376, -93.63284, -93.36780…\n$ ylat          &lt;dbl&gt; 35.07744, 35.07764, 35.07791, 41.88248, 42.49794, 41.742…\n\n\nAt this stage, we have a nice and large dataset, which can be used for visualisation. The following sections will answer questions I was interested in while looking at the data.\n\n\n\nHow Are Wind Projects Distributed Across The US?\n\n\n\nCode\nus &lt;- map_data('state')\n\nwind %&gt;% \n  filter(!is.na(p_year),\n         !is.na(p_cap)) %&gt;% \n  select(p_name, p_cap, xlong, ylat, p_year) %&gt;% \n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  mutate(p_year = cut(p_year, seq(1980, 2020, 10), dig.lab = 5)) %&gt;% \n  filter(dplyr::between(xlong, -125, -65),\n         dplyr::between(ylat, 20, 50)) %&gt;% # Removing Alaska and Islands\n  distinct() %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(x = xlong, y = ylat)) +\n  geom_polygon(data = us, aes(x = long, y = lat, group = group),\n               color = 'gray', fill = \"gray95\", alpha = 0.3) +\n  geom_point(aes(size = p_cap, colour = p_year), alpha = 0.25, shape = 19) +\n  scale_size_continuous(range = c(0.5, 4), \n                        labels = scales::comma_format(suffix = \" MW\")) +\n  xlim(-125, -65) + \n  ylim(20, 50) +\n  labs(title = \"Wind Energy Infrastructure in the USA\",\n       subtitle = \"Each dot constitutes one completed wind farm with size representing production capacity\",\n       y = NULL,\n       x = NULL,\n       size = \"Project Capacity:\",\n       colour = \"Period of Construction:\") +\n  theme_minimal() +\n  # coord_fixed() +\n  theme(panel.background = element_blank(),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\")) +\n  scale_colour_manual(values = c(\"firebrick\", \"darkorange\",\n                                 \"dodgerblue4\", \"dodgerblue\"))\n\n\n\n\n\n\n\n\nWhich States Are Forerunners For Wind Energy?\n\n\n\nCode\nwind %&gt;% \n  count(t_state) %&gt;% \n  mutate(t_state = usdata::abbr2state(t_state),\n         t_state = fct_reorder(t_state, n)) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = n,\n             y = t_state)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Number Of Wind Turbines In Each State In The US\",\n       subtitle = \"Some states exhibit no wind power at all\",\n       y = NULL,\n       x = \"Number of Wind Turbines\") + \n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\n\n\n\n\nWhen Have Most Wind Turbines Been Built Over The Available Period?\n\n\n\nCode\nwind %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(turbines = n()) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = p_year,\n             y = turbines)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Wind Turbines Built Each Year In The US\",\n       subtitle = \"The late 90s constitute a turning point for wind energy\",\n       y = \"Number of Wind Turbines\",\n       x = NULL) +\n  theme_bw() +\n  scale_y_continuous(labels = scales::comma_format()) +\n  scale_x_continuous(breaks = 1981:2022) +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\nHow Many Wind Turbines Are Usually In One Wind Park?\n\n\n\nCode\nwind %&gt;% \n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(p_tnum)) +\n  geom_histogram(fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Wind Turbines Per Project\",\n       subtitle = \"Many projects have either one or close to 100 turbines at a time\",\n       x = \"Number of Wind Turbines (Base 10 Log Scale)\",\n       y = \"Frequency\") +\n  scale_x_log10() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nWhat is this extremely large wind project with close to 1000 turbines?\n\n\nCode\nwind %&gt;% \n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  arrange(-p_tnum) %&gt;% \n  select(t_state, p_name, p_year, p_tnum) %&gt;% \n  head(10)\n\n\n# A tibble: 10 × 4\n   t_state p_name                                     p_year p_tnum\n   &lt;chr&gt;   &lt;chr&gt;                                       &lt;int&gt;  &lt;int&gt;\n 1 CA      unknown San Gorgonio Pass 1                  1981    731\n 2 CA      unknown Tehachapi Wind Resource Area 1       1982    713\n 3 CA      Mesa Wind Farm                               1983    460\n 4 OR      Stateline Wind Project                       2001    454\n 5 NM      Western Spirit                               2021    377\n 6 CA      Mojave 16, 17 & 18 (Desertwind III PPC Tru   1989    297\n 7 KS      Flat Ridge 2                                 2012    294\n 8 KS      Flat Ridge 2 Expansion                       2012    294\n 9 CO      Cedar Creek                                  2007    274\n10 CO      Peetz Table                                  2007    267\n\n\nIt looks like there are two of them and they are considerably larger and older than anything after them. Unfortunately, additional data for these two is missing.\n\n\n\nHow Has The Number Of Wind Turbines Per Project Developed Over Time?\n\n\n\nCode\nwind %&gt;% \n  filter(!is.na(p_tnum)) %&gt;%\n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(p_year, p_tnum) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = p_year %&gt;% as.factor(), y = p_tnum)) +\n  geom_boxplot(outlier.alpha = 0.4, fill = \"midnightblue\", \n               colour = \"midnightblue\", alpha = 0.4) +\n  labs(title = \"Wind Turbines Per Wind Farm\",\n       subtitle = \"Number of turbines per project have drastically decreased since the 1980s\",\n       y = NULL,\n       x = \"Year\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\nAnd how was the development of average and median projects over the past two decades?\n\n\nCode\nwind %&gt;% \n  filter(!is.na(p_tnum),\n         between(p_year, 2000, 2022)) %&gt;%\n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(p_year, p_tnum) %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(Mean = mean(p_tnum),\n            Median = median(p_tnum)) %&gt;% \n  pivot_longer(-c(p_year)) %&gt;% \n  ggplot(aes(x = p_year, y = value, colour = name)) +\n  geom_line() +\n  geom_point() +\n  geom_smooth(se = F, method = \"loess\", size = 0.5, lty = \"dashed\") +\n  labs(title = \"Wind Turbines Per Wind Farm Over The Two Most Recent Decades\",\n       subtitle = \"Number of turbines per project show a rebound from the pre-2000 drop\",\n       y = NULL,\n       x = \"Year\",\n       colour = NULL) +\n  scale_colour_manual(values = c(\"midnightblue\", \"dodgerblue\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nHow Has The Capacity Of Wind Turbines Developed Over Time?\n\n\n\nCode\nwind %&gt;% \n  filter(!is.na(t_cap)) %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(lower_band = quantile(t_cap, 0.1),\n            middle_band = mean(t_cap),\n            higher_band = quantile(t_cap, 0.9)) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = p_year, y = middle_band)) +\n  geom_line(colour = \"midnightblue\") +\n  geom_ribbon(aes(ymin = lower_band, ymax = higher_band),\n              fill = \"midnightblue\", colour = \"midnightblue\", \n              lty = \"dotted\", alpha = 0.4) +\n  labs(title = \"Wind Turbine Rated Capacity Over Time\",\n       subtitle = \"Confidence band represents the 10th and 90th percentile\",\n       y = NULL,\n       x = \"Year\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" kW\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nBearing in mind the decrease in wind turbines per project and the increase of wind turbine capacity, it would be interesting to complete the picture with total installed capacity over time:\n\n\n\nHow Has The Total Installed Wind Energy Capacity Developed Over Time?\n\n\n\nCode\nwind %&gt;% \n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(p_year, p_cap) %&gt;% \n  drop_na() %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(total_cap = sum(p_cap)) %&gt;% \n  ggplot(aes(x = p_year, y = total_cap)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Total Wind Turbine Capacity Installed By Year\",\n       subtitle = NULL,\n       y = NULL,\n       x = \"Year\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MW\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nIt is now visible, how the increase in wind turbine capacity and the slight creeping up of wind turbines per project since the mid-2000s lead to a strong increase in installed capacity by year.\nLooking at the cumulative picture:\n\n\nCode\nwind %&gt;% \n  group_by(p_name) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(p_year, p_cap) %&gt;% \n  drop_na() %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(total_cap = sum(p_cap)) %&gt;% \n  mutate(total_cap_cum = cumsum(total_cap)) %&gt;% \n  ggplot(aes(x = p_year, y = total_cap_cum)) +\n  geom_point(colour = \"midnightblue\") +\n  geom_line(colour = \"midnightblue\") +\n  labs(title = \"Cumulative Wind Turbine Capacity Installed Over Time In The US\",\n       subtitle = NULL,\n       y = NULL,\n       x = \"Year\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" MW\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\nHow Has The Size Of Wind Turbines Developed Over Time?\n\n\n\nCode\nwind %&gt;% \n  filter(!is.na(t_hh)) %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(lower_band_height = quantile(t_hh, 0.1),\n            middle_band_height = mean(t_hh),\n            higher_band_height = quantile(t_hh, 0.9)) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = p_year, y = middle_band_height)) +\n  geom_line(colour = \"midnightblue\") +\n  geom_ribbon(aes(ymin = lower_band_height, ymax = higher_band_height),\n              fill = \"midnightblue\", colour = \"midnightblue\", \n              lty = \"dotted\", alpha = 0.4) +\n  labs(title = \"Wind Turbine Hub Height Over Time\",\n       subtitle = \"Confidence band represents the 10th and 90th percentile\",\n       y = NULL,\n       x = \"Year\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" m\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\nCode\nwind %&gt;% \n  filter(!is.na(t_rd)) %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(lower_band_diameter = quantile(t_rd, 0.1),\n            middle_band_diameter = mean(t_rd),\n            higher_band_diameter = quantile(t_rd, 0.9),) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = p_year, y = middle_band_diameter)) +\n  geom_line(colour = \"midnightblue\") +\n  geom_ribbon(aes(ymin = lower_band_diameter, ymax = higher_band_diameter),\n              fill = \"midnightblue\", colour = \"midnightblue\", \n              lty = \"dotted\", alpha = 0.4) +\n  labs(title = \"Wind Turbine Rotor Diameter Over Time\",\n       subtitle = \"Confidence band represents the 10th and 90th percentile\",\n       y = NULL,\n       x = \"Year\") +\n  scale_y_continuous(labels = scales::comma_format(suffix = \" m\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nIt looks like the diameter has continually increased, while the hub height has stagnated. Let’s look at a ratio:\n\n\nCode\nwind %&gt;% \n  filter(!is.na(t_rd),\n         !is.na(t_hh)) %&gt;% \n  group_by(p_year) %&gt;% \n  summarise(ratio = t_rd/t_hh) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(x = p_year %&gt;% factor(), y = ratio)) +\n  geom_boxplot(outlier.colour = NA, alpha = 0.4,\n               fill = \"midnightblue\", colour = \"midnightblue\") +\n  labs(title = \"Wind Turbine Diameter In Relation To Hub Height Over Time\",\n       subtitle = \"Wind turbine height increases less strongly than wind turbine diameter\",\n       y = \"Diameter/Hub Height Ratio\",\n       x = \"Year\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\nIt becomes clear now, that the height of wind turbines is not increasing as strongly as the diameter of the rotors any more now. It seems like the optimal height for capturing wind has been reached, but that the trend of increasing wind turbine capacity leads to further increasing rotor diameters.\n\n\n\nWhat Are The Most Common Manufacturers?\n\n\n\nCode\nwind %&gt;% \n  count(t_manu) %&gt;% \n  drop_na() %&gt;% \n  slice_max(n = 15, order_by = n) %&gt;% \n  mutate(t_manu = fct_reorder(t_manu, n)) %&gt;% \n  ggplot(aes(x = n, y = t_manu)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Most Common Wind Turbine Manufacturers In The US\",\n       subtitle = NULL,\n       y = NULL,\n       x = \"Wind Turbines Installed\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220715-Mutual-Fund-Performance/index.html",
    "href": "posts/20220715-Mutual-Fund-Performance/index.html",
    "title": "Do Known Mutual Fund Brands Outperform?",
    "section": "",
    "text": "The Data\n\nThe data for this post comes from the CRSP Survivor-Bias-Free US Mutual Fund data base. It has been filtered for active equity mutual funds larger than USD 20 MM and aggregated to yearly values.\nThe question is: How have mutual funds from better known fund managers performed as opposed to mutual funds from lesser known fund managers. Let’s first look at the performance of the entire sample.\n\n\n\nHow Have Equity Mutual Funds Performed Over The Past Two Decades?\n\n\n\nCode\ngg1 &lt;- funds %&gt;% \n  filter(between(year, 2000, 2020)) %&gt;% \n  mutate(year = factor(year) %&gt;% fct_rev()) %&gt;% \n  ggplot(aes(y = year, x = yret)) +\n  geom_density_ridges(fill = \"dodgerblue\", colour = \"dodgerblue\",\n                      alpha = 0.5) +\n  geom_vline(xintercept = 0, lty = \"dashed\", colour = \"grey25\") +\n  labs(y = NULL,\n       x = \"Yearly Absolute Return\",\n       title = \"Equity Mutual Fund Performance (left) and Annual Realised Alpha (right)\",\n       subtitle = \"For funds larger than USD 20 million. Annual realised alphas were calculated with 36-months rolling window regressions\\non Carhart's four factors. Colour coding for annual realised alpha shows negative (red) and positive (blue) medians.\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        axis.title = element_text(size = 10)) +\n  scale_x_continuous(labels = scales::percent_format())\n\ngg2 &lt;- funds %&gt;% \n  filter(between(year, 2000, 2020)) %&gt;% \n  group_by(year) %&gt;%\n  mutate(mean_alpha = median(yrea_alpha_lagged)) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = factor(year) %&gt;% fct_rev(),\n         colour = ifelse(mean_alpha &lt; 0,\n                         \"Negative Median Alpha\",\n                         \"Positive Median Alpha\")) %&gt;% \n  ggplot(aes(x = yrea_alpha_lagged, y = year, fill = colour)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(outlier.colour = NA, width = 0.2, alpha = 0.5) +\n  geom_vline(xintercept = 0, lty = \"dashed\", colour = \"gray25\") +\n  scale_fill_manual(values = c(\"firebrick\", \"dodgerblue\")) +\n  labs(y = NULL,\n       x = \"Yearly Realised Alpha\",\n       title = \"\",\n       subtitle = \"\\n\",\n       fill = NULL) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(size = 12, face=\"bold\", colour=\"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        axis.title = element_text(size = 10),\n        legend.position = \"none\") +\n  scale_x_continuous(labels = scales::percent_format())\n\ngrid.arrange(gg1, gg2, ncol = 2)\n\n\n\n\n\n\n\n\nHow Do Funds By Well Known Managers Differ From Lesser Known Funds?\n\n\n\nCode\ndt %&gt;% count(mgmt_name, sort = T)\n\n\n# A tibble: 846 × 2\n   mgmt_name                                   n\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Fidelity Management & Research Company   1455\n 2 American Funds                           1030\n 3 MFS Investment Management                 744\n 4 Directed Services LLC                     663\n 5 John Hancock Group                        662\n 6 John Hancock Life Insurance Company       650\n 7 Lincoln Investment Advisors Corporation   608\n 8 JPMorgan Funds                            607\n 9 American Century Investment Mgmt Inc      596\n10 MassMutual Life Insurance Company         556\n# ℹ 836 more rows\n\n\nThere are 846 different management companies in the dataset. The big question is how to separate them into better and lesser known brands.\nBetter known brands probably have higher TNA across all their funds, as they attract more assets on average. However, this is not a very sophisticated approach, as subsidiaries of large and well known fund managers will slip into the “lesser known” bracket. Furthermore, if an investment company has some very large or very small funds deviating from their mean, this will also affect their status as “well known” or “less well known”. In this case of a short blog post however, this approach of using average fund size by investment firm will be sufficient. The TNA threshold set here is USD 200 MM, as it ensures balanced groups.\n\n\nCode\ntop_managers &lt;- dt %&gt;% \n  group_by(mgmt_name) %&gt;% \n  summarise(mean_tna = mean(tna_latest)) %&gt;% \n  arrange(desc(mean_tna)) %&gt;%\n  filter(mean_tna &gt; 200) %&gt;% \n  pull(mgmt_name)\n\nbottom_managers &lt;- dt %&gt;% \n  group_by(mgmt_name) %&gt;% \n  summarise(mean_tna = mean(tna_latest)) %&gt;% \n  arrange(desc(mean_tna)) %&gt;%\n  filter(mean_tna &lt;= 200) %&gt;% \n  pull(mgmt_name)\n\n\nHaving split the group into two subgroups, select characteristics can now be analysed by averaging both groups over time:\n\n\nCode\nfacet_labels &lt;- c(\"mean_alpha\" = \"Carhart Four Factor Alpha\",\n                  \"mean_exp_ratio\" = \"Expense Ratio\",\n                  \"mean_max_drawdown\" = \"Maximum Drawdown\",\n                  \"mean_mflows\" = \"Monthly Flows\",\n                  \"mean_turn_ratio\" = \"Turnover Ratio\",\n                  \"mean_yret\" = \"Yearly Returns\")\n\nbind_rows(\n  dt %&gt;%\n    filter(mgmt_name %in% top_managers) %&gt;%\n    mutate(class = \"Better Known Managers\") %&gt;%\n    group_by(year) %&gt;%\n    summarise(mean_alpha = mean(yrea_alpha),\n              mean_exp_ratio = mean(exp_ratio),\n              mean_turn_ratio = mean(turn_ratio),\n              mean_yret = mean(yret),\n              mean_max_drawdown = mean(max_drawdown),\n              mean_mflows = mean(mflows),\n              class = last(class)),\n  dt %&gt;%\n    filter(mgmt_name %in% bottom_managers) %&gt;%\n    mutate(class = \"Less Known Managers\") %&gt;%\n    group_by(year) %&gt;%\n    summarise(mean_alpha = mean(yrea_alpha),\n              mean_exp_ratio = mean(exp_ratio),\n              mean_turn_ratio = mean(turn_ratio),\n              mean_yret = mean(yret),\n              mean_max_drawdown = mean(max_drawdown),\n              mean_mflows = mean(mflows),\n              class = last(class))\n) %&gt;% \n  pivot_longer(-c(year, class)) %&gt;% \n  ggplot(aes(year, value, colour = class)) +\n  geom_line(alpha = 0.4) +\n  geom_point() +\n  facet_wrap(~ name,\n             scales = \"free_y\",\n             labeller = as_labeller(facet_labels)) +\n  labs(title = \"Characteristics Of Mutual Funds Managed By Better And Less Known Fund Managers\",\n       subtitle = \"Popularity of fund managers is approximated by the average of their funds' sizes. Values shown are means.\",\n       x = NULL,\n       y = NULL,\n       colour = NULL) +\n  scale_colour_manual(values = c(\"midnightblue\", \"firebrick\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +\n  theme_light() +\n  theme(strip.background = element_rect(fill = \"grey50\"),\n        plot.title = element_text(face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\nLooking at the four factor Carhart alpha, there exists an ever so slight and inconsistent outperformance of equity mutual funds from better known managers. The same holds for monthly flows. Yearly returns show no discernible difference.\nHowever, there are two fund characteristics which reveal an interesting disparity. Both expense ratios and turnover ratios of lesser known fund managers are considerably higher. As higher turnover is directly correlated with higher expenses, the two could be considered to be two sides of the same coin.\nEven though no causation can be implied here, it is likely the case that investors are sensitive to expense ratios and prefer funds with lower expense ratios, in turn leaving more annual performance on the table, though it can be said that both groups are very comparable from a performance perspective on average.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220729-Fuel-Stations/index.html",
    "href": "posts/20220729-Fuel-Stations/index.html",
    "title": "Alternative Fuel Stations in the US",
    "section": "",
    "text": "Description of the Data\n\nThe data comes from the U.S. Department of Transportation who regularly update their Alternative Fueling Stations Database. It covers 63,097 alternative fuelling stations in the United States as of 6 July 2022.\nTo show only the results, large code chunks are hidden, but can be unfolded by clicking the “Code” boxes on the top right of each hidden code chunk.\n\n\nData Cleaning\nThe first step is loading the data, removing unnecessary columns and modifying columns with information from the documentation. I used the very handy clean_names function from the janitor package, which removes the all caps and spaces on the column headers. The case_when function from dplyr is very useful for mutating a new column on a data frame conditionally on another column.\n\n\nCode\n# Load data\nfuel &lt;-\n  read.csv(\"C:/Users/mathi/OneDrive/R/Data Visualisation/Alternative Fueling Stations US/Alternative_Fueling_Stations.csv\") %&gt;%\n  as_tibble() %&gt;% \n  janitor::clean_names() %&gt;% \n  select(-c(contains(\"_fr\"), zip, plus4)) %&gt;% \n  mutate(\n    across(where(is.character), ~ na_if(.,\"\")),\n    fuel_type = case_when(\n      fuel_type_code == \"BD\" ~ \"Biodiesel\",\n      fuel_type_code == \"CNG\" ~ \"Compressed Natural Gas\",\n      fuel_type_code == \"E85\" ~ \"Ethanol\",\n      fuel_type_code == \"ELEC\" ~ \"Electric\",\n      fuel_type_code == \"HY\" ~ \"Hydrogen\",\n      fuel_type_code == \"LNG\" ~ \"Liquified Natural Gas\",\n      fuel_type_code == \"LPG\" ~ \"Propane\"),\n    status_code = case_when(\n      status_code == \"E\" ~ \"Available\",\n      status_code == \"P\" ~ \"Planned\",\n      status_code == \"T\" ~ \"Temporarily Unavailable\")\n  )\n\n\nAt this stage, we have a nice and large dataset, which can be used for visualisation. The following sections will answer questions I was interested in while looking at the data.\n\n\n\n\nHow Are Alternative Fuel Stations Distributed Across The US?\n\n\n\nCode\nus &lt;- map_data('state')\n\nfuel %&gt;% \n  filter(!fuel_type %in% c(\"Liquified Natural Gas\")) %&gt;% \n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_polygon(data = us, aes(x = long, y = lat, group = group),\n               color = 'gray', fill = \"gray95\", alpha = 0.3, size = 0.25) +\n  geom_point(aes(colour = status_code), alpha = 0.25, size = 0.5) +\n  facet_wrap(~ fuel_type) +\n  scale_size_continuous(range = c(0.5, 4), \n                        labels = scales::comma_format(suffix = \" MW\")) +\n  xlim(-125, -65) + \n  ylim(20, 50) +\n  labs(title = \"Alternative Fuel Stations in the USA\",\n       subtitle = \"The Alternative Fueling Stations dataset is recent as of July 06, 2022.\\nSource: U.S. Department of Transportation (USDOT)/Bureau of Transportation Statistics (BTS)\",\n       y = NULL,\n       x = NULL,\n       colour = \"Status\") +\n  coord_map() +\n  scale_colour_manual(values = c(\"#1E88E5\", \"#FBC02D\", \"#C62828\")) +\n  guides(colour = guide_legend(override.aes = list(size = 3))) +\n  theme_minimal() +\n  theme(plot.background = element_rect(colour = \"white\"),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        plot.title = element_text(size = 14, face = \"bold\", colour = \"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"bottom\")\n\n\n\n\n\nGenerally, it becomes clear that electricity is the most common alternative fuel source in the US. The next categories are ethanol and propane, the first of which is centred around the corn belt states, which have emerged from their former rust belt status. As ethanol is most commonly made from the biomass of corn, the correlation to local production is not surprising. Another thing that catches the eye is the particular distribution of biodiesel in two clusters. Reasons include strong tax incentives for this particular alternative fuel in these states. Lastly, the development of hydrogen is the weakest. There are virtually no fuelling stations for the latter and only some are planned in California and Connecticut.\n\n\n\nHow Many Fuel Stations Are There For Each Category?\n\n\n\nCode\nfuel %&gt;% \n  count(fuel_type) %&gt;% \n  ggplot(aes(x = n,\n             y = fuel_type %&gt;% fct_reorder(n))) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Frequency Of Alternative Fuel Stations In The US\",\n       subtitle = \"The Alternative Fueling Stations dataset is recent as of July 06, 2022.\\nSource: U.S. Department of Transportation (USDOT)/Bureau of Transportation Statistics (BTS)\",\n       y = NULL,\n       x = \"Frequency\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", colour = \"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\nWhat About Fast Charging Stations For Electric Vehicles?\n\n\n\nCode\nfuel %&gt;% \n  summarise(\"Level 2 Charging Available\" = mean(!is.na(ev_level2_evse_num)),\n            \"DC Fast Charging Available\" = mean(!is.na(ev_dc_fast_count)))\n\n\n# A tibble: 1 × 2\n  `Level 2 Charging Available` `DC Fast Charging Available`\n                         &lt;dbl&gt;                        &lt;dbl&gt;\n1                        0.745                        0.103\n\n\n3 out of 4 electric charging stations have fast chargers (Level 2 - up to 8x faster than regular home charger) available. Only 1 out of 10 stations has DC fast charging, which is capable of charging your electric vehicle almost fully in less than an hour. For the stations with DC fast charging, how many places are available?\n\n\nCode\nfuel %&gt;% \n  filter(!is.na(ev_dc_fast_count)) %&gt;% \n  ggplot(aes(ev_dc_fast_count)) +\n  geom_histogram(binwidth = 2, fill = \"midnightblue\") +\n  labs(title = \"Number Of Fast Chargers At Electric Charging Stations\",\n       subtitle = \"The Alternative Fueling Stations dataset is recent as of July 06, 2022.\\nSource: U.S. Department of Transportation (USDOT)/Bureau of Transportation Statistics (BTS)\",\n       y = NULL,\n       x = \"Frequency\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", colour = \"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\nThe exceeding majority of stations have less than 10 DC charging spaces available.\n\n\n\nHow Are DC Fast Charging Stations Distributed Across The US?\n\n\n\nCode\nfuel %&gt;% \n  filter(fuel_type_code == \"ELEC\",\n         !is.na(open_date)) %&gt;% \n  mutate(open_date = ymd_hms(open_date),\n         open_year = year(open_date)) %&gt;% \n  filter(open_year &gt; 2010) %&gt;% \n  mutate(time_period = cut(open_year, seq(2000, 2022, 2), dig.lab = 5)) %&gt;% \n  filter(!is.na(ev_dc_fast_count)) %&gt;% \n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_polygon(data = us, aes(x = long, y = lat, group = group),\n               color = 'gray', fill = \"gray95\", alpha = 0.3, size = 0.25) +\n  geom_point(aes(colour = time_period), alpha = 0.4, size = 0.5) +\n  xlim(-125, -65) + \n  ylim(20, 50) +\n  labs(title = \"Electric Car Fast Charging Stations in the USA\",\n       subtitle = \"DC fast charging charge some EVs to 80 percent in 20-30 minutes. \\nThe Alternative Fueling Stations dataset is recent as of July 06, 2022.\\nSource: U.S. Department of Transportation (USDOT)/Bureau of Transportation Statistics (BTS)\",\n       y = NULL,\n       x = NULL,\n       colour = \"Built in\") +\n  coord_map() +\n  scale_color_brewer(palette = \"RdYlBu\") +\n  guides(colour = guide_legend(override.aes = list(size = 3))) +\n  theme_minimal() +\n  theme(plot.background = element_rect(colour = \"white\"),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        plot.title = element_text(size = 14, face = \"bold\", colour = \"black\"),\n        plot.subtitle = element_text(face = \"italic\", colour = \"gray50\",\n                                     size = 10),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"right\")\n\n\n\n\n\nFrom the map, the highway systems in the US become visible. Fast charging stations cluster around large cities and next to the large highways. Strikingly, the vast majority of fast charging stations have been built over the past two years.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20220923-Shiny-Deployment-Used-Cars/index.html",
    "href": "posts/20220923-Shiny-Deployment-Used-Cars/index.html",
    "title": "Deploying A Model Predicting Used Car Prices With Shiny",
    "section": "",
    "text": "Shiny Application\n\nThe final result of this blog post can be interacted with below:\n\nknitr::include_app(\"https://mathiassteilen.shinyapps.io/car_price_prediction_xgboost/\", height = \"600px\")\n\n\n\n\n\n\nDescription of this project\n\nThe data for this post was scraped from the online platform Anibis. The goal of this post is to demonstrate how models can be deployed using Shiny. Specifically, the example above shows a simple UI, where users can enter information about their car and get back a prediction for the price in Swiss francs. Alternatively, I could have also allowed users to use the app at scale by submitting variables via a CSV, or by creating a RESTful API. The rest of the post below shows the scraping and model creation process.\n\n\nLimitations of the model\nOnly 25% of all cars on Anibis were scraped, as it was sufficient for proof of concept. It would be no problem to extend the sample to a full 100% of all cars on Anibis and to make the model training more thorough in order to achieve higher accuracy. Additionally, there is no information on previous owners, potential previous accidents and the general state of the car with regard to maintenance. These are crucial pieces of information for a model predicting price. However, in this case, the goal was to show how a model could be hosted using Shiny, therefore the achieved \\(R^2\\) of around 75% is sufficient.\n\n\n\n\n\nData Scraping\n\n\nRequesting sublinks to individual listings from site\n\npages &lt;- 1:6500\n\nstart_urls &lt;- paste0(\"https://www.anibis.ch/de/c/autos-autozubehoer-occasionen-neuwagen?pi=\",\n                     pages) %&gt;% \n  as_tibble() %&gt;% \n  rename(url = value)\n\nstart_urls\n\n# Fetch subpages from listings from each overview page\n\nlistings &lt;- start_urls %&gt;%\n  mutate(subpages = map(url, function(.x) {\n    return(\n      GET(.x, timeout(10)) %&gt;% \n        read_html(.) %&gt;%\n        html_nodes(\"[class ='sc-1yo7ctu-0 bRDNul']\") %&gt;%\n        html_attr('href') %&gt;%\n        as_tibble() %&gt;%\n        rename(subpage = value) %&gt;%\n        mutate(subpage = paste0(\"https://www.anibis.ch\", subpage))\n    )\n  }))\n\nlistings\n\n# Extract subpage urls and clean as tibble\n\nsubpage_urls &lt;- listings %&gt;% \n  select(subpages) %&gt;% \n  unnest(subpages)\n\nsubpage_urls\n\n# Read in html from each subpage (Danger of timeout here)\n\nsubpage_urls &lt;- subpage_urls %&gt;% \n  mutate(subpage_content = map(subpage, function(.x) {\n    return(GET(.x, timeout(20)) %&gt;% \n             read_html(.))\n  }))\n\nsubpage_urls\n\n\n\n\nUsing for-loop to store full html in memory\n\ntmp &lt;- subpage_urls %&gt;% \n  sample_n(40000)\n\nsubpages_content &lt;- vector(mode = \"list\", length = nrow(tmp))\n\nfor (x in 1:nrow(tmp)){\n  \n  url_tmp &lt;- tmp[x, \"subpage\"] %&gt;% \n    pull()\n  \n  tryCatch(\n    subpages_content[[x]] &lt;- url_tmp %&gt;%\n      GET(., timeout(90)) %&gt;% \n      read_html(.),\n    error = function(e){NA}\n  )\n  \n  print(paste(\"Link\", x, \"retrieved\"))\n  \n}\n\nsubpage_content &lt;- tibble(listing = subpages_content)\n\nsubpage_content &lt;- subpage_content %&gt;% \n  mutate(is_null = map(listing, is.null)) %&gt;% \n  unnest(is_null) %&gt;% \n  filter(is_null == FALSE) %&gt;% \n  select(-is_null)\n\n\n\n\nExtract text from scraped content into a tibble:\n\ncars_raw &lt;- tibble(\n  listing_no = 1:nrow(subpage_content),\n  listing = subpage_content %&gt;% pull(),\n  header = map(listing, function(.x){\n      return(.x %&gt;% \n        html_nodes(\".fauvte\") %&gt;%\n        html_text())\n      }),\n  content = map(listing, function(.x){\n      return(.x %&gt;% \n        html_nodes(\".goTXZq\") %&gt;%\n        html_text())\n      }),\n  price = map(listing, function(.x){\n      return(.x %&gt;% \n        html_node(\".knSuBJ\") %&gt;%\n        html_text())\n      }),\n  model_simple = map(listing, function(.x){\n      return(.x %&gt;% \n        html_node(\".jOflgH .sc-258i13-0\") %&gt;%\n        html_text())\n      })\n  ) %&gt;%\n  select(-listing) %&gt;% \n  unnest(everything()) %&gt;% \n  pivot_wider(values_from = content, names_from = header)\n\n\n\n\n\n\nData Cleaning\n\n\ncars &lt;- cars_raw %&gt;% \n  mutate(across(c(price, Kilometer), ~ gsub(\"'\", \"\", .x)),\n         across(c(price, Kilometer, Baujahr,\n                  `Leistung (PS)`), ~ parse_number(.x)),\n         across(c(Marke, Modell, Getriebeart, Treibstoff,\n                  Aufbau, Aussenfarbe, Antrieb), ~ trimws(.x)),\n         across(`Letzte Änderung`, ~ lubridate::dmy(.x)),\n         across(`Ab MFK`, ~ case_when(is.na(.x) ~ \"no\", .x == \"\" ~ \"yes\"))) %&gt;% \n  rename(full_name = model_simple, brand = Marke, model = Modell,\n         mileage_km = Kilometer, year = Baujahr, transmission = Getriebeart,\n         fuel = Treibstoff, body_type = Aufbau, horsepower = `Leistung (PS)`,\n         colour = Aussenfarbe, last_edited = `Letzte Änderung`,\n         id = Inseratnummer, drive = Antrieb, mfk = `Ab MFK`,\n         price_chf = price) %&gt;% \n  filter(`Art des Inserats` == \"Angebot\",\n         mileage_km &lt; 1e6,\n         price_chf &lt; 1e6) %&gt;% \n  select(- c(`Art des Inserats`))\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\nBefore training a model, I will explore the clean data set and gauge relations between variables. Let’s first take a look at the frequency of categorical predictors. I lumped levels beyond \\(N=15\\) together, so that the y axis can be read properly.\n\ncars %&gt;% \n  select(where(is.character)) %&gt;% \n  select(-c(full_name, model)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  drop_na() %&gt;% \n  group_by(name) %&gt;% \n  mutate(value = fct_lump(value, n = 15)) %&gt;% \n  count(value) %&gt;% \n  mutate(value = reorder_within(value, n, name)) %&gt;% \n  ggplot(aes(n, value)) +\n  geom_col(fill = \"midnightblue\", alpha = 0.8)  +\n  facet_wrap(~ name, scales = \"free\", ncol = 4) +\n  labs(title = \"Frequency Of Used Car Properties on anibis.ch\",\n       subtitle = \"Sample Size = 30,504 | Data as of 09/22\",\n       y = NULL,\n       x = \"Count\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_reordered() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nUnsurprisingly, most common brands are German car brands like VW, Mercedes, Audi and BMW. Next up, let’s inspect the numerical variables:\n\ncars %&gt;% \n  select(where(is.numeric)) %&gt;% \n  select(-c(listing_no, id)) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(value)) +\n  stat_ecdf() +\n  facet_wrap(~ name, scales = \"free\") +\n  labs(title = \"Cumulative Distribution Of Used Car Characteristics on anibis.ch\",\n       subtitle = \"Sample Size = 30,504 | Data as of 09/22\",\n       y = NULL,\n       x = NULL) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\nMore interesting plots:\n\ncars %&gt;% \n  filter(year &gt; 1995) %&gt;% \n  count(transmission, year) %&gt;% \n  drop_na() %&gt;% \n  pivot_wider(values_from = n, names_from = transmission) %&gt;% \n  mutate(pct_automatic = Automatik/(Automatik + Handschaltung)) %&gt;% \n  ggplot(aes(year, pct_automatic)) +\n  geom_area(alpha = 0.8, fill = \"midnightblue\") +\n  labs(title = \"Percentage of cars with automatic transmission by construction year\",\n       subtitle = \"Sample size: 29,284 / Scraped from anibis.ch in 09/2022\",\n       y = \"Percentage Automatic Cars\",\n       x = NULL) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  group_by(year) %&gt;% \n  summarise(mean_hp = mean(horsepower, na.rm = T),\n            median_hp = median(horsepower, na.rm = T)) %&gt;% \n  filter(year &gt; 1995) %&gt;% \n  pivot_longer(-year) %&gt;% \n  ggplot(aes(year, value, colour = name)) +\n  geom_line(size = 1) +\n  labs(title = \"Horsepower of used cars by construction year\",\n       subtitle = \"Sample size: 29,284 | Scraped from anibis.ch in 09/2022\",\n       y = \"Horsepower\",\n       x = NULL,\n       colour = NULL) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  ggsci::scale_colour_futurama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  select(brand, horsepower) %&gt;% \n  drop_na() %&gt;%\n  filter(horsepower &gt; 0) %&gt;% \n  mutate(brand = fct_lump(brand, n = 20)) %&gt;% \n  add_count(brand) %&gt;% \n  mutate(brand = paste0(brand, \" (N=\", n, \")\"),\n         brand = fct_reorder(brand, horsepower, .desc = TRUE)) %&gt;% \n  ggplot(aes(horsepower, brand)) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Horsepower distribution of used cars by brand\",\n       subtitle = \"Sample Size: N = 29,610 | as of 09/22 | Scraped from anibis.ch in 09/2022\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" HP\")) +\n  coord_cartesian(xlim = c(0,750)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  select(brand, price_chf) %&gt;% \n  drop_na() %&gt;%\n  filter(price_chf &gt; 0) %&gt;% \n  mutate(brand = fct_lump(brand, n = 20)) %&gt;% \n  add_count(brand) %&gt;% \n  mutate(brand = paste0(brand, \" (N=\", n, \")\"),\n         brand = fct_reorder(brand, price_chf, .desc = TRUE)) %&gt;% \n  ggplot(aes(price_chf, brand)) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Price distribution of used cars by brand\",\n       subtitle = \"Sample Size: N = 29,610 | as of 09/22 | Scraped from anibis.ch in 09/2022\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" CHF\")) +\n  coord_cartesian(xlim = c(0,3e5)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  select(year, price_chf, brand) %&gt;% \n  drop_na() %&gt;% \n  mutate(age = 2023 - year) %&gt;% \n  filter(age &lt; 20,\n         age &gt; 0,\n         fct_lump(brand, n = 12) != \"Other\") %&gt;% \n  group_by(brand, age) %&gt;% \n  summarise(median_price = median(price_chf)) %&gt;% \n  mutate(change = median_price/first(median_price)) %&gt;% \n  select(age, brand, change) %&gt;% \n  ggplot(aes(age, change, colour = brand)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0.5, lty = \"dashed\", colour = \"grey50\") +\n  facet_wrap(~ brand) +\n  expand_limits(y = 0) +\n  labs(title = \"Used Car Price Change By Age on anibis.ch\",\n       subtitle = \"sample size: n = 18,733 | as of 09/22 | Age 1 is 100%\",\n       x = \"Vehicle Age\",\n       y = \"Price (as %) compared to age = 1\") +\n  scale_y_continuous(labels = percent_format()) +\n  ggsci::scale_colour_futurama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"),\n        legend.position = \"none\")\n\n\n\n\n\ncars %&gt;%\n  select(brand, body_type) %&gt;% \n  drop_na() %&gt;% \n  group_by(brand = fct_lump(brand, n = 10)) %&gt;% \n  count(body_type) %&gt;% \n  filter(n &gt; 15) %&gt;%  \n  ggplot(aes(y = body_type %&gt;% reorder_within(by = n, within = brand),\n             x = n,\n             fill = brand)) +\n  geom_col() +\n  facet_wrap(~ brand, scales = \"free\") +\n  labs(title = \"Body Types By Brand on anibis.ch\",\n       subtitle = \"sample size: n = 29,989 | as of 09/22 | showing most frequent body types and brand\",\n       x = \"Count\",\n       y = NULL) +\n  scale_y_reordered() +\n  scale_fill_manual(values = MetBrewer::met.brewer(name = \"VanGogh1\",\n                                                   n = 12)) +\n  guides(fill = \"none\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  select(body_type, price_chf) %&gt;% \n  drop_na() %&gt;% \n  add_count(body_type) %&gt;% \n  mutate(body_type = paste0(body_type, \" (N=\", n, \")\"),\n         body_type = fct_reorder(body_type, price_chf, .desc = TRUE)) %&gt;% \n  ggplot(aes(price_chf, body_type)) +\n  geom_boxplot(outlier.colour = NA) +\n  labs(title = \"Prices of used cars by body type\",\n       subtitle = \"Sample Size: N = 30,065 | as of 09/22 | Scraped from anibis.ch in 09/2022\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format(suffix = \" CHF\"),\n                     breaks = seq(0, 250000, 50000)) +\n  coord_cartesian(xlim = c(0,250000)) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\nset.seed(12)\n\ncars %&gt;% \n  sample_n(1000) %&gt;% \n  mutate(age = 2022 - year) %&gt;%\n  select(price_chf, mileage_km, horsepower, age) %&gt;% \n  drop_na() %&gt;% \n  pivot_longer(-price_chf) %&gt;% \n  ggplot(aes(value, price_chf)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = F) +\n  labs(title = \"Relation of Age, Horsepower and Mileage with Price\") +\n  facet_wrap(~ name, scales = \"free\") +\n  scale_y_continuous(labels = comma_format(suffix = \" CHF\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\ncars %&gt;% \n  select(model, price_chf) %&gt;% \n  drop_na() %&gt;% \n  unnest_tokens(input = model, output = \"tokens\", token = \"words\") %&gt;% \n  add_count(tokens, sort = T) %&gt;% \n  group_by(tokens) %&gt;% \n  summarise(median_price = median(price_chf),\n            n = last(n)) %&gt;% \n  filter(n &gt; 1000) %&gt;% \n  ggplot(aes(median_price, \n             tokens %&gt;% fct_reorder(median_price))) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Components of car model names explain variance in prices\",\n       subtitle = \"Sample Size: N = 30,496 | as of 09/22 | Scraped from anibis.ch in 09/2022\\nModel names are tokenised by words. Only tokens with frequency N &gt; 1000 are shown.\",\n       x = \"Median Price associated with Token\",\n       y = NULL) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_reordered() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", size = 10, \n                                     colour = \"grey50\"))\n\n\n\n\n\n\n\n\n\nFitting A Model\n\nFirst, the data is split into training and testing sets. Also, five-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.\n\ndt_split &lt;- cars %&gt;% \n  mutate(across(where(is.character), as.factor)) %&gt;% \n  initial_split()\n\ndt_train &lt;- training(dt_split)\ndt_test &lt;- testing(dt_split)\n\nfolds &lt;- vfold_cv(dt_train, v = 5)\n\nThe recipe in the tidymodels framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit.\n\nmodel_rec &lt;- recipe(price_chf ~ .,\n                 data = dt_train) %&gt;%\n  step_rm(listing_no, full_name, last_edited, id) %&gt;% \n  step_mutate(horsepower = ifelse(horsepower == 0, NA, horsepower)) %&gt;% \n  step_impute_median(all_numeric_predictors()) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;% \n  step_tokenize(model) %&gt;% \n  step_stopwords(model) %&gt;% \n  step_tokenfilter(model, max_tokens = 500) %&gt;% \n  step_tf(model) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_other(all_nominal_predictors(), threshold = 0.005) %&gt;% \n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nSetting up the model specifications with tuning options for hyperparameters:\n\ngb_spec &lt;- \n  boost_tree(\n    trees = 1000,\n    tree_depth = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    mtry = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nSetting up the model workflow:\n\ngb_wflow &lt;- workflow() %&gt;% \n  add_recipe(model_rec,\n             blueprint = hardhat::default_recipe_blueprint(\n               allow_novel_levels = TRUE\n             )) %&gt;% \n  add_model(gb_spec)\n\nSetting up a space-filling design grid for time-efficient hyperparameter tuning:\n\ngb_grid &lt;- \n  grid_latin_hypercube(\n    tree_depth(),\n    min_n(),\n    loss_reduction(),\n    sample_size = sample_prop(),\n    finalize(mtry(), dt_train),\n    learn_rate(),\n    size = 30\n  )\n\nTuning the hyperparameters with parallel processing:\n\n# Gradient Boosting\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\ngb_tune &lt;- tune_grid(object = gb_wflow,\n                     resamples = folds,\n                     grid = gb_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\n\nwrite_rds(gb_tune, \"C:/Users/mathi/OneDrive/R/Data Visualisation/Used Cars Web Scraping/gb_tune.rds\")\n\nLooking at the tuning results reveals that the model captures strong signal in the predictors, as the \\(R^2\\) is fairly high. Though, as mentioned in the introduction, crucial variables are missing.\n\ngb_tune %&gt;% \n  show_best(metric = \"rsq\") %&gt;% \n  transmute(model = \"Gradient Boosting\", .metric, mean, n, std_err)\n\n# A tibble: 5 × 5\n  model             .metric  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Gradient Boosting rsq     0.726     5 0.00762\n2 Gradient Boosting rsq     0.709     5 0.00849\n3 Gradient Boosting rsq     0.665     5 0.00951\n4 Gradient Boosting rsq     0.648     5 0.00975\n5 Gradient Boosting rsq     0.573     5 0.0135 \n\n\nFitting the best model from training on the entire training data:\n\ngb_final_fit &lt;- gb_wflow %&gt;%\n  finalize_workflow(select_best(gb_tune, metric = \"rmse\")) %&gt;% \n  last_fit(dt_split)\n\n\n\n\n\nEvaluating Model Performance On The Training Data\n\n\ngb_final_fit %&gt;% \n  augment(dt_test) %&gt;% \n  ggplot(aes(price_chf, .pred)) +\n  geom_point(alpha = 0.1, colour = \"midnightblue\") + \n  geom_abline(colour = \"grey50\", lty = \"dashed\") +\n  labs(title = \"Out-Of-Sample Fit\",\n       subtitle = NULL,\n       y = \"Prediction\",\n       x = \"Truth\") +\n  scale_x_continuous(labels = comma_format(suffix = \" CHF\")) +\n  scale_y_continuous(labels = comma_format(suffix = \" CHF\")) +\n  theme_light() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\n\ngb_final_fit %&gt;% \n  augment(dt_test) %&gt;% \n  select(brand, year, price_chf, .pred) %&gt;% \n  drop_na() %&gt;% \n  filter(year &gt; 1990) %&gt;% \n  mutate(brand = fct_lump(brand, n = 11)) %&gt;% \n  ggplot(aes(price_chf/1000, .pred/1000, colour = year)) +\n  geom_point(alpha = 0.5, size = 0.1) +\n  geom_abline(colour = \"grey50\", lty = \"dashed\") +\n  facet_wrap(~ brand, scales = \"free\") +\n  labs(title = \"Out-of-sample fit by brand\",\n       subtitle = \"\",\n       y = \"Prediction\",\n       x = \"Truth\",\n       colour = \"Construction Year\") +\n  scale_x_continuous(labels = scales::comma_format(suffix = \"k CHF\")) +\n  scale_y_continuous(labels = scales::comma_format(suffix = \"k CHF\")) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n        plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\nThe final fit is not highly impressive, as expected with highly important information for used car prices. Also, it can be seen that the fit is better for some brands than others.\nThis final model can be saved using the bundle package (Silge, Couch, Yan & Kuhn, 2022) and then read into the Shiny application, where it can make predictions on new data. As you probably have seen, this is what I have done, so you can interact with the final model in the application right at the start of this blog post.\nI hope this has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out. Thank you for reading.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20221025-Customer-Churn-Classification/index.html",
    "href": "posts/20221025-Customer-Churn-Classification/index.html",
    "title": "Using Classification Models and Gain Curves For Profit Maximisation",
    "section": "",
    "text": "If you are only interested in the application to a business setting, feel free to use the table of contents to skip to the last section.\n\n\nWhat are we looking at?\n\nThe data comes from Kaggle, the biggest online platform for machine learning enthusiasts hosting datasets and competitions around data science. More precisely, the data set was part of season 1 episode 7 of SLICED, a data science competition streamed by Nick Wan and Meg Risdal on Twitch.\nThis dataset is about retail bank customer churn. The purpose of this post is to demonstrate how classification models can be set up, trained and used to create financial value within organisations, specifically using gain curves.\n\n\nData Cleaning\nFirstly, I start by loading the data. The first file is the one to be used for training, whereas the holdout will only be used for submission of out-of-sample predictions, as it doesn’t contain the target variable. The training data is rather small, but still acceptable, containing information on 7,088 customers with 14 predictors and the binary response variable of their churn status.\n\nnames(data)\n\n [1] \"id\"                       \"attrition_flag\"          \n [3] \"customer_age\"             \"gender\"                  \n [5] \"education_level\"          \"income_category\"         \n [7] \"total_relationship_count\" \"months_inactive_12_mon\"  \n [9] \"credit_limit\"             \"total_revolving_bal\"     \n[11] \"total_amt_chng_q4_q1\"     \"total_trans_amt\"         \n[13] \"total_trans_ct\"           \"total_ct_chng_q4_q1\"     \n[15] \"avg_utilization_ratio\"   \n\n\nIt is nice to see that there is no missing data in the variables. Therefore, no imputation steps will have to be done in the model recipe.\n\ncolMeans(is.na(data)) %&gt;% \n  tidy() %&gt;% \n  rename(pct = x) %&gt;% \n  mutate(names = fct_reorder(names, pct)) %&gt;% \n  ggplot(aes(pct, names)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Missing Data In Variables\",\n       subtitle = \"Percent missingness calculated for each column\",\n       y = NULL,\n       x = NULL) +\n  scale_x_continuous(labels = scales::percent_format()) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\nConverting the necessary columns to factors:\n\ndata &lt;- data %&gt;% \n  mutate(across(c(where(is.character),attrition_flag), as.factor))\n\nholdout &lt;- holdout %&gt;% \n  mutate(across(c(where(is.character)), as.factor))\n\n\n\n\n\n\nExploratory Data Analysis\n\nThe next step is to walk through the available predictors and understand relations to the target variable. Below, every variable is briefly looked at and presented, enabling a better understanding of the complete training data.\n\n\n‘attrition_flag’: whether the customer is churned (0 = no; 1 = yes)\nFirstly, the target variable must be inspected to judge class imbalance, which will potentially have to be rectified, as well as releveling, if the factor levels are the wrong way around.\n\ndata %&gt;% \n  count(attrition_flag) %&gt;% \n  mutate(pct = n/sum(n))\n\n# A tibble: 2 × 3\n  attrition_flag     n   pct\n  &lt;fct&gt;          &lt;int&gt; &lt;dbl&gt;\n1 0               5956 0.840\n2 1               1132 0.160\n\n\nThere is a class imbalance in the target variable in favour of the negative case. In order to avoid bias in the final model, the classes will be rebalanced in the recipe with step_smote, which creates new cases for the minority class with nearest neighbours.\n\nlevels(data$attrition_flag)\n\n[1] \"0\" \"1\"\n\n\nAs suspected, the negative case is case 1. Tidymodels treats the first factor level in binary classifications as the positive case, so these will have to be switched around.\n\ndata &lt;- data %&gt;% \n  mutate(attrition_flag = fct_rev(attrition_flag))\n\nlevels(data$attrition_flag)\n\n[1] \"1\" \"0\"\n\n\nNow we should be good to go.\n\n\n\n‘id’: unique identifier for the customer\nThe ID column stores a unique integer for each customer, so there should not be predictive power in it that holds for new customers.\n\n\n\n‘customer_age’: age of the customer\nThe age data looks almost normally distributed, with some peaks at both ends. Colouring by gender reveals that both groups are approximately distributed the same.\n\ndata %&gt;% \n  ggplot(aes(customer_age, fill = gender)) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = 0.4) +\n  labs(title = \"Distribution Of Customer Age\",\n       subtitle = NULL,\n       y = \"Frequency\",\n       x = NULL) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  ggsci::scale_fill_jama() +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nAt this stage, I’ll make a function to summarise the probability of churning by group for later use.\n\nsummarise_churn &lt;- function(tbl, category){\n  tbl %&gt;% \n    group_by({{ category }}) %&gt;% \n    summarise(n = n(),\n              prob_churning = mean(attrition_flag == 1)) %&gt;% \n    arrange(-prob_churning)\n}\n\n\ndata %&gt;% \n  group_by(customer_age = round(customer_age/5)*5) %&gt;% \n  summarise_churn(customer_age) %&gt;% \n  filter(n &gt; 30) %&gt;% \n  ggplot(aes(customer_age, prob_churning, size = n)) +\n  geom_point(colour = \"midnightblue\") + \n  labs(title = \"Relation: Customer Age And Probability Of Churning\",\n       subtitle = NULL,\n       y = \"Probability of Churning\",\n       x = \"Customer Age\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  ggsci::scale_fill_jama() +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nRounding customer ages to the nearest five and plotting it against the observed probability of churning reveals a non-linear pattern, which shows an increasing trend towards the age of 55, which reverses beyond that threshold.\n\n\n\n‘gender’: gender of the customer\nGender groups are distributed evenly.\n\ndata %&gt;% \n  group_by(gender) %&gt;% \n  summarise(n = n(),\n            prob_churning = mean(attrition_flag == 1))\n\n# A tibble: 2 × 3\n  gender     n prob_churning\n  &lt;fct&gt;  &lt;int&gt;         &lt;dbl&gt;\n1 F       3714         0.174\n2 M       3374         0.144\n\n\nThe observed data shows women churning at higher rates than men, though the absolute difference is rather small.\n\n\n\n‘education_level’: education level of the customer\n\ndata %&gt;% \n  summarise_churn(education_level)\n\n# A tibble: 7 × 3\n  education_level     n prob_churning\n  &lt;fct&gt;           &lt;int&gt;         &lt;dbl&gt;\n1 Doctorate         326         0.212\n2 Post-Graduate     343         0.195\n3 Unknown          1083         0.165\n4 Uneducated       1038         0.160\n5 Graduate         2212         0.156\n6 High School      1378         0.154\n7 College           708         0.134\n\n\nThough there is a class imbalance, the churn rates for people with higher degrees, that is PhDs or Master’s, are higher.\n\n\n\n‘income_category’: income range of the customer\n\ndata %&gt;% \n  summarise_churn(income_category)\n\n# A tibble: 6 × 3\n  income_category     n prob_churning\n  &lt;fct&gt;           &lt;int&gt;         &lt;dbl&gt;\n1 $120K +           521         0.177\n2 Unknown           772         0.174\n3 Less than $40K   2467         0.171\n4 $80K - $120K     1081         0.155\n5 $40K - $60K      1223         0.154\n6 $60K - $80K      1024         0.126\n\n\nThe churn probabilities of the income range does not show a linear trend. High earners and low earners are the most likely to churn whereas the midfield is lowest.\n\n\n\n‘total_relationship_count’: number of relationships\nI am not entirely sure what “relationship” means here because there is no further documentation on it. Assuming the variable describes dependents and plotting it against the target variable probabilities for each bucket, it appears like there is a negative correlation. Hence, a higher number of dependents is associated with a lower probability of changing banks.\n\ndata %&gt;% \n  ggplot(aes(total_relationship_count)) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = 0.6,\n                 fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Distribution Of Relationship Count\",\n       subtitle = NULL,\n       y = \"Frequency\",\n       x = NULL) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  ggsci::scale_fill_jama() +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\ndata %&gt;% \n  summarise_churn(total_relationship_count) %&gt;% \n  ggplot(aes(y = prob_churning, x = total_relationship_count, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  expand_limits(x = 0, y = 0) +\n  labs(title = \"Correlation Of Relationship Count And Churn Probability\",\n       subtitle = NULL,\n       y = \"Churn Probability\",\n       x = \"Total Relationships\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘months_inactive_12_mon’: number of months the customer is inactive in the past 12 months\nInterestingly, customers inactivity is positively correlated with churning up until 4 months, where the trend reverses. This has to be taken with a grain of salt, as the number of observations below 1 month and above 4 months is sparse, so it might well be noise. We’ll see, how important this variable turns out to be in classifying customers correctly.\n\ndata %&gt;% \n  summarise_churn(months_inactive_12_mon) %&gt;% \n  ggplot(aes(y = prob_churning, x = months_inactive_12_mon, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  expand_limits(x = 0) +\n  labs(title = \"Correlation Of Months Inactive And Churn Probability\",\n       subtitle = NULL,\n       y = \"Churn Rate\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘credit_limit’: the customer’s credit limit\nIt looks like low credit limits lead to higher churn probabilities than higher credit limits, though the relation does not look linear.\n\ndata %&gt;% \n  mutate(credit_limit = round(credit_limit/500)*500) %&gt;% \n  summarise_churn(credit_limit) %&gt;% \n  ggplot(aes(credit_limit, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  geom_smooth(se = F, method = \"loess\") +\n  labs(title = \"Correlation Of Credit Limit And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest USD 500\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘total_revolving_bal’: the customer’s total revolving balance\nLow total revolving balance seems to correlate highly with churning, as well as the highest percentiles. Seen the other way around, the area excluding customers on the very far tail ends of the revolving balances exhibits very low levels of churning, which will most likely lead to the variable being useful in the models.\n\ndata %&gt;% \n  group_by(total_revolving_bal = round(total_revolving_bal/100)*100) %&gt;% \n  summarise_churn(total_revolving_bal) %&gt;% \n  ggplot(aes(y = prob_churning, x = total_revolving_bal, size = n)) +\n  geom_point(colour = \"midnightblue\") +\n  expand_limits(x = 0) +\n  labs(title = \"Correlation Of Total Revolving Balance And Churn Probability\",\n       subtitle = \"Total revolving balances were rounded to the nearest 100\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘total_amt_chng_q4_q1’: the amount the balance changed from Q4 to Q1\nNegative changes of customer balances, that is values below 100%, are strongly associated with churning. The change is very likely a byproduct of the changing of banks process, where customers shift their assets to another bank before closing the accounts. Therefore, strong declines in the balances are often a strong indicator of the intention to leave the bank. Vice-versa, for clients with strong increases of their bank balance, e.g. 1.5 to 3 fold, the churning rates are very low. This variable will also be of high importance for the algorithms.\n\ndata %&gt;% \n  mutate(total_amt_chng_q4_q1 = round(total_amt_chng_q4_q1/0.1)*0.1) %&gt;% \n  summarise_churn(total_amt_chng_q4_q1) %&gt;% \n  ggplot(aes(total_amt_chng_q4_q1, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  geom_smooth(se = F, method = \"loess\") +\n  labs(title = \"Correlation Of Balance Change And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest 10%.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘total_trans_amt’: the value of all the customer’s transactions in the period\nThe shape of this relation with the target variable looks very peculiar. Up until USD 7,500, the data is a mess and no clear direction is discernible. Then, leading up to USD 10,000, the churning rates are unbelievably high and immediately plummet to virtually zero beyond that threshold. I am unsure whether this has to with my own ignorance about a phenomenon in the banking world that leads to this pattern, but I would almost suggest a closer look at the source of this variable, in order to exclude data errors. In any case, it will be included, but I am tempted to refrain from using it for predictions.\n\ndata %&gt;% \n  mutate(total_trans_amt = round(total_trans_amt/500)*500) %&gt;% \n  summarise_churn(total_trans_amt) %&gt;% \n  ggplot(aes(total_trans_amt, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  geom_smooth(se = F, method = \"loess\") +\n  labs(title = \"Correlation Of Total Transaction Amounts And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest USD 500.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘total_trans_ct’: the number of all of the customer’s transactions\nNow this variable shows a less confusing pattern. Customers below about 60 total transactions exhibit very high attrition rates as opposed to other customers. They use their cards/accounts a little, but not very often. From the transaction amount and the transaction count, it will be possible to generate an average transaction size, which will potentially reveal some additional insights.\n\ndata %&gt;% \n  mutate(total_trans_ct = round(total_trans_ct/5)*5) %&gt;% \n  summarise_churn(total_trans_ct) %&gt;% \n  ggplot(aes(total_trans_ct, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  labs(title = \"Correlation Of Total Transaction Count And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest 5.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘total_ct_chng_q4_q1’: the difference in number of the customer’s transactions from Q4 to Q1\nCustomers with strongly decreasing total transaction count are most likely to churn. This variable shows a similar patterns as the change in account balances, which is an indication of shifting the use to a competitor’s offering.\n\ndata %&gt;% \n  mutate(total_ct_chng_q4_q1 = round(total_ct_chng_q4_q1/0.2)*0.2) %&gt;% \n  summarise_churn(total_ct_chng_q4_q1) %&gt;% \n  ggplot(aes(total_ct_chng_q4_q1, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  labs(title = \"Correlation Of Total Transaction Count And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest 20%.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n‘avg_utilization_ratio’: the customer’s average utilization ratio during the period\nCustomers with virtually no utilisation are likely to change banks, which also holds for customers with very high utilisation. Everything in between is moderate.\n\ndata %&gt;% \n  mutate(avg_utilization_ratio = round(avg_utilization_ratio/0.1)*0.1) %&gt;% \n  summarise_churn(avg_utilization_ratio) %&gt;% \n  ggplot(aes(avg_utilization_ratio, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  labs(title = \"Correlation Of Total Transaction Count And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest 20%.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\n\n\n\navg_transaction_size: The average size of each transaction over the observed period\nUsing total transaction count, this variable can be created. From the looks of it, there is a non-linear relation with the target variable, but it remains to be seen whether the variable will actually be useful in the model.\n\ndata %&gt;% \n  mutate(avg_transaction_size = total_trans_amt/total_trans_ct) %&gt;% \n  mutate(avg_transaction_size = round(avg_transaction_size/5)*5) %&gt;% \n  summarise_churn(avg_transaction_size) %&gt;% \n  ggplot(aes(avg_transaction_size, prob_churning)) +\n  geom_point(aes(size = n), colour = \"midnightblue\") +\n  labs(title = \"Correlation Of Total Transaction Count And Churn Probability\",\n       subtitle = \"X variable was rounded to the nearest 20%.\",\n       y = \"Churn Rate\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"right\")\n\n\n\n\nHaving investigated the data set in closer detail and having learnt a bit about the relation of the variables with the target, the models can now be specified and accordingly trained.\n\n\n\n\n\nBuilding And Training The Models\n\nFirst, the data is split into training and testing sets. Also, three-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.\n\ndt_split &lt;- data %&gt;% \n  initial_split(strata = attrition_flag)\n\ndt_train &lt;- training(dt_split)\ndt_test &lt;- testing(dt_split)\n\nfolds &lt;- vfold_cv(dt_train, v = 3, strata = attrition_flag)\n\nThe recipe in the tidymodels framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit. As mentioned in the EDA, the class imbalance will be dealt with using step_smote from the themis package. It samples new observations for the minority class using nearest neighbours and is very simple and fast to put to use.\n\ngb_rec &lt;- recipe(attrition_flag ~ .,\n                 data = dt_train) %&gt;%\n  step_rm(id) %&gt;%\n  step_mutate(avg_transaction_size = total_trans_amt/total_trans_ct) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  step_zv(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_smote(attrition_flag, skip = TRUE)\n\nknn_rec &lt;- recipe(attrition_flag ~ .,\n                  data = dt_train) %&gt;%\n  step_rm(id) %&gt;%\n  step_mutate(avg_transaction_size = total_trans_amt/total_trans_ct) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  step_zv(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_smote(attrition_flag, skip = TRUE)\n\nlog_rec &lt;- recipe(attrition_flag ~ .,\n                  data = dt_train) %&gt;%\n  step_rm(id) %&gt;%\n  step_mutate(avg_transaction_size = total_trans_amt/total_trans_ct) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_smote(attrition_flag, skip = TRUE)\n\nSetting up the model specifications with tuning options for hyperparameters:\n\ngb_spec &lt;- \n  boost_tree(\n    trees = 1000,\n    tree_depth = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    mtry = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n\nlog_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"classification\")\n\nIn the model specification, you can specify the variable importance, which is calculated based on impurity in this case. Proceeding with setting up the workflow:\n\ngb_wflow &lt;- \n  workflow() %&gt;% \n  add_recipe(gb_rec) %&gt;% \n  add_model(gb_spec)\n\nknn_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(knn_rec) %&gt;% \n  add_model(knn_spec)\n\nlog_wflow &lt;- workflow() %&gt;% \n  add_recipe(\n    log_rec,\n    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)\n  ) %&gt;% \n  add_model(log_spec)\n\nNext up is setting up a space-filling design for time-efficient hyperparameter tuning. The latter is not required for KNN, as there is only hyperparameter to be tuned in this case.\n\ngb_grid &lt;- \n  grid_latin_hypercube(\n    tree_depth(),\n    min_n(),\n    loss_reduction(),\n    sample_size = sample_prop(),\n    finalize(mtry(), dt_train),\n    learn_rate(),\n    size = 50\n  )\n\nknn_grid &lt;- \n  grid_regular(neighbors(), levels = 10)\n\nNow, the hyperparameters can be trained with parallel computing in order to utilise more available computing power.\n\n# Gradient Boosting\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\ngb_tune &lt;- tune_grid(object = gb_wflow,\n                     resamples = folds,\n                     grid = gb_grid,\n                     control = control_grid(save_pred = TRUE,\n                                            save_workflow = TRUE),\n                     metrics = metric_set(roc_auc, accuracy, sensitivity, \n                                          specificity, mn_log_loss))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\n# KNN\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\nknn_tune &lt;- tune_grid(object = knn_wflow,\n                      resamples = folds,\n                      grid = knn_grid,\n                      control = control_grid(save_pred = TRUE,\n                                             save_workflow = TRUE),\n                      metrics = metric_set(roc_auc, accuracy, sensitivity, \n                                           specificity, mn_log_loss))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\n# Logistic Regression\nstart_time = Sys.time()\n\nunregister_dopar &lt;- function() {\n  env &lt;- foreach:::.foreachGlobals\n  rm(list=ls(name=env), pos=env)\n}\n\ncl &lt;- makePSOCKcluster(6)\nregisterDoParallel(cl)\n\nlog_tune &lt;- tune_grid(object = log_wflow,\n                      resamples = folds,\n                      grid = grid_latin_hypercube(mixture(), penalty(), \n                                                  size = 50),\n                      control = control_grid(save_pred = TRUE,\n                                             save_workflow = TRUE),\n                      metrics = metric_set(roc_auc, accuracy, sensitivity, \n                                           specificity, mn_log_loss))\n\nstopCluster(cl)\nunregister_dopar()\n\nend_time = Sys.time()\nend_time - start_time\n\n\ngb_tune %&gt;% \n  show_best(metric = \"sensitivity\") %&gt;%\n  transmute(model = \"XGBoost\", .metric, mean, n, std_err) %&gt;% \n  head(3)\n\n# A tibble: 3 × 5\n  model   .metric      mean     n std_err\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 XGBoost sensitivity 1         3 0      \n2 XGBoost sensitivity 0.986     3 0.00204\n3 XGBoost sensitivity 0.953     3 0.00623\n\ngb_tune %&gt;% \n  show_best(metric = \"specificity\") %&gt;%\n  transmute(model = \"XGBoost\", .metric, mean, n, std_err) %&gt;% \n  head(3)\n\n# A tibble: 3 × 5\n  model   .metric      mean     n std_err\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 XGBoost specificity 0.976     3 0.00404\n2 XGBoost specificity 0.964     3 0.00437\n3 XGBoost specificity 0.955     3 0.00331\n\nknn_tune %&gt;% \n  show_best(metric = \"sensitivity\") %&gt;%\n  transmute(model = \"KNN\", .metric, mean, n, std_err) %&gt;% \n  head(3)\n\n# A tibble: 3 × 5\n  model .metric      mean     n std_err\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 KNN   sensitivity 0.718     3  0.0337\n2 KNN   sensitivity 0.715     3  0.0371\n3 KNN   sensitivity 0.711     3  0.0356\n\nknn_tune %&gt;% \n  show_best(metric = \"specificity\") %&gt;%\n  transmute(model = \"KNN\", .metric, mean, n, std_err) %&gt;% \n  head(3)\n\n# A tibble: 3 × 5\n  model .metric      mean     n std_err\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 KNN   specificity 0.910     3 0.00641\n2 KNN   specificity 0.910     3 0.00641\n3 KNN   specificity 0.910     3 0.00641\n\n\nThe initial training results on the training data are promising. The best XGBoost models showed near perfect sensitivity and specificity. KNN was able to get very good specificity, however it lacked on the sensitivity front and falsely identified some churned customers as non-churned. Overall, the XGBoost model stood out as the better model, individually.\n\nbind_rows(\n  gb_tune %&gt;% \n    collect_metrics() %&gt;% \n    filter(!.metric %in% c(\"mn_log_loss\")) %&gt;% \n    mutate(model = \"XGBoost\"),\n  knn_tune %&gt;% \n    collect_metrics() %&gt;% \n    filter(!.metric %in% c(\"mn_log_loss\")) %&gt;% \n    mutate(model = \"KNN\"),\n  log_tune %&gt;% \n    collect_metrics() %&gt;% \n    filter(!.metric %in% c(\"mn_log_loss\")) %&gt;% \n    mutate(model = \"Logistic Regression\")\n) %&gt;% \n  ggplot(aes(.metric, mean, colour = .config)) +\n  geom_jitter(show.legend = F, width = 0.2, alpha = 0.4) +\n  facet_wrap(~ model) +\n  labs(title = \"Hyperparameter Tuning Results\",\n       subtitle = \"Colours indicate different parameter combinations\",\n       x = NULL,\n       y = NULL) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0,1)) +\n  theme_light() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\nLet’s take a look at the variable importance within the XGBoost model. Unfortunately, KNN does not provide a variable importance metric.\n\ngb_final_fit &lt;- gb_wflow %&gt;%\n  finalize_workflow(select_best(gb_tune, metric = \"roc_auc\")) %&gt;% \n  last_fit(dt_split)\n\ngb_final_fit %&gt;%\n  pluck(\".workflow\", 1) %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vi() %&gt;%\n  slice_max(order_by = Importance, n = 100) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance))) +\n  geom_col(fill = \"midnightblue\", colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = NULL,\n       y = \"Predictor\",\n       x = \"Relative Variable Importance\") +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"))\n\n\n\n\n\nlog_final_fit &lt;- log_wflow %&gt;% \n  finalize_workflow(select_best(log_tune, metric = \"roc_auc\")) %&gt;% \n  last_fit(dt_split)\n\nlog_final_fit %&gt;%\n  extract_workflow() %&gt;% \n  extract_fit_parsnip() %&gt;%\n  vi() %&gt;% \n  slice_max(order_by = Importance, n = 30) %&gt;% \n  mutate(Importance = ifelse(Sign == \"NEG\", Importance * -1, Importance)) %&gt;% \n  ggplot(aes(Importance, reorder(Variable, Importance),\n             fill = Sign)) +\n  geom_col(colour = \"white\") +\n  labs(title = \"Variable Importance\",\n       subtitle = \"Only the most important predictors are shown.\",\n       y = \"Predictor\",\n       x = \"Coefficient\") +\n  ggsci::scale_fill_jama() +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\"),\n        legend.position = \"bottom\")\n\n\n\n\nIt now becomes clear that the numeric variables were more important than the nominal ones, like income bracket or education status. The total transaction amount, count as well as average transaction size were highly important. Equally, the change in the latter constitutes an important metric as well. Surprisingly, the relationship count is highly important. From the EDA, we know that people with more dependents are less likely to churn, even though it wasn’t clear that the variable was going to be as important at this stage. Another surprise is that customer age is equally important as average utilisation ratio. Then, lastly, the nominal predictors come in last with virtually no importance.\nWith both these individual tuning results, a blended (“stacked”) model could easily be built with the stacks package. In this blog post, the focus will be the business/real life application however, therefore this won’t be implemented this time. If you are interested in seeing how the implementation works in R, feel free to check out the Airbnb price prediction blog post.\n\n\n\n\nEvaluating Model Performance On The Training Data\n\nUsing the fitted models to predict and evaluate on the test set, which was held out from the model tuning and training to prevent data leakage:\n\nmulti_metric &lt;- metric_set(accuracy, sensitivity, specificity)\n\nbind_rows(\n  gb_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    predict(dt_test) %&gt;% \n    bind_cols(dt_test %&gt;% select(attrition_flag)) %&gt;% \n    multi_metric(estimate = .pred_class, truth = attrition_flag),\n  gb_final_fit %&gt;% \n    extract_workflow() %&gt;% \n    predict(dt_test) %&gt;% \n    bind_cols(dt_test %&gt;% select(attrition_flag)) %&gt;% \n    mutate(.pred_class = .pred_class %&gt;% as.character %&gt;% as.numeric) %&gt;% \n    roc_auc(attrition_flag, .pred_class)\n)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.960\n2 sensitivity binary         0.859\n3 specificity binary         0.980\n4 roc_auc     binary         0.919\n\n\nAs we have a class imbalance in the out-of-sample data, the accuracy should not be considered a good metric to gauge model performance, as a model predicting the negative class only would lead to very high accuracy, but terrible sensitivity and ROC AUC.\nLet’s make this an example:\n\ndt_test %&gt;% \n  transmute(attrition_flag = factor(attrition_flag, levels = c(0,1)), \n            all_negative = factor(0, levels = c(0,1))) %&gt;%\n  multi_metric(truth = attrition_flag, \n               estimate = all_negative)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.840\n2 sensitivity binary         1    \n3 specificity binary         0    \n\n\n\nbind_rows(\n  dt_test %&gt;% \n    transmute(attrition_flag = factor(attrition_flag, levels = c(0,1)), \n              all_negative = factor(0, levels = c(0,1))) %&gt;%\n    multi_metric(truth = attrition_flag, \n                 estimate = all_negative),\n  dt_test %&gt;% \n    transmute(attrition_flag = factor(attrition_flag, levels = c(0,1)), \n              all_negative = 0) %&gt;%\n    roc_auc(attrition_flag, all_negative) \n)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.840\n2 sensitivity binary         1    \n3 specificity binary         0    \n4 roc_auc     binary         0.5  \n\n\nAs mentioned before, this “model” only predicts “does not churn” for all clients. The specificity is great, because all clients that did not churn, were predicted to not churn. The sensitivity, however, is terrible, because not a single churned client was predicted correctly. The ROC AUC curve lies on the identity line, thus giving the value 0.5, which indicates that the model is not better than random guessing.\nThe confusion matrix of the better model can now be visualised, like in the chart below. Especially for a binary classification, this chart is easy to interpret and tells us a lot about what is going on. The model predicts both classes very accurately, but misses a higher fraction of the positive class. The minority class being quite rare and the training data set not being extremely large, this is not surprising. In general, the model performance can be considered very good.\n\ngb_final_fit %&gt;% \n  extract_workflow() %&gt;% \n  predict(dt_test) %&gt;% \n  bind_cols(dt_test %&gt;% select(attrition_flag)) %&gt;% \n  conf_mat(truth = attrition_flag, estimate = .pred_class) %&gt;% \n  autoplot() +\n  labs(title = \"Confusion Matrix Of The OOS Prediction\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12))\n\n\n\n\n\n\n\n\nMaximising Business Profit by Using Classification Thresholds and Gain Curves\n\nFrom the EDA as well as variable importance, we have learned about the most important variables to predict bank customer churn. Transaction amounts and frequency, as well as personal traits, such as relationships and age are of high importance.\n\n\nPotential use in a real life setting\nThe real question following the above is: How can we use any of the above to create business value? That’s were the gain curve comes in.\nClassification models work by assigning probabilities of each class to any individual observation. For instance, one specific customer might be categorised with 75% probability of churning and 25% probability of not churning. In that case, the model would predict the customer to churn, as the probability is higher than 50%. This can be seen below. The XGBoost model predicted around 70% probability of the customer churning and around 30% of them not churning based on the available variables.\n\ngb_final_fit %&gt;% \n  extract_workflow() %&gt;% \n  augment(dt_test) %&gt;% \n  filter(round(.pred_1, 1) == 0.7) %&gt;% \n  head(1) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 18\n$ id                       &lt;dbl&gt; 9718\n$ attrition_flag           &lt;fct&gt; 0\n$ customer_age             &lt;dbl&gt; 48\n$ gender                   &lt;fct&gt; F\n$ education_level          &lt;fct&gt; High School\n$ income_category          &lt;fct&gt; $40K - $60K\n$ total_relationship_count &lt;dbl&gt; 5\n$ months_inactive_12_mon   &lt;dbl&gt; 3\n$ credit_limit             &lt;dbl&gt; 3871\n$ total_revolving_bal      &lt;dbl&gt; 2055\n$ total_amt_chng_q4_q1     &lt;dbl&gt; 0.824\n$ total_trans_amt          &lt;dbl&gt; 5480\n$ total_trans_ct           &lt;dbl&gt; 58\n$ total_ct_chng_q4_q1      &lt;dbl&gt; 0.933\n$ avg_utilization_ratio    &lt;dbl&gt; 0.531\n$ .pred_class              &lt;fct&gt; 1\n$ .pred_1                  &lt;dbl&gt; 0.6876283\n$ .pred_0                  &lt;dbl&gt; 0.3123717\n\n\nIn a business setting, a logical consequence for a model as reliable as this one predicting a customer to churn would be to target said customer with a retention programme, for instance via selected benefits only attributed to customers at risk of churning (e.g. coupons, discounts etc.). However, it would likely not be economically viable to target all customers that have a greater than 50% chance of churning, as it would most likely be a waste of resources. If a customer has only a 50.00001% chance of churning, according to the model, and assuming that the model is right, then the customer should not deserve a discount equal to the one given to a customer with &gt;90% probability of leaving. After all, there is an around 50% chance that the first might not intend to leave in the first place. Then the discount would be wasted.\nThe business only wants to give costly retention programmes to customers that are at a high risk of leaving, not to the ones who were more likely going to stay anyway.\nTherefore, businesses must find a threshold: Where do you set the minimum probability proposed by the model to classify a customer as at risk of churning? There exists an inherent trade-off in wanting to prevent customers from leaving the business, and not wanting to accumulate costs giving out retention programmes to many clients, who were not at high risk of leaving. As an example, the bank might decide that it targets the top 25% of customers with highest probability. This can be visualised with a gain curve, which comes with the tidymodels package in R:\n\ngb_final_fit %&gt;% \n  collect_predictions() %&gt;%\n  gain_curve(truth = attrition_flag, .pred_1) %&gt;% \n  autoplot() +\n  geom_vline(xintercept = 25, lty = \"dashed\", colour = \"grey50\")\n\n\n\n\nThe curve can be read the following way: If targeting the x customers with highest probability of leaving, how many % y do we get right of the customers who will actually leave? In the case of targeting the 25% of customers with highest modelled probability of leaving, we would get close to but not 100% of the customers with an intention of leaving right. The curve being very close to the upper edge of the grey area indicates that the underlying model works very well.\nThe gain curve is really useful and quick to make in R, however it only says “target the 25% of customers with highest probabilities”. This being dependent on the customers that predictions are being made on, I wanted to create a function that says “target only customers that have a probability x of leaving or higher”. This can be seen here (Code for the function call on the chart can be inspected by clicking the “Code” button to the right below):\n\ncase_counts &lt;- function(probs){\n  \n  gb_final_fit %&gt;% \n    collect_predictions() %&gt;% \n    mutate(\n      .pred_thr = ifelse(.pred_1 &gt; probs, 1, 0),\n      case = case_when(\n        .pred_thr == 1 & attrition_flag == 1 ~ \"TP\",\n        .pred_thr == 1 & attrition_flag == 0 ~ \"FP\",\n        .pred_thr == 0 & attrition_flag == 1 ~ \"FN\",\n        .pred_thr == 0 & attrition_flag == 0 ~ \"TN\"\n      )) %&gt;% \n    count(case)  \n  \n}\n\nthreshold_curve &lt;- function(probs){\n  \n  probs %&gt;% \n    as_tibble() %&gt;% \n    rename(threshold = value) %&gt;%\n    mutate(counts = map(threshold, case_counts)) %&gt;%\n    unnest(counts) %&gt;%\n    pivot_wider(values_from = n, names_from = case) %&gt;% \n    mutate(across(everything(), ~ replace(., is.na(.), 0)),\n           sensitivity = TP/(TP + FN),\n           specificity = TN/(TN + FP))\n  \n}\n\n\nthreshold_curve(seq(0, 1, 0.01)) %&gt;% \n  ggplot(aes(threshold, sensitivity)) +\n  geom_line() +\n  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),\n               size = 0.25, colour = \"grey50\", lty = \"dashed\") +\n  geom_ribbon(aes(ymin = 1-threshold, ymax = sensitivity), alpha = 0.1) +\n  labs(\n    title = \"What % of churned customers is correctly targeted?\",\n    subtitle = \"Threshold: Modelled Probability &gt; x to classify positively\",\n    y = \"sensitivity (true positive)\"\n  ) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     breaks = seq(0,1,0.1)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0,1,0.1)) +\n  theme_light() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"))\n\n\n\n\nThis curve now demonstrates the real trade off of setting the classification threshold better. At 50%, the default threshold for the model to classify clients above 50% as will churn and below 50% as will not churn has a higher sensitivity. However, at the same time, we are classifying more clients on an absolute level as will churn. Therefore, there are likely also more clients in our predicted will churn class, that are not actually going to leave us. Remember, we didn’t want to spend additional money on them, as they are not going to leave.\nTherefore, while we, as the business, want to maximise the number of churning customers we target with retention programmes, we also want to minimise the non-churning customers we wrongly give the coupons/discounts.\nThis can be seen below: With an increasing threshold, i.e. the “stricter” we make our model, the fewer % of actually non-churning customers we target with coupons that were designed for the customers at risk of churning.\n\nthreshold_curve(seq(0, 1, 0.01)) %&gt;% \n  ggplot(aes(threshold, 1-specificity)) +\n  geom_line() +\n  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),\n               size = 0.25, colour = \"grey50\", lty = \"dashed\") +\n  geom_ribbon(aes(ymin = 1-threshold, ymax = 1-specificity), alpha = 0.1) +\n  labs(\n    title = \"What % of non-churning customers is wrongly targeted?\",\n    subtitle = \"Threshold: Modelled Probability &gt; x to classify positively\",\n    y = NULL\n  ) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     breaks = seq(0,1,0.1)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0,1,0.1)) +\n  theme_light() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\", size = 12,\n                                     colour = \"grey50\"))\n\n\n\n\n\n\n\nEnough theory, how can this create value?\nWith this curve, we can look at an actual business case: Let’s make some quick and dirty assumptions about profit generated per customers.\n\nLet’s say, a regular, non-churning customer generates USD 100 of profit for us.\nWe are going to give out a discount of 50% to customers we believe will churn in the next period. It is effective, but not perfectly effective, so only 50% of those customers, who were going to leave, stay after getting the discount. The others still leave and leave us with USD \\(0\\).\nCustomers who leave us do not spend any money anymore, so we get USD \\(0\\) from them.\n\nIn model terms this implies:\n\nTP = True Positive: We predicted the customer leaves, we gave out a 50% voucher. 50% of them stay and pay USD 50, the rest leaves. Our profit from this group is \\(N_{TP}*50*0.5\\).\nFP = False Positive: We predicted the customers leaves, but they weren’t planning on leaving. We gave them a 50% discount, all of them stay and our profit from this group is \\(N_{FP}*50\\).\nTN = True Negative: We predicted the customer is not going to leave, they actually didn’t leave. We like those customers because of their loyalty and because they give us the most money, namely \\(N_{TN}*100\\).\nFN = False Negatives: We predicted the customer is not going to leave, but they actually left. These are bad, because we didn’t target them with a voucher. Ouch: The profit from this group is \\(0\\).\n\nNow I can go ahead and write a function, which counts our TP, FP, TN and FN and calculates the profit based on the sum of all of the four points above, for each threshold we could use in our model.\n\nprofit_curve &lt;- function(probs){\n  \n  probs %&gt;% \n    as_tibble() %&gt;% \n    rename(threshold = value) %&gt;%\n    mutate(counts = map(threshold, case_counts)) %&gt;%\n    unnest(counts) %&gt;%\n    pivot_wider(values_from = n, names_from = case) %&gt;% \n    mutate(\n      across(everything(), ~ replace(., is.na(.), 0)),\n      profit_without_model = FP * 100 + TP * 0 + TN * 100 + FN * 0,\n      profit_with_model = FP * 50 + TP * (50*0.5) + TN * 100 + FN * 0,\n      value_add = profit_with_model - profit_without_model,\n      sign = ifelse(value_add &gt; 0, \"positive\", \"negative\")\n    )\n  \n}\n\n\nprofit_curve(probs = seq(0.01, 1, 0.01)) %&gt;% \n  ggplot(aes(threshold, profit_with_model)) +\n  geom_line(colour = \"grey50\", size = 0.4) + \n  geom_point(aes(colour = sign, group = 1)) +\n  geom_curve(aes(x = 0.95, y = 140000, xend = 0.92, yend = 152000),\n             arrow = arrow(length = unit(0.08, \"inch\")), size = 0.5,\n             color = \"gray20\", curvature = 0.1) +\n  geom_curve(aes(x = 0.4, y = 142000, xend = 0.5, yend = 132000),\n             arrow = arrow(length = unit(0.08, \"inch\")), size = 0.5,\n             color = \"gray20\", curvature = -0.2) +\n  annotate(\"text\", x = 0.35, y = 144000, \n           label = \"Normally, classification models use 50%\\nas the threshold to predict the one\\nor the other class\",\n           size = 3) +\n  annotate(\"text\", x = 0.77, y = 137500, \n           label = \"In this bank customer churn model\\nhowever, profit is maximised when only\\nsaying a customer will churn if they show\\nmore than 90% forecasted probability\",\n           size = 3) +\n  labs(colour = \"Value-add of the \\nmodel compared \\nto using no model:\",\n       y = \"Profit\",\n       x = \"Classification Threshold\", \n       title = \"Forecasted Annual Profit Depending On Classification Threshold\",\n       subtitle = \"Optimal binary classification threshold with profit maximisation in mind\\nis not at 50%, but closer to 90%.\") +\n  scale_y_continuous(labels = dollar_format()) +\n  scale_x_continuous(labels = percent_format(), \n                     breaks = seq(0, 1, 0.1)) +\n  scale_colour_manual(values = c(\"firebrick\", \"dodgerblue\")) +\n  theme_bw() +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(face = \"italic\", colour = \"grey50\",\n                                     size = 12))\n\n\n\n\nThis is the final product! We can now see that the profit is indeed worse at the default of 50% than for higher values. The optimal level for this bank and this type of discount is 90%, as it maximises the total profit. From the colouring, we can see that the baseline revenue without the model is a little under USD 150,000 and that the model generates value for all thresholds with blue coloured points and destroys value for thresholds with red colouring. At the maximisation point, this particular model increases profits by about 3% compared to not using a model at all. This number is of course not carved in stone, it will move with the type of discount and the variance in model performance in future periods.\n\n\n\nConcluding remarks\nIn the above blog post, we have seen that varying and optimising the classification threshold for binary classification models can be crucial for organisational value creation. In cases like these, simple visualisations with data from the predictive models, such as scatter plots and line charts, can be highly beneficial for strategic and financial decision making.\nI hope this post has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out.\nThank you for reading.\n \n\n\nA work by Mathias Steilen"
  },
  {
    "objectID": "posts/20221215-Wasserkraft-Schweiz/index.html",
    "href": "posts/20221215-Wasserkraft-Schweiz/index.html",
    "title": "Schweizerische Wasserkraft",
    "section": "",
    "text": "Beschreibung der Daten\n\nDie Daten zur Schweizer Wasserkraft stammen vom Schweizer Bundesamt für Energie und wurden von Swiss Open Data abgerufen. Vier Dateien enthalten Informationen zu den Namen, dem Standort, dem Alter, dem Typ und den Spezifikationen der Wasserkraftanlagen. Der zweite Datensatz über den Füllstand der Wasserreservoirs stammt ebenfalls vom BFE und wurde ebenfalls über Open Data bezogen.\n\n\n\nDatenaufbereitung\n\nNachdem ich die vier verschiedenen Dateien der ersten Datengrundlage eingelesen habe, verbinde ich die Informationen mit der Haupttabelle per left-join und verwerfe unnötige Informationen. Was bleibt, sind alle nützlichen Informationen:\n\n\nCode\ndata &lt;- plants %&gt;% \n  left_join(plant_type %&gt;% transmute(id, type = de), \n            by = c(\"type_code\" = \"id\")) %&gt;% \n  left_join(plant_specs, \n            by = c(\"wasta_number\" = \"hydropower_plant_r\")) %&gt;% \n  left_join(plant_status %&gt;% transmute(id, status = de), \n            by = c(\"operational_status_code\" = \"id\")) %&gt;% \n  select(-c(type_code, type_code, xtf_id, operational_status_code))\n\nglimpse(data)\n\n\nRows: 718\nColumns: 16\n$ wasta_number                  &lt;dbl&gt; 100100, 100125, 100150, 100200, 100250, …\n$ name                          &lt;chr&gt; \"Val Giuv\", \"Curnera Druckminderer\", \"Va…\n$ location                      &lt;chr&gt; \"Rueras\", \"Schiebekammer Curnera Sataum.…\n$ canton                        &lt;chr&gt; \"GR\", \"GR\", \"GR\", \"GR\", \"GR\", \"GR\", \"GR\"…\n$ beginning_of_operation        &lt;dbl&gt; 1979, 2021, 1945, 1968, 1968, 1962, 1947…\n$ x                             &lt;dbl&gt; 2700400, 2697758, 2701420, 2701770, 2701…\n$ y                             &lt;dbl&gt; 1169740, 1165697, 1170140, 1169620, 1169…\n$ type                          &lt;chr&gt; \"Laufkraftwerk\", \"Laufkraftwerk\", \"Laufk…\n$ year                          &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021…\n$ performance_generator_maximum &lt;dbl&gt; 1.42, 2.10, 2.00, 147.00, 0.70, 176.40, …\n$ performance_turbine_maximum   &lt;dbl&gt; 1.50, 2.10, 2.00, 150.00, 0.75, 180.00, …\n$ production_expected           &lt;dbl&gt; 6.10, 10.00, 6.00, 258.40, 2.90, 563.40,…\n$ pumps_power_input_maximum     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ engine_power_demand           &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, …\n$ date_of_statistic             &lt;chr&gt; \"31/12/2021\", \"31/12/2021\", \"31/12/2021\"…\n$ status                        &lt;chr&gt; \"im Normalbetrieb\", \"im Normalbetrieb\", …\n\n\nGlücklicherweise gibt es praktisch keine fehlenden Werte, mit Ausnahme der Spalte canton, in der nur etwa 2% der Daten fehlen.\n\n\nCode\ncolMeans(is.na(data)) %&gt;% \n  enframe() %&gt;% \n  rename(missing = value) %&gt;% \n  filter(missing &gt; 0)\n\n\n# A tibble: 1 × 2\n  name   missing\n  &lt;chr&gt;    &lt;dbl&gt;\n1 canton  0.0209\n\n\nMit den bereinigten Daten kann ich die Fragen, an denen ich interessiert bin, visuell untersuchen.\n\n\n\n\nWelche Kantone haben die höchste erwartete Produktion?\n\nWallis und Graubünden sind die beiden wichtigsten Kantone für Speicherkraftwerke und Pumpspeicherkraftwerke. Aargau ist ebenfalls noch ein wichtiger Kanton im Bezug auf Lauftkraftwerke. Laufkraftwerke erzeugen in Erwartung jedoch noch mehr als Speicherkraftwerke, da sie in vielen Kantonen der Schweiz zur Anwendung kommen. Speicherkraftwerke sind konzentriert auf wenige Kantone, die durch die Höhendifferenzen der Alpen begünstigt sind.\n\n\nCode\ndata %&gt;% \n  filter(type != \"reines Umwälzwerk\",\n         !is.na(canton)) %&gt;% \n  left_join(data %&gt;% \n              group_by(type) %&gt;% \n              summarise(gwh = sum(production_expected)) %&gt;% \n              mutate(share = gwh/sum(gwh)) %&gt;% \n              select(type, share),\n            by = \"type\") %&gt;% \n  mutate(type = paste0(type, \" (\", percent(share), \")\")) %&gt;% \n  group_by(canton, type) %&gt;% \n  summarise(gwh = sum(production_expected)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = gwh,\n             y = canton %&gt;% reorder_within(gwh, type),\n             fill = type)) +\n  geom_col() +\n  facet_wrap(~ type, scales = \"free_y\") +\n  labs(title = \"Erwartete Wasserkraftproduktion nach Kanton\",\n       y = NULL,\n       x = NULL,\n       fill = NULL) +\n  scale_y_reordered() +\n  scale_x_continuous(labels = comma_format(suffix = \" GWh\")) +\n  ggsci::scale_fill_jama() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nWelche Kantone haben die höchste erwartete Produktion (Karte)?\n\nFür diese Visualisierung ziehe ich Basisgeometriedaten des Bundesamts für Statistik zur Hilfe. Da diese Kantonsnummern und ausgeschriebene Namen statt Kürzeln verwenden, left-joine ich die Kantonsnummern inklusive Produktionswerten ebenfalls auf die Geodaten, um ein Choroplethendiagramm zu ermöglichen.\n\n\nCode\nswiss_cantons &lt;- swiss_cantons %&gt;% \n  left_join(\n    data %&gt;%\n      filter(!is.na(canton)) %&gt;% \n      group_by(canton) %&gt;% \n      summarise(production = sum(production_expected)) %&gt;% \n      mutate(KTNR = case_when(canton == \"ZH\" ~ 1,\n                              canton == \"BE\" ~ 2,\n                              canton == \"LU\" ~ 3,\n                              canton == \"UR\" ~ 4,\n                              canton == \"SZ\" ~ 5,\n                              canton == \"OW\" ~ 6,\n                              canton == \"NW\" ~ 7,\n                              canton == \"GL\" ~ 8,\n                              canton == \"ZG\" ~ 9,\n                              canton == \"FR\" ~ 10,\n                              canton == \"SO\" ~ 11,\n                              canton == \"BS\" ~ 12,\n                              canton == \"BL\" ~ 13,\n                              canton == \"SH\" ~ 14,\n                              canton == \"AR\" ~ 15,\n                              canton == \"AI\" ~ 16,\n                              canton == \"SG\" ~ 17,\n                              canton == \"GR\" ~ 18,\n                              canton == \"AG\" ~ 19,\n                              canton == \"TG\" ~ 20,\n                              canton == \"TI\" ~ 21,\n                              canton == \"VD\" ~ 22,\n                              canton == \"VS\" ~ 23,\n                              canton == \"NE\" ~ 24,\n                              canton == \"GE\" ~ 25,\n                              canton == \"JU\" ~ 26)),\n    by = \"KTNR\")\n\n\nWie bereits vom Barchart im ersten Abschnitt gelernt, sind Wallis und Graubünden besonders wichtig für die Wasserkraft in der Schweiz durch ihre Speicherkraftwerke. Dies wird im nachfolgenden Diagramm besonders ersichtlich:\n\n\nCode\nggplot() +\n  geom_sf(data = swiss_cantons, aes(fill = production), colour = \"grey50\",\n          size = 0.25) +\n  labs(title = \"Schweizerische Wasserkraft\",\n       subtitle = \"nach erwarteter Jahresproduktion pro Kanton\",\n       caption = \"Daten: Bundesamt für Energie | 13.12.2022\",\n       fill = \"Erwartete Produktion\") +\n  scale_fill_continuous(labels = comma_format(suffix = \" GWh\"),\n                        type = \"gradient\", low = \"lightblue\",\n                        high = \"midnightblue\") +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(colour = \"midnightblue\", face = \"bold\",\n                                  size = 14),\n        plot.margin = margin(5, 15, 5, 15))\n\n\n\n\n\n\n\n\n\nDie Lage der Kraftwerke in der Schweiz\n\nDa Längen- und Breitengrade der Kraftwerke in den Daten verfügbar sind, kann ich nun ebenfalls die genaue Lage der Kraftwerke abbilden. Ebenfalls vermerkt sind die erwartete jährliche Produktion in Gigawattstunden und der Typ des Kraftwerks. Die graue Schattierung der Kantone ist vergleichbar mit dem Diagramm zuvor und indiziert die erwartete Gesamtproduktion in einem Jahr. Das besonders grosse Speicherkraftwerk im Wallis ist der Grande Dixence Damm, mit der höchsten Gewichtsstaumauer der Welt - die sechsthöchste Staumauer insgesamt und die höchste Staumauer in Europa. Die erwartete jährliche Produktion des Kraftwerks von über 2,000 GWh reicht, um 400,000 Haushalte in der Schweiz zu versorgen.\n\n\nCode\nggplot() +\n  geom_sf(data = swiss_cantons, aes(fill = production),\n          colour = \"grey65\", alpha = 0.2, size = 0.25) +\n  geom_sf(data = swiss_lakes, fill = \"#4BB6EF\",\n          colour = \"grey65\", size = 0.25) +\n  geom_point(data = data %&gt;% arrange(type),\n             aes(x = x, y = y, colour = type, size = production_expected),\n             alpha = 0.5) +\n  labs(title = \"Schweizerische Wasserkraft\",\n       subtitle = \"nach Kraftwerkstyp und erwarteter Jahresproduktion\",\n       caption = \"Daten: Bundesamt für Energie | 13.12.2022\",\n       colour = \"Kraftwerkstyp\",\n       size = \"Produktion (Erwartung)\") +\n  guides(fill = \"none\") +\n  scale_colour_manual(values = c(\"#7393B3\", \"#E14D2A\", \n                                 \"#FD841F\", \"midnightblue\")) +\n  theme_void() +\n  scale_fill_continuous(labels = comma_format(suffix = \" GWh\"),\n                        type = \"gradient\", low = \"white\",\n                        high = \"black\") +\n  scale_size_continuous(labels = comma_format(suffix = \" GWh\"),\n                        range = c(1, 5)) +\n  theme(legend.position = \"right\",\n        plot.title = element_text(colour = \"midnightblue\", face = \"bold\",\n                                  size = 14),\n        plot.margin = margin(5, 15, 5, 15))\n\n\n\n\n\n\n\n\n\nFüllstand der Speicherseen\n\nMit der zweiten Datei des BFE kann ich die Verläufe der Pegelstände in den Stauseen visualisieren. Die Daten spannen von 2000 bis 2022. 2000 bis 2021 verwende ich, um empirische Konfidenzbänder zu erstellen, während 2022 als Linie über diese Bänder zum Vergleich gezogen wird. Aus der untenstehenden Graphik wird ersichtlich, dass die Speicherseen deutlichen Zulauf in den Sommermonaten haben, und/oder Wasser in die Seen pumpen, und Wasser ablassen, also Energie generieren, über die kälteren Wintermonate. Dies ist sowohl zuträglich für die Handelsergebnisse der Energieversorger, aber auch für die Konsumenten, da die hohe Nachfrage nach Elektrizität im Winter durch die Wasserkraft gestützt werden kann. Das Jahr 2022 hat die Medianlinie mehrfach durchschritten und sitzt aktuell über dem 90sten Perzentil des Füllungsgrad. Sofern diese Niveaus beibehalten werden, wird diese einer möglichen Mangellage im fortschreitenden Winter positiv entgegenwirken können.\n\n\nCode\nreservoirs %&gt;% \n  filter(year(datum) &lt; 2022) %&gt;% \n  group_by(week) %&gt;% \n  summarise(quantile = c(\"zero\", \"ten\", \"twentyfive\", \"fifty\", \"seventyfive\",\n                         \"ninety\", \"hundred\"),\n            pct_fill = quantile(pct_fill, c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1),\n                                na.rm = T)) %&gt;% \n  pivot_wider(names_from = quantile, values_from = pct_fill) %&gt;%  \n  ggplot(aes(x = week)) +\n  # Q25 - Q75\n  geom_ribbon(aes(ymin = twentyfive, ymax = seventyfive, fill = \"25%-75%\")) +\n  # Q10 - Q90\n  geom_ribbon(aes(ymin = seventyfive, ymax = ninety, fill = \"10%-90%\")) +\n  geom_ribbon(aes(ymin = ten, ymax = twentyfive, fill = \"10%-90%\")) +\n  # Min Max\n  geom_ribbon(aes(ymin = zero, ymax = ten, fill = \"Min-Max\")) +\n  geom_ribbon(aes(ymin = ninety, ymax = hundred, fill = \"Min-Max\")) +\n  # Median Line\n  geom_line(aes(y = fifty, colour = \"Median\"), lty = \"dotted\") +\n  # Line for 2022\n  geom_line(data = reservoirs %&gt;% filter(year(datum) == 2022),\n            aes(x = week, y = pct_fill, colour = \"2022\")) +\n  scale_fill_manual(values = c(\"25%-75%\" = \"#87CEEB\",\n                               \"10%-90%\" = \"#ADD8E6\",\n                               \"Min-Max\" = \"#CCCCCC\")) +\n  scale_colour_manual(values = c(\"Median\" = \"dodgerblue\",\n                                 \"2022\" = \"midnightblue\")) +\n  labs(title = \"Füllungsgrad der Speicherseen in der Schweiz\",\n       subtitle = \"Datengrundlage ist der Zeitraum 2000-2021\",\n       caption = \"Daten: Bundesamt für Energie BFE | 15.12.2022\",\n       y = NULL, x = \"Kalenderwoche\",\n       fill = NULL, colour = NULL) +\n  scale_x_continuous(breaks = c(1, seq(5, 52, 5))) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 1)) +\n  theme(plot.title = element_text(colour = \"#3F68E6\", size = 14),\n        plot.subtitle = element_text(colour = \"black\", face = \"plain\",\n                                     size = 10),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        plot.margin = margin(15, 15, 15, 15))\n\n\n\n\n\n\nIch hoffe, dieser Artikel war interessant. Falls Sie konstruktives Feedback teilen oder sich über dieses oder ein verwandtes Thema austauschen möchten, können Sie mich gerne über die verlinkten sozialen Netzwerke kontaktieren. Vielen Dank fürs Lesen.\n\n \n\n\nA work by Mathias Steilen"
  }
]