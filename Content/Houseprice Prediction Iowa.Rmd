---
title: "Predicting house prices in Iowa (USA)"
output: 
  html_document:
    theme: readable
    toc: FALSE
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

[Return to Homepage](https://mathiassteilen.github.io/)

[Return to Statistical Modelling Section](https://mathiassteilen.github.io/modelling.html)

![](../Graphics/houses cropped.jpg)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(doParallel)
library(vip)
```

***
### What are we looking at?
***

TBD: Data Description

***
### Data Exploration
***

Firstly, I start by loading the data. The first file is the one to be used for training, whereas the holdout will only be used for submission of out-of-sample predictions.

```{r}
data <- read_csv("C:/Users/mathi/OneDrive/R/Kaggle/House Prices/train.csv")
holdout <- read_csv("C:/Users/mathi/OneDrive/R/Kaggle/House Prices/test.csv")
```

Looking at the data, it becomes quickly apparent that there is a huge number of predictors, namely 79, which can be used for the prediction of the sales price, some of which are numeric and some of which are characters.

```{r}
glimpse(data)
```

Some of the variables have manageable missingness of less than 20%, whereas there exist some variables with too many missing values. These will be discarded. The rest will be imputed as part of the model recipe.

```{r}
data <- 
  data %>% 
  select(-c(Alley, FireplaceQu, PoolQC, Fence, MiscFeature))
```



***
### Building a Model
***

First, the data is split into training and testing sets. Also, five-fold cross validation is employed for stable calculation of performance metrics.

```{r}
dt_split <- initial_split(data, strata = "SalePrice")

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

folds <- vfold_cv(dt_train, v = 5)
```

Creating the recipe with steps for normalisation of numeric predictors as well as the imputation of missing values:

```{r}
gb_rec <- 
  recipe(SalePrice ~ ., 
         data = dt_train) %>%
  update_role(Id, new_role = "ID") %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_impute_median() %>% 
  step_zv(all_numeric())

summary(gb_rec) %>% as.data.frame()
```

Setting up the model specifications with tuning options for hyperparameters:

```{r}
gb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("regression")
```

Setting up the workflow:

```{r}
gb_wflow <- 
  workflow() %>% 
  add_recipe(gb_rec) %>% 
  add_model(gb_spec)
```

Setting up a space-filling design for time-efficient hyperparameter tuning:

```{r}
gb_grid <- 
  grid_latin_hypercube(
    tree_depth(),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(),
    finalize(mtry(),
      dt_train %>%
        select(-Id)),
    learn_rate(),
    size = 30
  )
```

Now, the hyperparameters can be trained:

```{r}
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

gb_tune <- tune_grid(object = gb_wflow,
                     resamples = folds,
                     grid = gb_grid)

stopCluster(cl)
unregister_dopar()
```

We can now look at the training results of the previous step:

```{r}
# Look at tuning results
gb_tune %>% 
  show_best(metric = "rsq") %>%
  select(.metric, mean)

# Visualise tuning results
gb_tune %>%
 collect_metrics() %>%
 filter(.metric == "rsq") %>%
 select(mean, mtry:sample_size) %>%
 pivot_longer(mtry:sample_size,
              values_to = "value",
              names_to = "parameter") %>%
 ggplot(aes(value, mean, color = parameter)) +
 geom_point(alpha = 0.8, show.legend = FALSE) +
 facet_wrap(~parameter, scales = "free_x") +
 labs(x = NULL, y = "rsq")
```

The best model out of training is selected and used for the final fit:

```{r}
gb_final_wflow <- gb_wflow %>%
  finalize_workflow(select_best(gb_tune, metric = "rsq"))

gb_final_fit <- gb_final_wflow %>% 
  last_fit(dt_split)

gb_final_fit %>% collect_metrics()
```

We can see that the $R^2$ for the final out-of-sample fit is very high, indicating high predictive ability by the regressors employed for the prediction.

```{r}
gb_final_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vi() %>%
  slice_max(order_by = Importance, n = 15) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col()
```

```{r}
gb_final_fit %>% 
  collect_predictions() %>% 
  ggplot(aes(SalePrice, .pred)) +
  geom_point(alpha = 0.4, colour = "midnightblue") +
  geom_abline(lty = "dashed", colour = "grey50") +
  labs(x = "True Sales Price",
       y = "Predicted Sales Price",
       title = "Predictions of Final Fit") +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_x_continuous(labels = scales::dollar_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12)) +
  coord_equal()
```


***
### Evaluating Model Performance on the Prediction Data
***

&nbsp;
<hr />
<p style="text-align: center;">A work by <a href="https://www.linkedin.com/in/mathias-steilen/">Mathias Steilen</a></p>
&nbsp;